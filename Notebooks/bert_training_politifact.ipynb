{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM9BLE2OzoIk"
      },
      "source": [
        "## Let's install all relevant libraries to construct our GCN!\n",
        "\n",
        "The entire thing will take about 10 minutes. Please be patient!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  \n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric\n",
        "#!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLAEO36k0Ckb"
      },
      "source": [
        "Now for our dataset consisting of 314 Politifact(Political News) and 5464 Gossicop(Celebrity News) Graphs. Read more at https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.UPFD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q_KobTgtj-BZ"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import UPFD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBbas_ZN0_JQ"
      },
      "source": [
        "name can be politifact and gossicop depending on which graphs you would like to extract. feature refers to the embedding type- which refers to the features and transformers used to create the node vectors. We use 'content' which is an aggregation of user profiles and user activities on twitter\n",
        "\n",
        "- bert: the 768-dimensional node feature composed of Twitter user historical tweets encoded by the bert-as-service\n",
        "- content: the 310-dimensional node feature composed of a 300-dimensional “spacy” vector plus a 10-dimensional “profile” vector\n",
        "- profile: the 10-dimensional node feature composed of ten Twitter user profile attributes.\n",
        "- spacy: the 300-dimensional node feature composed of Twitter user historical tweets encoded by the spaCy word2vec encoder.\n",
        "\n",
        "Statistics:\n",
        "\n",
        "Politifact:\n",
        "- Graphs: 314\n",
        "- Nodes: 41,054\n",
        "- Edges: 40,740\n",
        "- Classes:\n",
        "    - Fake: 157\n",
        "    - Real: 157\n",
        "- Node feature size:\n",
        "    - bert: 768\n",
        "    - content: 310\n",
        "    - profile: 10\n",
        "    - spacy: 300\n",
        "    \n",
        "Gossipcop:\n",
        "- Graphs: 5,464\n",
        "- Nodes: 314,262\n",
        "- Edges: 308,798\n",
        "- Classes:\n",
        "    - Fake: 2,732\n",
        "    - Real: 2,732\n",
        "- Node feature size:\n",
        "    - bert: 768\n",
        "    - content: 310\n",
        "    - profile: 10\n",
        "    - spacy: 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3VsMus-oeyA",
        "outputId": "b732c119-ffc8-4abe-ccb5-2d32240fe052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gossipcop Dataset\n",
            "Train Samples:  1092\n",
            "Validation Samples:  546\n",
            "Test Samples:  3826\n",
            "Politifact Dataset\n",
            "Train Samples:  93\n",
            "Validation Samples:  31\n",
            "Test Samples:  221\n"
          ]
        }
      ],
      "source": [
        "test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\",split=\"test\")\n",
        "train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"train\")\n",
        "val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"val\")\n",
        "\n",
        "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\",split=\"test\")\n",
        "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"train\")\n",
        "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"val\")\n",
        "train_data_pol = train_data_pol + val_data_pol\n",
        "\n",
        "print(\"Gossipcop Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_gos))\n",
        "print(\"Validation Samples: \", len(val_data_gos))\n",
        "print(\"Test Samples: \", len(test_data_gos))\n",
        "\n",
        "print(\"Politifact Dataset\")\n",
        "print(\"Train Samples: \", len(train_data_pol))\n",
        "print(\"Validation Samples: \", len(val_data_pol))\n",
        "print(\"Test Samples: \", len(test_data_pol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m08p_57bkebo"
      },
      "outputs": [],
      "source": [
        "# combing all data availbale\n",
        "def combineAllUPFDData(feature):\n",
        "  test_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\",split=\"train\")\n",
        "  train_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"test\")\n",
        "  val_data_gos = UPFD(root=\".\", name=\"gossipcop\", feature=\"content\", split=\"val\")\n",
        "\n",
        "  test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\",split=\"train\")\n",
        "  train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\", split=\"test\")\n",
        "  val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"content\", split=\"val\")\n",
        "  print(\"Gossipcop Dataset\")\n",
        "  print(\"Train Samples: \", len(train_data_gos))\n",
        "  print(\"Validation Samples: \", len(val_data_gos))\n",
        "  print(\"Test Samples: \", len(test_data_gos))\n",
        "\n",
        "  print(\"Politifact Dataset\")\n",
        "  print(\"Train Samples: \", len(train_data_pol))\n",
        "  print(\"Validation Samples: \", len(val_data_pol))\n",
        "  print(\"Test Samples: \", len(test_data_pol))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWWbaOW71z0E"
      },
      "source": [
        "train is a indexable object with each index refering to a different graph. Each graph has the attribute x, which refers to the node embeddings (node vectors) and edge-index, which specifies the directed edges in the graph as an object containing two lists- the first one specifying the source node index and the other specifying the destination node index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DcPMdUZozNv",
        "outputId": "db36ab9f-2f0c-456c-ea25-9b469da21a85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,\n",
              "          7, 12, 14, 16, 30, 32, 33, 35],\n",
              "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "         37, 38, 39, 40, 41, 42, 43, 44]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_pol[0].edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_z4dvppo1fT",
        "outputId": "a77fdf9b-db44-41eb-9a1f-7798e8e5dd4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5780, 0.5658, 0.3858,  ..., 0.5833, 0.1750, 0.3777],\n",
              "        [0.6087, 0.6196, 0.2900,  ..., 0.4122, 0.1538, 0.2703],\n",
              "        [0.5888, 0.5631, 0.2753,  ..., 0.2230, 0.1538, 0.0000],\n",
              "        ...,\n",
              "        [0.5958, 0.5722, 0.4397,  ..., 0.7095, 0.2308, 0.1081],\n",
              "        [0.5987, 0.5378, 0.4844,  ..., 0.6419, 0.0769, 0.0811],\n",
              "        [0.5906, 0.5376, 0.3346,  ..., 0.9324, 0.1538, 0.3784]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data_pol[0].x # what do you mean by node vectors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MafoKOINzhJd"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_loader = DataLoader(train_data_pol, batch_size=256, shuffle=True)\n",
        "test_loader = DataLoader(test_data_pol, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG3L1fy6Ua5X",
        "outputId": "9ba9bfb6-aed0-431a-c0c2-3840a0c1317d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data_pol.num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHYdr_JY2qMl"
      },
      "source": [
        "Let's build our model! Remember the architecture given by the paper https://arxiv.org/pdf/1902.06673.pdf Page 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1clhMrboqQR5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import LeakyReLU, Softmax, Linear, SELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, GATv2Conv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.transforms import ToUndirected\n",
        "from torch.nn import LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oBU_JkWBpjjj"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
        "\n",
        "        self.softmax = Linear(hidden_channels[5],out_channels)\n",
        "\n",
        "        #droupouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h,batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.softmax(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YmMxppph-NWK"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EFKDXt0qdK_",
        "outputId": "fc7ceb3c-4466-4808-8b1d-24e46409efa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_pol.num_features,[512,512,512,256,256,256],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfZjTdshMx2o",
        "outputId": "06c14dcc-dbfe-4406-ac18-ff2a83c57858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Parameters of the model : 2099713\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 512, aggr=mean)\n",
            "  (full1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (full2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (full3): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (softmax): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Parameters of the model :\",sum([param.nelement() for param in model.parameters()]))\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2wE-4i3CyBFk"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oRomTbeV2aZK",
        "outputId": "10041d06-cb5d-485e-fdc6-f234a03799b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69356 | TestLoss: 0.69266 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 01 |  TrainLoss: 0.69438 | TestLoss: 0.69253 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 02 |  TrainLoss: 0.69463 | TestLoss: 0.69240 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 03 |  TrainLoss: 0.69284 | TestLoss: 0.69234 | TestAcc: 0.51131 | TestF1: 0.68\n",
            "Epoch: 04 |  TrainLoss: 0.69205 | TestLoss: 0.69224 | TestAcc: 0.70588 | TestF1: 0.76\n",
            "Epoch: 05 |  TrainLoss: 0.69238 | TestLoss: 0.69213 | TestAcc: 0.61991 | TestF1: 0.42\n",
            "Epoch: 06 |  TrainLoss: 0.69101 | TestLoss: 0.69203 | TestAcc: 0.50679 | TestF1: 0.07\n",
            "Epoch: 07 |  TrainLoss: 0.69066 | TestLoss: 0.69193 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 08 |  TrainLoss: 0.69055 | TestLoss: 0.69186 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 09 |  TrainLoss: 0.68927 | TestLoss: 0.69167 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 10 |  TrainLoss: 0.68942 | TestLoss: 0.69131 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 11 |  TrainLoss: 0.68921 | TestLoss: 0.69085 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 12 |  TrainLoss: 0.68744 | TestLoss: 0.69031 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 13 |  TrainLoss: 0.68767 | TestLoss: 0.68963 | TestAcc: 0.48869 | TestF1: 0.00\n",
            "Epoch: 14 |  TrainLoss: 0.68710 | TestLoss: 0.68873 | TestAcc: 0.49321 | TestF1: 0.02\n",
            "Epoch: 15 |  TrainLoss: 0.68525 | TestLoss: 0.68768 | TestAcc: 0.51131 | TestF1: 0.08\n",
            "Epoch: 16 |  TrainLoss: 0.68345 | TestLoss: 0.68644 | TestAcc: 0.55656 | TestF1: 0.23\n",
            "Epoch: 17 |  TrainLoss: 0.68256 | TestLoss: 0.68506 | TestAcc: 0.57919 | TestF1: 0.31\n",
            "Epoch: 18 |  TrainLoss: 0.67917 | TestLoss: 0.68354 | TestAcc: 0.59276 | TestF1: 0.35\n",
            "Epoch: 19 |  TrainLoss: 0.67800 | TestLoss: 0.68187 | TestAcc: 0.59729 | TestF1: 0.36\n",
            "Epoch: 20 |  TrainLoss: 0.67432 | TestLoss: 0.67991 | TestAcc: 0.60181 | TestF1: 0.37\n",
            "Epoch: 21 |  TrainLoss: 0.67312 | TestLoss: 0.67769 | TestAcc: 0.62896 | TestF1: 0.45\n",
            "Epoch: 22 |  TrainLoss: 0.66830 | TestLoss: 0.67516 | TestAcc: 0.64706 | TestF1: 0.49\n",
            "Epoch: 23 |  TrainLoss: 0.66642 | TestLoss: 0.67237 | TestAcc: 0.65611 | TestF1: 0.51\n",
            "Epoch: 24 |  TrainLoss: 0.66296 | TestLoss: 0.66920 | TestAcc: 0.65158 | TestF1: 0.50\n",
            "Epoch: 25 |  TrainLoss: 0.65692 | TestLoss: 0.66553 | TestAcc: 0.67873 | TestF1: 0.56\n",
            "Epoch: 26 |  TrainLoss: 0.65365 | TestLoss: 0.66103 | TestAcc: 0.68778 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.64668 | TestLoss: 0.65593 | TestAcc: 0.71041 | TestF1: 0.63\n",
            "Epoch: 28 |  TrainLoss: 0.64404 | TestLoss: 0.65015 | TestAcc: 0.72398 | TestF1: 0.66\n",
            "Epoch: 29 |  TrainLoss: 0.63868 | TestLoss: 0.64389 | TestAcc: 0.74661 | TestF1: 0.70\n",
            "Epoch: 30 |  TrainLoss: 0.62628 | TestLoss: 0.63690 | TestAcc: 0.74661 | TestF1: 0.70\n",
            "Epoch: 31 |  TrainLoss: 0.61224 | TestLoss: 0.62928 | TestAcc: 0.74661 | TestF1: 0.70\n",
            "Epoch: 32 |  TrainLoss: 0.60318 | TestLoss: 0.62122 | TestAcc: 0.73303 | TestF1: 0.68\n",
            "Epoch: 33 |  TrainLoss: 0.59445 | TestLoss: 0.61125 | TestAcc: 0.74208 | TestF1: 0.69\n",
            "Epoch: 34 |  TrainLoss: 0.57257 | TestLoss: 0.59972 | TestAcc: 0.78281 | TestF1: 0.75\n",
            "Epoch: 35 |  TrainLoss: 0.56183 | TestLoss: 0.58798 | TestAcc: 0.78281 | TestF1: 0.76\n",
            "Epoch: 36 |  TrainLoss: 0.54655 | TestLoss: 0.57527 | TestAcc: 0.77828 | TestF1: 0.75\n",
            "Epoch: 37 |  TrainLoss: 0.52271 | TestLoss: 0.56109 | TestAcc: 0.78733 | TestF1: 0.76\n",
            "Epoch: 38 |  TrainLoss: 0.50622 | TestLoss: 0.54412 | TestAcc: 0.80090 | TestF1: 0.78\n",
            "Epoch: 39 |  TrainLoss: 0.48749 | TestLoss: 0.52745 | TestAcc: 0.80090 | TestF1: 0.78\n",
            "Epoch: 40 |  TrainLoss: 0.45364 | TestLoss: 0.51254 | TestAcc: 0.80090 | TestF1: 0.78\n",
            "Epoch: 41 |  TrainLoss: 0.42465 | TestLoss: 0.49518 | TestAcc: 0.80543 | TestF1: 0.79\n",
            "Epoch: 42 |  TrainLoss: 0.39449 | TestLoss: 0.47345 | TestAcc: 0.81900 | TestF1: 0.81\n",
            "Epoch: 43 |  TrainLoss: 0.35908 | TestLoss: 0.45906 | TestAcc: 0.82353 | TestF1: 0.81\n",
            "Epoch: 44 |  TrainLoss: 0.31834 | TestLoss: 0.44684 | TestAcc: 0.81448 | TestF1: 0.80\n",
            "Epoch: 45 |  TrainLoss: 0.31456 | TestLoss: 0.42976 | TestAcc: 0.82353 | TestF1: 0.81\n",
            "Epoch: 46 |  TrainLoss: 0.27573 | TestLoss: 0.41523 | TestAcc: 0.83258 | TestF1: 0.83\n",
            "Epoch: 47 |  TrainLoss: 0.24176 | TestLoss: 0.42434 | TestAcc: 0.80995 | TestF1: 0.79\n",
            "Epoch: 48 |  TrainLoss: 0.20574 | TestLoss: 0.40592 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 49 |  TrainLoss: 0.19083 | TestLoss: 0.39260 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 50 |  TrainLoss: 0.16973 | TestLoss: 0.43138 | TestAcc: 0.81900 | TestF1: 0.81\n",
            "Epoch: 51 |  TrainLoss: 0.14591 | TestLoss: 0.43239 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 52 |  TrainLoss: 0.10807 | TestLoss: 0.41296 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 53 |  TrainLoss: 0.09644 | TestLoss: 0.41937 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 54 |  TrainLoss: 0.09242 | TestLoss: 0.48034 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 55 |  TrainLoss: 0.07175 | TestLoss: 0.47307 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 56 |  TrainLoss: 0.07472 | TestLoss: 0.47174 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 57 |  TrainLoss: 0.06522 | TestLoss: 0.51556 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 58 |  TrainLoss: 0.06216 | TestLoss: 0.49157 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 59 |  TrainLoss: 0.03639 | TestLoss: 0.50162 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 60 |  TrainLoss: 0.02694 | TestLoss: 0.53874 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 61 |  TrainLoss: 0.01869 | TestLoss: 0.63150 | TestAcc: 0.82805 | TestF1: 0.82\n",
            "Epoch: 62 |  TrainLoss: 0.02086 | TestLoss: 0.66372 | TestAcc: 0.82805 | TestF1: 0.82\n",
            "Epoch: 63 |  TrainLoss: 0.01700 | TestLoss: 0.60469 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 64 |  TrainLoss: 0.01254 | TestLoss: 0.58378 | TestAcc: 0.86425 | TestF1: 0.86\n",
            "Epoch: 65 |  TrainLoss: 0.01221 | TestLoss: 0.59925 | TestAcc: 0.86878 | TestF1: 0.87\n",
            "Epoch: 66 |  TrainLoss: 0.01201 | TestLoss: 0.64595 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 67 |  TrainLoss: 0.00730 | TestLoss: 0.73267 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 68 |  TrainLoss: 0.01686 | TestLoss: 0.71812 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 69 |  TrainLoss: 0.00857 | TestLoss: 0.68360 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 70 |  TrainLoss: 0.00469 | TestLoss: 0.67963 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 71 |  TrainLoss: 0.00507 | TestLoss: 0.69263 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 72 |  TrainLoss: 0.00337 | TestLoss: 0.71476 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 73 |  TrainLoss: 0.00586 | TestLoss: 0.77113 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 74 |  TrainLoss: 0.00181 | TestLoss: 0.85026 | TestAcc: 0.83258 | TestF1: 0.82\n",
            "Epoch: 75 |  TrainLoss: 0.00245 | TestLoss: 0.92316 | TestAcc: 0.82353 | TestF1: 0.81\n",
            "Epoch: 76 |  TrainLoss: 0.00768 | TestLoss: 0.88296 | TestAcc: 0.82805 | TestF1: 0.82\n",
            "Epoch: 77 |  TrainLoss: 0.00238 | TestLoss: 0.83595 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 78 |  TrainLoss: 0.00234 | TestLoss: 0.79774 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 79 |  TrainLoss: 0.00220 | TestLoss: 0.78384 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 80 |  TrainLoss: 0.00333 | TestLoss: 0.79056 | TestAcc: 0.85520 | TestF1: 0.86\n",
            "Epoch: 81 |  TrainLoss: 0.00149 | TestLoss: 0.80335 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 82 |  TrainLoss: 0.00130 | TestLoss: 0.82260 | TestAcc: 0.85973 | TestF1: 0.86\n",
            "Epoch: 83 |  TrainLoss: 0.00123 | TestLoss: 0.84921 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 84 |  TrainLoss: 0.00075 | TestLoss: 0.87549 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 85 |  TrainLoss: 0.00071 | TestLoss: 0.90120 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 86 |  TrainLoss: 0.00081 | TestLoss: 0.92331 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 87 |  TrainLoss: 0.00099 | TestLoss: 0.94787 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 88 |  TrainLoss: 0.00115 | TestLoss: 0.96248 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 89 |  TrainLoss: 0.00082 | TestLoss: 0.97001 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 90 |  TrainLoss: 0.00065 | TestLoss: 0.97199 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 91 |  TrainLoss: 0.00101 | TestLoss: 0.96081 | TestAcc: 0.83710 | TestF1: 0.83\n",
            "Epoch: 92 |  TrainLoss: 0.00101 | TestLoss: 0.94180 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 93 |  TrainLoss: 0.00076 | TestLoss: 0.92546 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 94 |  TrainLoss: 0.00070 | TestLoss: 0.91269 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 95 |  TrainLoss: 0.00057 | TestLoss: 0.90379 | TestAcc: 0.85520 | TestF1: 0.85\n",
            "Epoch: 96 |  TrainLoss: 0.00093 | TestLoss: 0.90189 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 97 |  TrainLoss: 0.00050 | TestLoss: 0.90209 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 98 |  TrainLoss: 0.00048 | TestLoss: 0.90424 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 99 |  TrainLoss: 0.00045 | TestLoss: 0.90817 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 100 |  TrainLoss: 0.00036 | TestLoss: 0.91094 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 101 |  TrainLoss: 0.00047 | TestLoss: 0.91499 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 102 |  TrainLoss: 0.00046 | TestLoss: 0.91781 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 103 |  TrainLoss: 0.00041 | TestLoss: 0.92184 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 104 |  TrainLoss: 0.00027 | TestLoss: 0.92586 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 105 |  TrainLoss: 0.00051 | TestLoss: 0.92922 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 106 |  TrainLoss: 0.00021 | TestLoss: 0.93260 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 107 |  TrainLoss: 0.00026 | TestLoss: 0.93659 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 108 |  TrainLoss: 0.00024 | TestLoss: 0.94021 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 109 |  TrainLoss: 0.00064 | TestLoss: 0.94609 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 110 |  TrainLoss: 0.00035 | TestLoss: 0.95219 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 111 |  TrainLoss: 0.00034 | TestLoss: 0.95752 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 112 |  TrainLoss: 0.00029 | TestLoss: 0.96278 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 113 |  TrainLoss: 0.00026 | TestLoss: 0.96689 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 114 |  TrainLoss: 0.00030 | TestLoss: 0.97026 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 115 |  TrainLoss: 0.00018 | TestLoss: 0.97304 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 116 |  TrainLoss: 0.00031 | TestLoss: 0.97448 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 117 |  TrainLoss: 0.00023 | TestLoss: 0.97546 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 118 |  TrainLoss: 0.00029 | TestLoss: 0.97562 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 119 |  TrainLoss: 0.00021 | TestLoss: 0.97595 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 120 |  TrainLoss: 0.00023 | TestLoss: 0.97647 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 121 |  TrainLoss: 0.00025 | TestLoss: 0.97631 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 122 |  TrainLoss: 0.00025 | TestLoss: 0.97536 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 123 |  TrainLoss: 0.00038 | TestLoss: 0.97678 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 124 |  TrainLoss: 0.00018 | TestLoss: 0.97786 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 125 |  TrainLoss: 0.00019 | TestLoss: 0.97945 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 126 |  TrainLoss: 0.00034 | TestLoss: 0.98041 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 127 |  TrainLoss: 0.00023 | TestLoss: 0.98073 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 128 |  TrainLoss: 0.00021 | TestLoss: 0.98019 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 129 |  TrainLoss: 0.00022 | TestLoss: 0.98006 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 130 |  TrainLoss: 0.00023 | TestLoss: 0.97999 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 131 |  TrainLoss: 0.00049 | TestLoss: 0.98594 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 132 |  TrainLoss: 0.00027 | TestLoss: 0.99269 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 133 |  TrainLoss: 0.00027 | TestLoss: 0.99802 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 134 |  TrainLoss: 0.00021 | TestLoss: 1.00330 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 135 |  TrainLoss: 0.00038 | TestLoss: 1.00891 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 136 |  TrainLoss: 0.00016 | TestLoss: 1.01372 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 137 |  TrainLoss: 0.00022 | TestLoss: 1.01706 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 138 |  TrainLoss: 0.00018 | TestLoss: 1.01894 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 139 |  TrainLoss: 0.00021 | TestLoss: 1.01974 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 140 |  TrainLoss: 0.00037 | TestLoss: 1.02093 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 141 |  TrainLoss: 0.00019 | TestLoss: 1.02198 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 142 |  TrainLoss: 0.00028 | TestLoss: 1.02309 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 143 |  TrainLoss: 0.00017 | TestLoss: 1.02480 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 144 |  TrainLoss: 0.00023 | TestLoss: 1.02513 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 145 |  TrainLoss: 0.00014 | TestLoss: 1.02469 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 146 |  TrainLoss: 0.00019 | TestLoss: 1.02370 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 147 |  TrainLoss: 0.00017 | TestLoss: 1.02245 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 148 |  TrainLoss: 0.00019 | TestLoss: 1.02025 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 149 |  TrainLoss: 0.00057 | TestLoss: 1.01316 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 150 |  TrainLoss: 0.00024 | TestLoss: 1.00770 | TestAcc: 0.85068 | TestF1: 0.85\n",
            "Epoch: 151 |  TrainLoss: 0.00020 | TestLoss: 1.00327 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 152 |  TrainLoss: 0.00024 | TestLoss: 1.00004 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 153 |  TrainLoss: 0.00012 | TestLoss: 0.99786 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 154 |  TrainLoss: 0.00023 | TestLoss: 0.99864 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 155 |  TrainLoss: 0.00027 | TestLoss: 1.00230 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 156 |  TrainLoss: 0.00022 | TestLoss: 1.00731 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 157 |  TrainLoss: 0.00014 | TestLoss: 1.01262 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 158 |  TrainLoss: 0.00015 | TestLoss: 1.01766 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 159 |  TrainLoss: 0.00013 | TestLoss: 1.02248 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 160 |  TrainLoss: 0.00016 | TestLoss: 1.02684 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 161 |  TrainLoss: 0.00023 | TestLoss: 1.03156 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 162 |  TrainLoss: 0.00012 | TestLoss: 1.03568 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 163 |  TrainLoss: 0.00025 | TestLoss: 1.03736 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 164 |  TrainLoss: 0.00023 | TestLoss: 1.04117 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 165 |  TrainLoss: 0.00020 | TestLoss: 1.04414 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 166 |  TrainLoss: 0.00013 | TestLoss: 1.04687 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 167 |  TrainLoss: 0.00015 | TestLoss: 1.04842 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 168 |  TrainLoss: 0.00014 | TestLoss: 1.04986 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 169 |  TrainLoss: 0.00015 | TestLoss: 1.05173 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 170 |  TrainLoss: 0.00016 | TestLoss: 1.05180 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 171 |  TrainLoss: 0.00010 | TestLoss: 1.05223 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 172 |  TrainLoss: 0.00011 | TestLoss: 1.05268 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 173 |  TrainLoss: 0.00014 | TestLoss: 1.05170 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 174 |  TrainLoss: 0.00016 | TestLoss: 1.05183 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 175 |  TrainLoss: 0.00012 | TestLoss: 1.05124 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 176 |  TrainLoss: 0.00011 | TestLoss: 1.05105 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 177 |  TrainLoss: 0.00022 | TestLoss: 1.05091 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 178 |  TrainLoss: 0.00010 | TestLoss: 1.05134 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 179 |  TrainLoss: 0.00009 | TestLoss: 1.05181 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 180 |  TrainLoss: 0.00012 | TestLoss: 1.05216 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 181 |  TrainLoss: 0.00007 | TestLoss: 1.05234 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 182 |  TrainLoss: 0.00011 | TestLoss: 1.05350 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 183 |  TrainLoss: 0.00039 | TestLoss: 1.05066 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 184 |  TrainLoss: 0.00008 | TestLoss: 1.04806 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 185 |  TrainLoss: 0.00008 | TestLoss: 1.04587 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 186 |  TrainLoss: 0.00008 | TestLoss: 1.04420 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 187 |  TrainLoss: 0.00018 | TestLoss: 1.04410 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 188 |  TrainLoss: 0.00009 | TestLoss: 1.04372 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 189 |  TrainLoss: 0.00010 | TestLoss: 1.04398 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 190 |  TrainLoss: 0.00008 | TestLoss: 1.04433 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 191 |  TrainLoss: 0.00020 | TestLoss: 1.04701 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 192 |  TrainLoss: 0.00019 | TestLoss: 1.05114 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 193 |  TrainLoss: 0.00010 | TestLoss: 1.05540 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 194 |  TrainLoss: 0.00013 | TestLoss: 1.06110 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 195 |  TrainLoss: 0.00008 | TestLoss: 1.06644 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 196 |  TrainLoss: 0.00011 | TestLoss: 1.07190 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 197 |  TrainLoss: 0.00019 | TestLoss: 1.07956 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 198 |  TrainLoss: 0.00007 | TestLoss: 1.08649 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 199 |  TrainLoss: 0.00012 | TestLoss: 1.09187 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 200 |  TrainLoss: 0.00015 | TestLoss: 1.09672 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 201 |  TrainLoss: 0.00015 | TestLoss: 1.10216 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 202 |  TrainLoss: 0.00008 | TestLoss: 1.10612 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 203 |  TrainLoss: 0.00008 | TestLoss: 1.10961 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 204 |  TrainLoss: 0.00012 | TestLoss: 1.11163 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 205 |  TrainLoss: 0.00007 | TestLoss: 1.11360 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 206 |  TrainLoss: 0.00017 | TestLoss: 1.11362 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 207 |  TrainLoss: 0.00009 | TestLoss: 1.11244 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 208 |  TrainLoss: 0.00010 | TestLoss: 1.11161 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 209 |  TrainLoss: 0.00009 | TestLoss: 1.10981 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 210 |  TrainLoss: 0.00011 | TestLoss: 1.10702 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 211 |  TrainLoss: 0.00007 | TestLoss: 1.10361 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 212 |  TrainLoss: 0.00012 | TestLoss: 1.09910 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 213 |  TrainLoss: 0.00007 | TestLoss: 1.09480 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 214 |  TrainLoss: 0.00007 | TestLoss: 1.09137 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 215 |  TrainLoss: 0.00006 | TestLoss: 1.08819 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 216 |  TrainLoss: 0.00010 | TestLoss: 1.08504 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 217 |  TrainLoss: 0.00009 | TestLoss: 1.08162 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 218 |  TrainLoss: 0.00008 | TestLoss: 1.07918 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 219 |  TrainLoss: 0.00007 | TestLoss: 1.07722 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 220 |  TrainLoss: 0.00006 | TestLoss: 1.07569 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 221 |  TrainLoss: 0.00011 | TestLoss: 1.07609 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 222 |  TrainLoss: 0.00006 | TestLoss: 1.07657 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 223 |  TrainLoss: 0.00028 | TestLoss: 1.08148 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 224 |  TrainLoss: 0.00007 | TestLoss: 1.08657 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 225 |  TrainLoss: 0.00009 | TestLoss: 1.09123 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 226 |  TrainLoss: 0.00010 | TestLoss: 1.09631 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 227 |  TrainLoss: 0.00006 | TestLoss: 1.10153 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 228 |  TrainLoss: 0.00010 | TestLoss: 1.10681 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 229 |  TrainLoss: 0.00009 | TestLoss: 1.11150 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 230 |  TrainLoss: 0.00009 | TestLoss: 1.11534 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 231 |  TrainLoss: 0.00009 | TestLoss: 1.11814 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 232 |  TrainLoss: 0.00007 | TestLoss: 1.12148 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 233 |  TrainLoss: 0.00009 | TestLoss: 1.12376 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 234 |  TrainLoss: 0.00006 | TestLoss: 1.12513 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 235 |  TrainLoss: 0.00009 | TestLoss: 1.12502 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 236 |  TrainLoss: 0.00006 | TestLoss: 1.12476 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 237 |  TrainLoss: 0.00016 | TestLoss: 1.12798 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 238 |  TrainLoss: 0.00013 | TestLoss: 1.12981 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 239 |  TrainLoss: 0.00006 | TestLoss: 1.13182 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 240 |  TrainLoss: 0.00009 | TestLoss: 1.13226 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 241 |  TrainLoss: 0.00010 | TestLoss: 1.13317 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 242 |  TrainLoss: 0.00014 | TestLoss: 1.13095 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 243 |  TrainLoss: 0.00005 | TestLoss: 1.12858 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 244 |  TrainLoss: 0.00007 | TestLoss: 1.12614 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 245 |  TrainLoss: 0.00011 | TestLoss: 1.12516 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 246 |  TrainLoss: 0.00006 | TestLoss: 1.12504 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 247 |  TrainLoss: 0.00005 | TestLoss: 1.12487 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 248 |  TrainLoss: 0.00010 | TestLoss: 1.12449 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 249 |  TrainLoss: 0.00009 | TestLoss: 1.12288 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 250 |  TrainLoss: 0.00007 | TestLoss: 1.12121 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 251 |  TrainLoss: 0.00005 | TestLoss: 1.12012 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 252 |  TrainLoss: 0.00005 | TestLoss: 1.11903 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 253 |  TrainLoss: 0.00011 | TestLoss: 1.11858 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 254 |  TrainLoss: 0.00006 | TestLoss: 1.11853 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 255 |  TrainLoss: 0.00005 | TestLoss: 1.11855 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 256 |  TrainLoss: 0.00008 | TestLoss: 1.11882 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 257 |  TrainLoss: 0.00006 | TestLoss: 1.11910 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 258 |  TrainLoss: 0.00010 | TestLoss: 1.11894 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 259 |  TrainLoss: 0.00006 | TestLoss: 1.11917 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 260 |  TrainLoss: 0.00006 | TestLoss: 1.11958 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 261 |  TrainLoss: 0.00006 | TestLoss: 1.12034 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 262 |  TrainLoss: 0.00004 | TestLoss: 1.12115 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 263 |  TrainLoss: 0.00003 | TestLoss: 1.12203 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 264 |  TrainLoss: 0.00005 | TestLoss: 1.12334 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 265 |  TrainLoss: 0.00008 | TestLoss: 1.12600 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 266 |  TrainLoss: 0.00005 | TestLoss: 1.12844 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 267 |  TrainLoss: 0.00005 | TestLoss: 1.13136 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 268 |  TrainLoss: 0.00041 | TestLoss: 1.12323 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 269 |  TrainLoss: 0.00005 | TestLoss: 1.11712 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 270 |  TrainLoss: 0.00008 | TestLoss: 1.11257 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 271 |  TrainLoss: 0.00004 | TestLoss: 1.10870 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 272 |  TrainLoss: 0.00007 | TestLoss: 1.10623 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 273 |  TrainLoss: 0.00005 | TestLoss: 1.10430 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 274 |  TrainLoss: 0.00006 | TestLoss: 1.10310 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 275 |  TrainLoss: 0.00007 | TestLoss: 1.10282 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 276 |  TrainLoss: 0.00010 | TestLoss: 1.10487 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 277 |  TrainLoss: 0.00011 | TestLoss: 1.10893 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 278 |  TrainLoss: 0.00004 | TestLoss: 1.11279 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 279 |  TrainLoss: 0.00008 | TestLoss: 1.11849 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 280 |  TrainLoss: 0.00005 | TestLoss: 1.12318 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 281 |  TrainLoss: 0.00006 | TestLoss: 1.12832 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 282 |  TrainLoss: 0.00008 | TestLoss: 1.13221 | TestAcc: 0.84163 | TestF1: 0.84\n",
            "Epoch: 283 |  TrainLoss: 0.00003 | TestLoss: 1.13640 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 284 |  TrainLoss: 0.00008 | TestLoss: 1.13887 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 285 |  TrainLoss: 0.00006 | TestLoss: 1.14244 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 286 |  TrainLoss: 0.00005 | TestLoss: 1.14561 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 287 |  TrainLoss: 0.00005 | TestLoss: 1.14842 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 288 |  TrainLoss: 0.00005 | TestLoss: 1.15062 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 289 |  TrainLoss: 0.00004 | TestLoss: 1.15299 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 290 |  TrainLoss: 0.00003 | TestLoss: 1.15520 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 291 |  TrainLoss: 0.00005 | TestLoss: 1.15655 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 292 |  TrainLoss: 0.00004 | TestLoss: 1.15823 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 293 |  TrainLoss: 0.00004 | TestLoss: 1.15998 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 294 |  TrainLoss: 0.00005 | TestLoss: 1.16189 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 295 |  TrainLoss: 0.00003 | TestLoss: 1.16386 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 296 |  TrainLoss: 0.00004 | TestLoss: 1.16600 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 297 |  TrainLoss: 0.00008 | TestLoss: 1.16957 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 298 |  TrainLoss: 0.00002 | TestLoss: 1.17274 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 299 |  TrainLoss: 0.00008 | TestLoss: 1.17733 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 300 |  TrainLoss: 0.00009 | TestLoss: 1.18279 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 301 |  TrainLoss: 0.00004 | TestLoss: 1.18690 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 302 |  TrainLoss: 0.00003 | TestLoss: 1.19045 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 303 |  TrainLoss: 0.00009 | TestLoss: 1.19109 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 304 |  TrainLoss: 0.00004 | TestLoss: 1.19133 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 305 |  TrainLoss: 0.00003 | TestLoss: 1.19128 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 306 |  TrainLoss: 0.00009 | TestLoss: 1.18988 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 307 |  TrainLoss: 0.00008 | TestLoss: 1.18702 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 308 |  TrainLoss: 0.00005 | TestLoss: 1.18555 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 309 |  TrainLoss: 0.00006 | TestLoss: 1.18322 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 310 |  TrainLoss: 0.00004 | TestLoss: 1.18137 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 311 |  TrainLoss: 0.00005 | TestLoss: 1.18042 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 312 |  TrainLoss: 0.00009 | TestLoss: 1.18158 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 313 |  TrainLoss: 0.00003 | TestLoss: 1.18298 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 314 |  TrainLoss: 0.00003 | TestLoss: 1.18432 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 315 |  TrainLoss: 0.00007 | TestLoss: 1.18549 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 316 |  TrainLoss: 0.00003 | TestLoss: 1.18643 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 317 |  TrainLoss: 0.00004 | TestLoss: 1.18692 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 318 |  TrainLoss: 0.00005 | TestLoss: 1.18831 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 319 |  TrainLoss: 0.00005 | TestLoss: 1.18912 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 320 |  TrainLoss: 0.00003 | TestLoss: 1.19015 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 321 |  TrainLoss: 0.00004 | TestLoss: 1.19129 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 322 |  TrainLoss: 0.00003 | TestLoss: 1.19253 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 323 |  TrainLoss: 0.00004 | TestLoss: 1.19467 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 324 |  TrainLoss: 0.00006 | TestLoss: 1.19496 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 325 |  TrainLoss: 0.00004 | TestLoss: 1.19488 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 326 |  TrainLoss: 0.00004 | TestLoss: 1.19476 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 327 |  TrainLoss: 0.00003 | TestLoss: 1.19463 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 328 |  TrainLoss: 0.00004 | TestLoss: 1.19471 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 329 |  TrainLoss: 0.00003 | TestLoss: 1.19484 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 330 |  TrainLoss: 0.00003 | TestLoss: 1.19444 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 331 |  TrainLoss: 0.00006 | TestLoss: 1.19436 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 332 |  TrainLoss: 0.00005 | TestLoss: 1.19333 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 333 |  TrainLoss: 0.00009 | TestLoss: 1.18911 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 334 |  TrainLoss: 0.00005 | TestLoss: 1.18593 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 335 |  TrainLoss: 0.00005 | TestLoss: 1.18209 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 336 |  TrainLoss: 0.00003 | TestLoss: 1.17904 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 337 |  TrainLoss: 0.00004 | TestLoss: 1.17700 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 338 |  TrainLoss: 0.00002 | TestLoss: 1.17578 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 339 |  TrainLoss: 0.00005 | TestLoss: 1.17673 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 340 |  TrainLoss: 0.00004 | TestLoss: 1.17765 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 341 |  TrainLoss: 0.00008 | TestLoss: 1.18131 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 342 |  TrainLoss: 0.00002 | TestLoss: 1.18486 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 343 |  TrainLoss: 0.00002 | TestLoss: 1.18785 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 344 |  TrainLoss: 0.00004 | TestLoss: 1.19106 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 345 |  TrainLoss: 0.00003 | TestLoss: 1.19409 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 346 |  TrainLoss: 0.00002 | TestLoss: 1.19701 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 347 |  TrainLoss: 0.00013 | TestLoss: 1.20518 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 348 |  TrainLoss: 0.00004 | TestLoss: 1.21210 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 349 |  TrainLoss: 0.00003 | TestLoss: 1.21819 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 350 |  TrainLoss: 0.00003 | TestLoss: 1.22397 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 351 |  TrainLoss: 0.00004 | TestLoss: 1.22857 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 352 |  TrainLoss: 0.00003 | TestLoss: 1.23290 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 353 |  TrainLoss: 0.00005 | TestLoss: 1.23635 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 354 |  TrainLoss: 0.00004 | TestLoss: 1.23985 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 355 |  TrainLoss: 0.00004 | TestLoss: 1.24235 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 356 |  TrainLoss: 0.00003 | TestLoss: 1.24440 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 357 |  TrainLoss: 0.00005 | TestLoss: 1.24525 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 358 |  TrainLoss: 0.00003 | TestLoss: 1.24532 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 359 |  TrainLoss: 0.00004 | TestLoss: 1.24556 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 360 |  TrainLoss: 0.00004 | TestLoss: 1.24571 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 361 |  TrainLoss: 0.00005 | TestLoss: 1.24337 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 362 |  TrainLoss: 0.00003 | TestLoss: 1.24050 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 363 |  TrainLoss: 0.00003 | TestLoss: 1.23733 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 364 |  TrainLoss: 0.00003 | TestLoss: 1.23463 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 365 |  TrainLoss: 0.00003 | TestLoss: 1.23129 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 366 |  TrainLoss: 0.00003 | TestLoss: 1.22781 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 367 |  TrainLoss: 0.00002 | TestLoss: 1.22469 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 368 |  TrainLoss: 0.00005 | TestLoss: 1.22225 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 369 |  TrainLoss: 0.00004 | TestLoss: 1.21932 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 370 |  TrainLoss: 0.00002 | TestLoss: 1.21746 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 371 |  TrainLoss: 0.00004 | TestLoss: 1.21738 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 372 |  TrainLoss: 0.00004 | TestLoss: 1.21664 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 373 |  TrainLoss: 0.00003 | TestLoss: 1.21583 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 374 |  TrainLoss: 0.00002 | TestLoss: 1.21686 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 375 |  TrainLoss: 0.00003 | TestLoss: 1.21627 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 376 |  TrainLoss: 0.00004 | TestLoss: 1.21646 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 377 |  TrainLoss: 0.00003 | TestLoss: 1.21691 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 378 |  TrainLoss: 0.00003 | TestLoss: 1.21726 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 379 |  TrainLoss: 0.00001 | TestLoss: 1.21809 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 380 |  TrainLoss: 0.00003 | TestLoss: 1.21873 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 381 |  TrainLoss: 0.00002 | TestLoss: 1.21899 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 382 |  TrainLoss: 0.00003 | TestLoss: 1.21885 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 383 |  TrainLoss: 0.00004 | TestLoss: 1.21889 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 384 |  TrainLoss: 0.00002 | TestLoss: 1.21868 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 385 |  TrainLoss: 0.00003 | TestLoss: 1.21836 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 386 |  TrainLoss: 0.00005 | TestLoss: 1.21742 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 387 |  TrainLoss: 0.00005 | TestLoss: 1.21505 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 388 |  TrainLoss: 0.00004 | TestLoss: 1.21546 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 389 |  TrainLoss: 0.00003 | TestLoss: 1.21571 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 390 |  TrainLoss: 0.00002 | TestLoss: 1.21617 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 391 |  TrainLoss: 0.00002 | TestLoss: 1.21820 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 392 |  TrainLoss: 0.00004 | TestLoss: 1.22060 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 393 |  TrainLoss: 0.00003 | TestLoss: 1.22374 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 394 |  TrainLoss: 0.00003 | TestLoss: 1.22669 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 395 |  TrainLoss: 0.00003 | TestLoss: 1.22942 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 396 |  TrainLoss: 0.00002 | TestLoss: 1.23193 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 397 |  TrainLoss: 0.00002 | TestLoss: 1.23429 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 398 |  TrainLoss: 0.00003 | TestLoss: 1.23569 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 399 |  TrainLoss: 0.00003 | TestLoss: 1.23714 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 400 |  TrainLoss: 0.00005 | TestLoss: 1.24084 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 401 |  TrainLoss: 0.00006 | TestLoss: 1.24650 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 402 |  TrainLoss: 0.00002 | TestLoss: 1.25168 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 403 |  TrainLoss: 0.00003 | TestLoss: 1.25726 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 404 |  TrainLoss: 0.00005 | TestLoss: 1.26034 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 405 |  TrainLoss: 0.00004 | TestLoss: 1.26402 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 406 |  TrainLoss: 0.00002 | TestLoss: 1.26724 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 407 |  TrainLoss: 0.00003 | TestLoss: 1.26967 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 408 |  TrainLoss: 0.00002 | TestLoss: 1.27188 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 409 |  TrainLoss: 0.00004 | TestLoss: 1.27236 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 410 |  TrainLoss: 0.00003 | TestLoss: 1.27234 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 411 |  TrainLoss: 0.00013 | TestLoss: 1.26664 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 412 |  TrainLoss: 0.00001 | TestLoss: 1.26116 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 413 |  TrainLoss: 0.00005 | TestLoss: 1.25706 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 414 |  TrainLoss: 0.00002 | TestLoss: 1.25403 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 415 |  TrainLoss: 0.00003 | TestLoss: 1.24970 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 416 |  TrainLoss: 0.00003 | TestLoss: 1.24777 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 417 |  TrainLoss: 0.00003 | TestLoss: 1.24546 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 418 |  TrainLoss: 0.00005 | TestLoss: 1.24643 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 419 |  TrainLoss: 0.00003 | TestLoss: 1.24636 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 420 |  TrainLoss: 0.00003 | TestLoss: 1.24731 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 421 |  TrainLoss: 0.00003 | TestLoss: 1.24770 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 422 |  TrainLoss: 0.00002 | TestLoss: 1.24889 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 423 |  TrainLoss: 0.00002 | TestLoss: 1.24856 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 424 |  TrainLoss: 0.00005 | TestLoss: 1.25084 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 425 |  TrainLoss: 0.00003 | TestLoss: 1.25345 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 426 |  TrainLoss: 0.00003 | TestLoss: 1.25588 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 427 |  TrainLoss: 0.00002 | TestLoss: 1.25781 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 428 |  TrainLoss: 0.00002 | TestLoss: 1.25977 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 429 |  TrainLoss: 0.00002 | TestLoss: 1.26170 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 430 |  TrainLoss: 0.00002 | TestLoss: 1.26270 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 431 |  TrainLoss: 0.00002 | TestLoss: 1.26376 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 432 |  TrainLoss: 0.00002 | TestLoss: 1.26466 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 433 |  TrainLoss: 0.00006 | TestLoss: 1.26238 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 434 |  TrainLoss: 0.00002 | TestLoss: 1.25954 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 435 |  TrainLoss: 0.00002 | TestLoss: 1.25840 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 436 |  TrainLoss: 0.00002 | TestLoss: 1.25708 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 437 |  TrainLoss: 0.00002 | TestLoss: 1.25596 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 438 |  TrainLoss: 0.00002 | TestLoss: 1.25516 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 439 |  TrainLoss: 0.00002 | TestLoss: 1.25512 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 440 |  TrainLoss: 0.00002 | TestLoss: 1.25497 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 441 |  TrainLoss: 0.00001 | TestLoss: 1.25488 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 442 |  TrainLoss: 0.00002 | TestLoss: 1.25597 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 443 |  TrainLoss: 0.00003 | TestLoss: 1.25765 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 444 |  TrainLoss: 0.00002 | TestLoss: 1.25910 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 445 |  TrainLoss: 0.00011 | TestLoss: 1.26917 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 446 |  TrainLoss: 0.00001 | TestLoss: 1.27900 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 447 |  TrainLoss: 0.00005 | TestLoss: 1.28863 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 448 |  TrainLoss: 0.00002 | TestLoss: 1.29709 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 449 |  TrainLoss: 0.00002 | TestLoss: 1.30455 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 450 |  TrainLoss: 0.00003 | TestLoss: 1.30943 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 451 |  TrainLoss: 0.00002 | TestLoss: 1.31400 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 452 |  TrainLoss: 0.00002 | TestLoss: 1.31694 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 453 |  TrainLoss: 0.00003 | TestLoss: 1.31874 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 454 |  TrainLoss: 0.00001 | TestLoss: 1.32012 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 455 |  TrainLoss: 0.00002 | TestLoss: 1.32109 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 456 |  TrainLoss: 0.00002 | TestLoss: 1.32135 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 457 |  TrainLoss: 0.00002 | TestLoss: 1.32202 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 458 |  TrainLoss: 0.00003 | TestLoss: 1.32110 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 459 |  TrainLoss: 0.00003 | TestLoss: 1.31899 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 460 |  TrainLoss: 0.00002 | TestLoss: 1.31630 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 461 |  TrainLoss: 0.00002 | TestLoss: 1.31266 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 462 |  TrainLoss: 0.00002 | TestLoss: 1.30976 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 463 |  TrainLoss: 0.00004 | TestLoss: 1.30780 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 464 |  TrainLoss: 0.00003 | TestLoss: 1.30301 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 465 |  TrainLoss: 0.00003 | TestLoss: 1.29939 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 466 |  TrainLoss: 0.00005 | TestLoss: 1.29419 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 467 |  TrainLoss: 0.00003 | TestLoss: 1.28840 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 468 |  TrainLoss: 0.00001 | TestLoss: 1.28422 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 469 |  TrainLoss: 0.00001 | TestLoss: 1.28002 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 470 |  TrainLoss: 0.00002 | TestLoss: 1.27611 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 471 |  TrainLoss: 0.00001 | TestLoss: 1.27312 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 472 |  TrainLoss: 0.00001 | TestLoss: 1.27030 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 473 |  TrainLoss: 0.00002 | TestLoss: 1.26809 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 474 |  TrainLoss: 0.00003 | TestLoss: 1.26676 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 475 |  TrainLoss: 0.00002 | TestLoss: 1.26629 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 476 |  TrainLoss: 0.00001 | TestLoss: 1.26599 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 477 |  TrainLoss: 0.00004 | TestLoss: 1.26762 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 478 |  TrainLoss: 0.00001 | TestLoss: 1.26936 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 479 |  TrainLoss: 0.00001 | TestLoss: 1.27103 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 480 |  TrainLoss: 0.00002 | TestLoss: 1.27284 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 481 |  TrainLoss: 0.00001 | TestLoss: 1.27453 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 482 |  TrainLoss: 0.00001 | TestLoss: 1.27595 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 483 |  TrainLoss: 0.00001 | TestLoss: 1.27740 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 484 |  TrainLoss: 0.00002 | TestLoss: 1.27860 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 485 |  TrainLoss: 0.00002 | TestLoss: 1.27980 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 486 |  TrainLoss: 0.00002 | TestLoss: 1.28121 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 487 |  TrainLoss: 0.00002 | TestLoss: 1.28329 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 488 |  TrainLoss: 0.00003 | TestLoss: 1.28525 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 489 |  TrainLoss: 0.00002 | TestLoss: 1.28828 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 490 |  TrainLoss: 0.00002 | TestLoss: 1.29131 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 491 |  TrainLoss: 0.00002 | TestLoss: 1.29308 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 492 |  TrainLoss: 0.00002 | TestLoss: 1.29423 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 493 |  TrainLoss: 0.00002 | TestLoss: 1.29619 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 494 |  TrainLoss: 0.00002 | TestLoss: 1.29701 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 495 |  TrainLoss: 0.00004 | TestLoss: 1.30058 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 496 |  TrainLoss: 0.00002 | TestLoss: 1.30396 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 497 |  TrainLoss: 0.00001 | TestLoss: 1.30596 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 498 |  TrainLoss: 0.00004 | TestLoss: 1.30888 | TestAcc: 0.84615 | TestF1: 0.84\n",
            "Epoch: 499 |  TrainLoss: 0.00002 | TestLoss: 1.31148 | TestAcc: 0.84615 | TestF1: 0.84\n"
          ]
        }
      ],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "\n",
        "# without droupout training results\n",
        "for epoch in range(500):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  # weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(test_loss/ len(test_loader.dataset))\n",
        "\n",
        "  # wloss.append(weighted_loss/(1-exp_param**(epoch+1)))\n",
        "\n",
        "  # if(epoch-20>=0 and wloss[epoch-6]-weighted_loss<0.01):\n",
        "  #     print(\"Stopped Early at Epoch {} \".format(epoch))\n",
        "  #     break\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "          f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "  # print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.7f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Metrics found after training \n",
        "**Epoch: 49 |  TrainLoss: 0.19083 | TestLoss: 0.39260 | TestAcc: 0.85068 | TestF1: 0.85**\n",
        "\n",
        "*Note : We choose the best epoch based on lowest Test Loss.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wloss = []\n",
        "weighted_loss = 0\n",
        "exp_param = 0.8\n",
        "\n",
        "for epoch in range(400):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  # weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(test_loss/ len(test_loader.dataset))\n",
        "\n",
        "  # wloss.append(weighted_loss/(1-exp_param**(epoch+1)))\n",
        "\n",
        "  # if(epoch-20>=0 and wloss[epoch-6]-weighted_loss<0.01):\n",
        "  #     print(\"Stopped Early at Epoch {} \".format(epoch))\n",
        "  #     break\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "          f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}')\n",
        "  # print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.7f} |')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.12 ('torch-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
