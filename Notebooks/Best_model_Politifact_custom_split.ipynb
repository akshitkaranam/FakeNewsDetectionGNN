{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0NGBxXcVj4Z"
      },
      "source": [
        "# Installlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcSe3ryFVgv3",
        "outputId": "793e93d6-7d74-4bee-cefa-d4fc5a5bf91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp38-cp38-macosx_11_0_arm64.whl size=247022 sha256=df2640db7b8f9f14e3a28f7c1debd6561ebb30e1ce7f6cf9fa63750a55e297db\n",
            "  Stored in directory: /Users/pratham/Library/Caches/pip/wheels/7c/51/2a/409339f45a48bf748a5db76dfa11373ea7c883ecf1932eee2f\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://data.pyg.org/whl/torch-+.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.15.tar.gz (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-sparse) (1.9.0)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from scipy->torch-sparse) (1.23.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for torch-sparse: filename=torch_sparse-0.6.15-cp38-cp38-macosx_11_0_arm64.whl size=459763 sha256=3660e5612f5276e955aee8d9782a277751bac4ed59e1e73f4dca0cd3c8014004\n",
            "  Stored in directory: /Users/pratham/Library/Caches/pip/wheels/3c/4e/f2/2d7bcec11f628aa3070dceb8fcdfb84907c4f745f213e49dab\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.0/467.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (1.23.2)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (1.9.0)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (2.28.1)\n",
            "Requirement already satisfied: pyparsing in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from torch-geometric) (3.0.9)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.1.2-cp38-cp38-macosx_12_0_arm64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from requests->torch-geometric) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from requests->torch-geometric) (2022.6.15)\n",
            "Collecting joblib>=1.0.0\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689840 sha256=16dc9e6abc40c0f6236937a4ad1a9721a7d3175fc5008e82ce5cf933c09d7dc6\n",
            "  Stored in directory: /Users/pratham/Library/Caches/pip/wheels/99/cf/87/0e3b43df44e007139c943e336304552d8f76e9fdd0c0e842ae\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn, torch-geometric\n",
            "Successfully installed joblib-1.2.0 scikit-learn-1.1.2 threadpoolctl-3.1.0 torch-geometric-2.1.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  \n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "!pip install torch-geometric\n",
        "#!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep-VMvdQVnZM",
        "outputId": "d8daf464-f6d6-4c0f-f488-67e2f7a24f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.9.0,>=1.7.0\n",
            "  Downloading scipy-1.8.1-cp38-cp38-macosx_12_0_arm64.whl (28.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.6/28.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting sqlalchemy>=1.3.0\n",
            "  Downloading SQLAlchemy-1.4.41.tar.gz (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from optuna) (1.23.2)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from optuna) (4.64.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting cliff\n",
            "  Downloading cliff-4.0.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata<5.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from optuna) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: importlib-resources in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from alembic>=1.5.0->optuna) (5.9.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from importlib-metadata<5.0.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.1/147.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-4.0.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m999.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
            "  Downloading prettytable-3.4.1-py3-none-any.whl (26 kB)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
            "Building wheels for collected packages: sqlalchemy, pyperclip\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.41-cp38-cp38-macosx_11_0_arm64.whl size=1549831 sha256=9ebb36a4f41734e629a940afeae3f31ba04fdb072c04b5d52f0151d8cda24ec3\n",
            "  Stored in directory: /Users/pratham/Library/Caches/pip/wheels/6a/b0/55/4dc3d0b964f7934215c73af83b8f9cf47bf59e2642dbfa8bd6\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11123 sha256=a6518cacf11c43f90f5863ba9ebb9fccafeb7aed6cd03e5f27231bcc920af464\n",
            "  Stored in directory: /Users/pratham/Library/Caches/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built sqlalchemy pyperclip\n",
            "Installing collected packages: pyperclip, sqlalchemy, scipy, PrettyTable, pbr, Mako, colorlog, cmd2, cmaes, autopage, stevedore, alembic, cliff, optuna\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.9.0\n",
            "    Uninstalling scipy-1.9.0:\n",
            "      Successfully uninstalled scipy-1.9.0\n",
            "Successfully installed Mako-1.2.3 PrettyTable-3.4.1 alembic-1.8.1 autopage-0.5.1 cliff-4.0.0 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.10.0 pyperclip-1.8.2 scipy-1.8.1 sqlalchemy-1.4.41 stevedore-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlNCSrUqVq9_"
      },
      "source": [
        "# Model definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fmUK6R5FVpah"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import UPFD\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AyeWEOJAWKnW"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import ReLU, LeakyReLU, Softmax, Linear, SELU, GELU, ELU,Dropout\n",
        "from torch_geometric.nn import SAGEConv, global_max_pool, TopKPooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IZ3ZOPO7ZQg6"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import accuracy_score, f1_score "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFNw7_rmV4ec",
        "outputId": "054ad4b8-063a-45ba-f521-7a1a7e7c8247"
      },
      "outputs": [],
      "source": [
        "test_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\",split=\"train\")\n",
        "train_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"test\")\n",
        "val_data_pol = UPFD(root=\".\", name=\"politifact\", feature=\"bert\", split=\"val\")\n",
        "# train_data_pol = train_data_pol + val_data_pol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tAaqMY3rV8E7"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data_pol, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data_pol, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data_pol, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bi29BEZXYD-Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels[0])\n",
        "        self.conv2 = SAGEConv(hidden_channels[0], hidden_channels[1])\n",
        "        self.conv3 = SAGEConv(hidden_channels[1], hidden_channels[2])\n",
        "        \n",
        "        self.full1 = Linear(hidden_channels[2],hidden_channels[3])\n",
        "        self.full2 = Linear(hidden_channels[3],hidden_channels[4])\n",
        "        self.full3 = Linear(hidden_channels[4],hidden_channels[5])\n",
        "\n",
        "        self.full4 = Linear(hidden_channels[5],out_channels)\n",
        "\n",
        "        #droupouts\n",
        "        self.dp1 = Dropout(0.2)\n",
        "        self.dp2 = Dropout(0.2)\n",
        "        self.dp3 = Dropout(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h = self.conv1(x, edge_index).relu()\n",
        "        h = self.conv2(h, edge_index).relu()\n",
        "        h = self.conv3(h, edge_index).relu()\n",
        "\n",
        "        h = global_max_pool(h,batch)\n",
        "\n",
        "        h = self.full1(h).relu()\n",
        "        h = self.dp1(h)\n",
        "        h = self.full2(h).relu()\n",
        "        h = self.dp2(h)\n",
        "        h = self.full3(h).relu()\n",
        "        h = self.dp3(h)\n",
        "        \n",
        "        h = self.full4(h)\n",
        "\n",
        "        return torch.sigmoid(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XI5gL3-_edt-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyper Parameter Tuning\n",
        "\n",
        "- Number neurons in each layer\n",
        "- learning rate\n",
        "- Beta 1 of Adam Optimize also called momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1v_ZxrlHZZMF"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  \n",
        "  model = Net(768,[trial.suggest_categorical(name=\"layer_size1\",choices = [256,512]),\n",
        "                   trial.suggest_categorical(name=\"layer_size2\",choices = [256,512]),\n",
        "                   trial.suggest_categorical(name=\"layer_size3\",choices = [256,512]),\n",
        "                   trial.suggest_categorical(name=\"layer_size4\",choices = [64,128,512]),\n",
        "                   trial.suggest_categorical(name=\"layer_size5\",choices = [64,128,512]),\n",
        "                   trial.suggest_categorical(name=\"layer_size6\",choices = [64,128,512]),\n",
        "\n",
        "  \n",
        "  ],1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3),0.99))\n",
        "  lossff = torch.nn.BCELoss()\n",
        "\n",
        "  total_loss = 0\n",
        "  weighted_loss = 0\n",
        "  exp_param = 0.8\n",
        "\n",
        "  wloss = []\n",
        "\n",
        "  for i in range(600):\n",
        "    print(\"Epoch:\", i)\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "          data = data.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += float(loss) * data.num_graphs\n",
        "    print(\"Train: \",total_loss / len(train_loader.dataset))\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for data in val_loader:\n",
        "          data = data.to(device)\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "          total_loss += float(loss) * data.num_graphs\n",
        "          \n",
        "    print(\"Test\", total_loss / len(val_loader.dataset))\n",
        "\n",
        "    weighted_loss = exp_param*(weighted_loss) + (1-exp_param)*(total_loss/ len(val_loader.dataset))\n",
        "    print(weighted_loss/(1-exp_param**(i+1)))\n",
        "    wloss.append(weighted_loss/(1-exp_param**(i+1)))\n",
        "\n",
        "    if(i-30>=0 and wloss[i-20]-weighted_loss<0.01):\n",
        "      break\n",
        "\n",
        "  return weighted_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKKZ-PZTi3Q8",
        "outputId": "bd6546f8-095c-495e-b7ab-d2ad152d1df5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 05:56:23,941]\u001b[0m A new study created in memory with name: no-name-fc3ffb2f-514b-429a-ab80-f9de943b2a38\u001b[0m\n",
            "/var/folders/5r/z92hbcf14gx38w5z2nf1210w0000gn/T/ipykernel_53308/466619184.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
            "  optimizer = torch.optim.Adam(model.parameters(),lr=trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),betas=(trial.suggest_loguniform('b1', 1-1e-1,1-1e-3),0.99))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Train:  0.6932932847765236\n",
            "Test 0.6980131268501282\n",
            "0.6980131268501282\n",
            "Epoch: 1\n",
            "Train:  0.7915146437165964\n",
            "Test 0.6963661909103394\n",
            "0.6970981624391345\n",
            "Epoch: 2\n",
            "Train:  0.7914332577545719\n",
            "Test 0.6951037049293518\n",
            "0.696280761820371\n",
            "Epoch: 3\n",
            "Train:  0.7899675614693585\n",
            "Test 0.6941914558410645\n",
            "0.6955730023422861\n",
            "Epoch: 4\n",
            "Train:  0.7892284474221829\n",
            "Test 0.6931468844413757\n",
            "0.6948512871171223\n",
            "Epoch: 5\n",
            "Train:  0.7901976664141832\n",
            "Test 0.6920807957649231\n",
            "0.694100329924337\n",
            "Epoch: 6\n",
            "Train:  0.7888829333210423\n",
            "Test 0.6910011768341064\n",
            "0.6933160169550806\n",
            "Epoch: 7\n",
            "Train:  0.7875912728892193\n",
            "Test 0.6901895999908447\n",
            "0.6925646801662907\n",
            "Epoch: 8\n",
            "Train:  0.7871230943170608\n",
            "Test 0.6895183324813843\n",
            "0.6918609587380685\n",
            "Epoch: 9\n",
            "Train:  0.7867448405442734\n",
            "Test 0.6888068914413452\n",
            "0.6911766703752393\n",
            "Epoch: 10\n",
            "Train:  0.7856241208395807\n",
            "Test 0.6880888342857361\n",
            "0.690501069482554\n",
            "Epoch: 11\n",
            "Train:  0.7835118258161242\n",
            "Test 0.6872237324714661\n",
            "0.6897972349387794\n",
            "Epoch: 12\n",
            "Train:  0.7842600820291096\n",
            "Test 0.686354398727417\n",
            "0.6890686111818564\n",
            "Epoch: 13\n",
            "Train:  0.7830863497915311\n",
            "Test 0.6855672597885132\n",
            "0.6883361258579556\n",
            "Epoch: 14\n",
            "Train:  0.7821267775820391\n",
            "Test 0.6846538782119751\n",
            "0.6875728198874765\n",
            "Epoch: 15\n",
            "Train:  0.7822408276985134\n",
            "Test 0.6836551427841187\n",
            "0.6867665911451583\n",
            "Epoch: 16\n",
            "Train:  0.7801117222773004\n",
            "Test 0.682669460773468\n",
            "0.685928288165895\n",
            "Epoch: 17\n",
            "Train:  0.7783011818363655\n",
            "Test 0.6815593838691711\n",
            "0.6850384779100028\n",
            "Epoch: 18\n",
            "Train:  0.7771750998712773\n",
            "Test 0.6802608370780945\n",
            "0.6840689777741797\n",
            "Epoch: 19\n",
            "Train:  0.7753923257551582\n",
            "Test 0.678874135017395\n",
            "0.6830178910175738\n",
            "Epoch: 20\n",
            "Train:  0.7742572612891909\n",
            "Test 0.6774029731750488\n",
            "0.6818844533316105\n",
            "Epoch: 21\n",
            "Train:  0.7702551254859338\n",
            "Test 0.6756672859191895\n",
            "0.6806317767272352\n",
            "Epoch: 22\n",
            "Train:  0.7677336122115813\n",
            "Test 0.673876941204071\n",
            "0.6792727875665234\n",
            "Epoch: 23\n",
            "Train:  0.7695202285348021\n",
            "Test 0.6719929575920105\n",
            "0.6778099133434209\n",
            "Epoch: 24\n",
            "Train:  0.7653403241709886\n",
            "Test 0.6699152588844299\n",
            "0.6762249947986516\n",
            "Epoch: 25\n",
            "Train:  0.762876833995543\n",
            "Test 0.6677294969558716\n",
            "0.67452074444947\n",
            "Epoch: 26\n",
            "Train:  0.7615832095771893\n",
            "Test 0.6654097437858582\n",
            "0.672694127828757\n",
            "Epoch: 27\n",
            "Train:  0.7575700366658862\n",
            "Test 0.6629695296287537\n",
            "0.6707454388761336\n",
            "Epoch: 28\n",
            "Train:  0.7544721904383526\n",
            "Test 0.6606171727180481\n",
            "0.6687166462399718\n",
            "Epoch: 29\n",
            "Train:  0.7527539862227116\n",
            "Test 0.6583564281463623\n",
            "0.6666420343761577\n",
            "Epoch: 30\n",
            "Train:  0.7487004067563363\n",
            "Test 0.6558398604393005\n",
            "0.6644794578767569\n",
            "Epoch: 31\n",
            "Train:  0.7450612961436829\n",
            "Test 0.6531919240951538\n",
            "0.6622201611211311\n",
            "Epoch: 32\n",
            "Train:  0.7418122839064619\n",
            "Test 0.6504567265510559\n",
            "0.6598659820688717\n",
            "Epoch: 33\n",
            "Train:  0.7375813852068526\n",
            "Test 0.6473872661590576\n",
            "0.6573689727527655\n",
            "Epoch: 34\n",
            "Train:  0.7327337432231299\n",
            "Test 0.6441380977630615\n",
            "0.6547217239031148\n",
            "Epoch: 35\n",
            "Train:  0.7272297435216775\n",
            "Test 0.6404626965522766\n",
            "0.651868992668733\n",
            "Epoch: 36\n",
            "Train:  0.723328791053047\n",
            "Test 0.6367288827896118\n",
            "0.6488401843693179\n",
            "Epoch: 37\n",
            "Train:  0.7184530011129595\n",
            "Test 0.6324574947357178\n",
            "0.6455629657909288\n",
            "Epoch: 38\n",
            "Train:  0.711768063215109\n",
            "Test 0.6281225681304932\n",
            "0.6420743066059096\n",
            "Epoch: 39\n",
            "Train:  0.7070893359400029\n",
            "Test 0.6236863136291504\n",
            "0.6383962191088706\n",
            "Epoch: 40\n",
            "Train:  0.70202007282913\n",
            "Test 0.6189917325973511\n",
            "0.63451490907489\n",
            "Epoch: 41\n",
            "Train:  0.6971182483353766\n",
            "Test 0.6143397092819214\n",
            "0.6304795258238554\n",
            "Epoch: 42\n",
            "Train:  0.6907930500906517\n",
            "Test 0.6096031665802002\n",
            "0.626303969801507\n",
            "Epoch: 43\n",
            "Train:  0.6820993067452271\n",
            "Test 0.604694128036499\n",
            "0.6219817661253538\n",
            "Epoch: 44\n",
            "Train:  0.6785649681522835\n",
            "Test 0.5996777415275574\n",
            "0.6175207669018746\n",
            "Epoch: 45\n",
            "Train:  0.674718245391932\n",
            "Test 0.5944576859474182\n",
            "0.6129079899791663\n",
            "Epoch: 46\n",
            "Train:  0.6671658767833969\n",
            "Test 0.5890027284622192\n",
            "0.608126804395775\n",
            "Epoch: 47\n",
            "Train:  0.6540079548348129\n",
            "Test 0.5836359262466431\n",
            "0.603228519530546\n",
            "Epoch: 48\n",
            "Train:  0.6508591533786031\n",
            "Test 0.5778716206550598\n",
            "0.5981570492773961\n",
            "Epoch: 49\n",
            "Train:  0.6405165715994339\n",
            "Test 0.5719063878059387\n",
            "0.5929068420496431\n",
            "Epoch: 50\n",
            "Train:  0.631733640826126\n",
            "Test 0.5653391480445862\n",
            "0.5873932402944287\n",
            "Epoch: 51\n",
            "Train:  0.6234097766660457\n",
            "Test 0.5584085583686829\n",
            "0.5815962509573458\n",
            "Epoch: 52\n",
            "Train:  0.6107340403811424\n",
            "Test 0.5516389608383179\n",
            "0.5756047491505917\n",
            "Epoch: 53\n",
            "Train:  0.6034815570347989\n",
            "Test 0.5446349382400513\n",
            "0.5694107507583284\n",
            "Epoch: 54\n",
            "Train:  0.5983455860776599\n",
            "Test 0.5375925898551941\n",
            "0.563047088816094\n",
            "Epoch: 55\n",
            "Train:  0.5835332358045275\n",
            "Test 0.5306035876274109\n",
            "0.5565583643011567\n",
            "Epoch: 56\n",
            "Train:  0.5727820145598365\n",
            "Test 0.5234179496765137\n",
            "0.5499302615372869\n",
            "Epoch: 57\n",
            "Train:  0.5617613822086904\n",
            "Test 0.5163367390632629\n",
            "0.5432115409543425\n",
            "Epoch: 58\n",
            "Train:  0.5542102483063261\n",
            "Test 0.5090547800064087\n",
            "0.5363801756784597\n",
            "Epoch: 59\n",
            "Train:  0.539360028046828\n",
            "Test 0.5020190477371216\n",
            "0.5295079395585209\n",
            "Epoch: 60\n",
            "Train:  0.5394215169805208\n",
            "Test 0.4953552484512329\n",
            "0.5226773929628376\n",
            "Epoch: 61\n",
            "Train:  0.5156675167213198\n",
            "Test 0.4891281723976135\n",
            "0.5159675422687903\n",
            "Epoch: 62\n",
            "Train:  0.5154382980247428\n",
            "Test 0.48256394267082214\n",
            "0.5092868171072478\n",
            "Epoch: 63\n",
            "Train:  0.5048405565827141\n",
            "Test 0.47577717900276184\n",
            "0.5025848852794798\n",
            "Epoch: 64\n",
            "Train:  0.4942361048983233\n",
            "Test 0.46883371472358704\n",
            "0.49583464777854697\n",
            "Epoch: 65\n",
            "Train:  0.48743729960864485\n",
            "Test 0.46166208386421204\n",
            "0.48900013225001926\n",
            "Epoch: 66\n",
            "Train:  0.4796707866148711\n",
            "Test 0.45448049902915955\n",
            "0.4820962033870101\n",
            "Epoch: 67\n",
            "Train:  0.4771496985293082\n",
            "Test 0.44780758023262024\n",
            "0.4752384769929415\n",
            "Epoch: 68\n",
            "Train:  0.45512640975179713\n",
            "Test 0.44124922156333923\n",
            "0.46844062450878393\n",
            "Epoch: 69\n",
            "Train:  0.45196311816370865\n",
            "Test 0.4355716109275818\n",
            "0.4618668207108211\n",
            "Epoch: 70\n",
            "Train:  0.43010934432167813\n",
            "Test 0.4298606514930725\n",
            "0.4554655860246105\n",
            "Epoch: 71\n",
            "Train:  0.42584734372963196\n",
            "Test 0.42445045709609985\n",
            "0.4492625595856535\n",
            "Epoch: 72\n",
            "Train:  0.4250030019973738\n",
            "Test 0.4195750951766968\n",
            "0.44332506620362927\n",
            "Epoch: 73\n",
            "Train:  0.3995609878145192\n",
            "Test 0.41505926847457886\n",
            "0.437671906276797\n",
            "Epoch: 74\n",
            "Train:  0.3985512511492854\n",
            "Test 0.4110000729560852\n",
            "0.4323375393250261\n",
            "Epoch: 75\n",
            "Train:  0.39538592181054716\n",
            "Test 0.4070870876312256\n",
            "0.4272874487684257\n",
            "Epoch: 76\n",
            "Train:  0.37629418702147127\n",
            "Test 0.40330708026885986\n",
            "0.4224913749030062\n",
            "Epoch: 77\n",
            "Train:  0.3927415513614724\n",
            "Test 0.4003523588180542\n",
            "0.41806357156377744\n",
            "Epoch: 78\n",
            "Train:  0.38892886035852303\n",
            "Test 0.39753901958465576\n",
            "0.4139586610772938\n",
            "Epoch: 79\n",
            "Train:  0.37406504046323613\n",
            "Test 0.39506208896636963\n",
            "0.4101793465883342\n",
            "Epoch: 80\n",
            "Train:  0.3529556430033429\n",
            "Test 0.392734557390213\n",
            "0.4066903886993944\n",
            "Epoch: 81\n",
            "Train:  0.35339479067476626\n",
            "Test 0.390961617231369\n",
            "0.4035446343702177\n",
            "Epoch: 82\n",
            "Train:  0.33620036599053515\n",
            "Test 0.3894292712211609\n",
            "0.4007215617148681\n",
            "Epoch: 83\n",
            "Train:  0.3440312702057049\n",
            "Test 0.38833630084991455\n",
            "0.39824450952395096\n",
            "Epoch: 84\n",
            "Train:  0.32171208839610693\n",
            "Test 0.38710322976112366\n",
            "0.3960162535584848\n",
            "Epoch: 85\n",
            "Train:  0.3305197279917169\n",
            "Test 0.3862431049346924\n",
            "0.3940616238246731\n",
            "Epoch: 86\n",
            "Train:  0.32870968218842245\n",
            "Test 0.385629266500473\n",
            "0.3923751523535841\n",
            "Epoch: 87\n",
            "Train:  0.31957455741334284\n",
            "Test 0.38521307706832886\n",
            "0.390942737292287\n",
            "Epoch: 88\n",
            "Train:  0.3192672136817043\n",
            "Test 0.384965181350708\n",
            "0.3897472261011361\n",
            "Epoch: 89\n",
            "Train:  0.33237545131558205\n",
            "Test 0.3849247992038727\n",
            "0.3887827407198537\n",
            "Epoch: 90\n",
            "Train:  0.3147998600523936\n",
            "Test 0.385011225938797\n",
            "0.3880284377624975\n",
            "Epoch: 91\n",
            "Train:  0.29831945781524366\n",
            "Test 0.38518866896629333\n",
            "0.3874604840025671\n",
            "Epoch: 92\n",
            "Train:  0.3161769311082849\n",
            "Test 0.38536152243614197\n",
            "0.3870406916888742\n",
            "Epoch: 93\n",
            "Train:  0.3141965367810219\n",
            "Test 0.3856228291988373\n",
            "0.3867571191906465\n",
            "Epoch: 94\n",
            "Train:  0.3097583070329951\n",
            "Test 0.38596946001052856\n",
            "0.386599587354525\n",
            "Epoch: 95\n",
            "Train:  0.30418745654201074\n",
            "Test 0.3865478038787842\n",
            "0.3865892306593717\n",
            "Epoch: 96\n",
            "Train:  0.27902413965350364\n",
            "Test 0.386564701795578\n",
            "0.38658432488661104\n",
            "Epoch: 97\n",
            "Train:  0.30538729170328893\n",
            "Test 0.38655713200569153\n",
            "0.3865788863104254\n",
            "Epoch: 98\n",
            "Train:  0.29202059497930344\n",
            "Test 0.38657811284065247\n",
            "0.3865787316164708\n",
            "Epoch: 99\n",
            "Train:  0.28929192353697386\n",
            "Test 0.38681116700172424\n",
            "0.38662521869353095\n",
            "Epoch: 100\n",
            "Train:  0.2750207516132976\n",
            "Test 0.38731008768081665\n",
            "0.3867621924910104\n",
            "Epoch: 101\n",
            "Train:  0.2879799411577337\n",
            "Test 0.3879541754722595\n",
            "0.38700058908729135\n",
            "Epoch: 102\n",
            "Train:  0.29675369618704955\n",
            "Test 0.3878924548625946\n",
            "0.38717896224237064\n",
            "Epoch: 103\n",
            "Train:  0.2764376393135856\n",
            "Test 0.3878856897354126\n",
            "0.38732030774099085\n",
            "Epoch: 104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:08:00,123]\u001b[0m Trial 0 finished with value: 0.38741885250162544 and parameters: {'layer_size1': 256, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 512, 'layer_size5': 128, 'layer_size6': 64, 'learning_rate': 3.28934533167165e-05, 'b1': 0.998112210667536}. Best is trial 0 with value: 0.38741885250162544.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.2563836398572404\n",
            "Test 0.3878130316734314\n",
            "0.3874188525274855\n",
            "Epoch: 0\n",
            "Train:  0.6948622343766743\n",
            "Test 0.6984020471572876\n",
            "0.6984020471572876\n",
            "Epoch: 1\n",
            "Train:  0.7927552564111771\n",
            "Test 0.6982921361923218\n",
            "0.6983409855100845\n",
            "Epoch: 2\n",
            "Train:  0.7925487592209518\n",
            "Test 0.6981660723686218\n",
            "0.6982692997963703\n",
            "Epoch: 3\n",
            "Train:  0.7918210660710054\n",
            "Test 0.6980740427970886\n",
            "0.6982031558264239\n",
            "Epoch: 4\n",
            "Train:  0.7913928433780757\n",
            "Test 0.6979750394821167\n",
            "0.698135296371311\n",
            "Epoch: 5\n",
            "Train:  0.7924287885562327\n",
            "Test 0.6979028582572937\n",
            "0.6980722927191033\n",
            "Epoch: 6\n",
            "Train:  0.7910173778080832\n",
            "Test 0.6978173851966858\n",
            "0.6980077824254933\n",
            "Epoch: 7\n",
            "Train:  0.7914189159600443\n",
            "Test 0.6977236270904541\n",
            "0.697939494554941\n",
            "Epoch: 8\n",
            "Train:  0.7915958802624525\n",
            "Test 0.6976227760314941\n",
            "0.6978663310047761\n",
            "Epoch: 9\n",
            "Train:  0.7915951064808876\n",
            "Test 0.6975158452987671\n",
            "0.6977878018609627\n",
            "Epoch: 10\n",
            "Train:  0.7916395478119138\n",
            "Test 0.6974022388458252\n",
            "0.697703442876333\n",
            "Epoch: 11\n",
            "Train:  0.7911902822520398\n",
            "Test 0.6973013877868652\n",
            "0.6976170983046546\n",
            "Epoch: 12\n",
            "Train:  0.7912487355292652\n",
            "Test 0.6972258687019348\n",
            "0.697534300528213\n",
            "Epoch: 13\n",
            "Train:  0.7911884652543392\n",
            "Test 0.6971247792243958\n",
            "0.6974486283659042\n",
            "Epoch: 14\n",
            "Train:  0.7911111434660346\n",
            "Test 0.6970169544219971\n",
            "0.6973591451669552\n",
            "Epoch: 15\n",
            "Train:  0.7906924931172332\n",
            "Test 0.6969290375709534\n",
            "0.6972706322300699\n",
            "Epoch: 16\n",
            "Train:  0.7908225496430202\n",
            "Test 0.6968345046043396\n",
            "0.6971813973132328\n",
            "Epoch: 17\n",
            "Train:  0.7914244718681094\n",
            "Test 0.6967530250549316\n",
            "0.6970941511748677\n",
            "Epoch: 18\n",
            "Train:  0.7905911911127255\n",
            "Test 0.6966718435287476\n",
            "0.6970084546282502\n",
            "Epoch: 19\n",
            "Train:  0.7920144728945391\n",
            "Test 0.6966027617454529\n",
            "0.6969263696766315\n",
            "Epoch: 20\n",
            "Train:  0.7905215140920958\n",
            "Test 0.696503758430481\n",
            "0.6968410605899568\n",
            "Epoch: 21\n",
            "Train:  0.7910942626754622\n",
            "Test 0.6964154243469238\n",
            "0.6967553005439016\n",
            "Epoch: 22\n",
            "Train:  0.7914679937772622\n",
            "Test 0.6963368654251099\n",
            "0.6966711165857654\n",
            "Epoch: 23\n",
            "Train:  0.7910216998190901\n",
            "Test 0.6962590217590332\n",
            "0.6965883065611341\n",
            "Epoch: 24\n",
            "Train:  0.7914963939071241\n",
            "Test 0.6962084174156189\n",
            "0.6965120406074041\n",
            "Epoch: 25\n",
            "Train:  0.790333169617804\n",
            "Test 0.6961483359336853\n",
            "0.696439079160217\n",
            "Epoch: 26\n",
            "Train:  0.7909438024818628\n",
            "Test 0.6960722804069519\n",
            "0.6963655416066695\n",
            "Epoch: 27\n",
            "Train:  0.7905122235889348\n",
            "Test 0.6959890723228455\n",
            "0.696290101828151\n",
            "Epoch: 28\n",
            "Train:  0.7896771412089939\n",
            "Test 0.695932149887085\n",
            "0.6962184004874871\n",
            "Epoch: 29\n",
            "Train:  0.7904549985449778\n",
            "Test 0.6958783864974976\n",
            "0.6961503134017596\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:08:53,053]\u001b[0m Trial 1 finished with value: 0.695398614409811 and parameters: {'layer_size1': 256, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 128, 'layer_size5': 128, 'layer_size6': 128, 'learning_rate': 3.4142768387675937e-06, 'b1': 0.9250503780212823}. Best is trial 0 with value: 0.38741885250162544.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7907745484852683\n",
            "Test 0.695838987827301\n",
            "0.6960879865613548\n",
            "Epoch: 0\n",
            "Train:  0.6950251516713276\n",
            "Test 0.6994564533233643\n",
            "0.6994564533233643\n",
            "Epoch: 1\n",
            "Train:  0.7924356867824744\n",
            "Test 0.69889235496521\n",
            "0.699143065346612\n",
            "Epoch: 2\n",
            "Train:  0.7925705610357259\n",
            "Test 0.698448657989502\n",
            "0.6988584721674684\n",
            "Epoch: 3\n",
            "Train:  0.7918961805995234\n",
            "Test 0.6980832815170288\n",
            "0.6985958737086474\n",
            "Epoch: 4\n",
            "Train:  0.791691186740927\n",
            "Test 0.697795033454895\n",
            "0.6983576418387781\n",
            "Epoch: 5\n",
            "Train:  0.7919913899844588\n",
            "Test 0.6975086331367493\n",
            "0.6981275132765578\n",
            "Epoch: 6\n",
            "Train:  0.7911236847148222\n",
            "Test 0.6972493529319763\n",
            "0.6979052743205304\n",
            "Epoch: 7\n",
            "Train:  0.7914904868440931\n",
            "Test 0.6967929601669312\n",
            "0.6976379643123481\n",
            "Epoch: 8\n",
            "Train:  0.7903076202621288\n",
            "Test 0.6963974833488464\n",
            "0.6973514070557215\n",
            "Epoch: 9\n",
            "Train:  0.7902111831833335\n",
            "Test 0.6958886981010437\n",
            "0.6970236753397353\n",
            "Epoch: 10\n",
            "Train:  0.7897583672363834\n",
            "Test 0.6951211094856262\n",
            "0.6966074048019959\n",
            "Epoch: 11\n",
            "Train:  0.7900813259690056\n",
            "Test 0.6945551037788391\n",
            "0.6961666566124033\n",
            "Epoch: 12\n",
            "Train:  0.7894870897763455\n",
            "Test 0.6941657662391663\n",
            "0.6957431986924105\n",
            "Epoch: 13\n",
            "Train:  0.7894153290204873\n",
            "Test 0.6936376690864563\n",
            "0.6953027203278762\n",
            "Epoch: 14\n",
            "Train:  0.7885504764129673\n",
            "Test 0.6932230591773987\n",
            "0.6948716201072092\n",
            "Epoch: 15\n",
            "Train:  0.7885837398503162\n",
            "Test 0.6925106048583984\n",
            "0.6943857407699326\n",
            "Epoch: 16\n",
            "Train:  0.7880646346381347\n",
            "Test 0.6917756795883179\n",
            "0.6938517030737624\n",
            "Epoch: 17\n",
            "Train:  0.787296708352965\n",
            "Test 0.6911628246307373\n",
            "0.693304061959892\n",
            "Epoch: 18\n",
            "Train:  0.7867071544962232\n",
            "Test 0.6908227205276489\n",
            "0.692800537115681\n",
            "Epoch: 19\n",
            "Train:  0.7861173314746149\n",
            "Test 0.6901136040687561\n",
            "0.6922568825964329\n",
            "Epoch: 20\n",
            "Train:  0.785917425047758\n",
            "Test 0.6891190409660339\n",
            "0.6916234720895964\n",
            "Epoch: 21\n",
            "Train:  0.7841043199888721\n",
            "Test 0.688642144203186\n",
            "0.6910227741438056\n",
            "Epoch: 22\n",
            "Train:  0.7845511822139516\n",
            "Test 0.6880254149436951\n",
            "0.6904197426340463\n",
            "Epoch: 23\n",
            "Train:  0.7841241462198318\n",
            "Test 0.6871653199195862\n",
            "0.6897657697917229\n",
            "Epoch: 24\n",
            "Train:  0.7819235117187328\n",
            "Test 0.6862693428993225\n",
            "0.6890638325693934\n",
            "Epoch: 25\n",
            "Train:  0.7805527785784518\n",
            "Test 0.6853359937667847\n",
            "0.6883160046376333\n",
            "Epoch: 26\n",
            "Train:  0.7800769088494831\n",
            "Test 0.6843607425689697\n",
            "0.6875230349408192\n",
            "Epoch: 27\n",
            "Train:  0.7797166567582351\n",
            "Test 0.6831033229827881\n",
            "0.686637379442334\n",
            "Epoch: 28\n",
            "Train:  0.7776872515678406\n",
            "Test 0.6823270916938782\n",
            "0.6857739858557798\n",
            "Epoch: 29\n",
            "Train:  0.7765408036935383\n",
            "Test 0.6810983419418335\n",
            "0.6848376980047713\n",
            "Epoch: 30\n",
            "Train:  0.7745102846244881\n",
            "Test 0.6791073083877563\n",
            "0.6836904839355845\n",
            "Epoch: 31\n",
            "Train:  0.7740950098943926\n",
            "Test 0.6772562861442566\n",
            "0.6824026240295797\n",
            "Epoch: 32\n",
            "Train:  0.773507084242359\n",
            "Test 0.6757736206054688\n",
            "0.6810759824857828\n",
            "Epoch: 33\n",
            "Train:  0.7712735924785493\n",
            "Test 0.674505889415741\n",
            "0.6797612972471617\n",
            "Epoch: 34\n",
            "Train:  0.7681528143753293\n",
            "Test 0.6731278300285339\n",
            "0.6784340654142427\n",
            "Epoch: 35\n",
            "Train:  0.7655272839835326\n",
            "Test 0.6701775789260864\n",
            "0.6767822320660424\n",
            "Epoch: 36\n",
            "Train:  0.7643530638509206\n",
            "Test 0.6673113107681274\n",
            "0.6748875559204094\n",
            "Epoch: 37\n",
            "Train:  0.75957180013484\n",
            "Test 0.6647437214851379\n",
            "0.6728583675874268\n",
            "Epoch: 38\n",
            "Train:  0.7610637006177082\n",
            "Test 0.6639075875282288\n",
            "0.6710679140854722\n",
            "Epoch: 39\n",
            "Train:  0.7569381918303028\n",
            "Test 0.6612334847450256\n",
            "0.6691007667386504\n",
            "Epoch: 40\n",
            "Train:  0.7526868638409748\n",
            "Test 0.6591050624847412\n",
            "0.6671014132801413\n",
            "Epoch: 41\n",
            "Train:  0.7524742179326882\n",
            "Test 0.6542966961860657\n",
            "0.6645402519818191\n",
            "Epoch: 42\n",
            "Train:  0.7456233919475952\n",
            "Test 0.651232123374939\n",
            "0.6618784451072545\n",
            "Epoch: 43\n",
            "Train:  0.7412500427319453\n",
            "Test 0.649705171585083\n",
            "0.6594436578403923\n",
            "Epoch: 44\n",
            "Train:  0.736796370728523\n",
            "Test 0.647104024887085\n",
            "0.6569756237516852\n",
            "Epoch: 45\n",
            "Train:  0.7347111321682305\n",
            "Test 0.6411564350128174\n",
            "0.6538116757564145\n",
            "Epoch: 46\n",
            "Train:  0.7296252808959236\n",
            "Test 0.6370580792427063\n",
            "0.6504608630466473\n",
            "Epoch: 47\n",
            "Train:  0.7255094790350798\n",
            "Test 0.6320759654045105\n",
            "0.6467838015170077\n",
            "Epoch: 48\n",
            "Train:  0.720296256682452\n",
            "Test 0.6290079355239868\n",
            "0.6432285648908627\n",
            "Epoch: 49\n",
            "Train:  0.7167859864990096\n",
            "Test 0.6255371570587158\n",
            "0.6396902328236705\n",
            "Epoch: 50\n",
            "Train:  0.709517516162061\n",
            "Test 0.6195417642593384\n",
            "0.6356604930993103\n",
            "Epoch: 51\n",
            "Train:  0.7045308675161853\n",
            "Test 0.6131632924079895\n",
            "0.6311610118610511\n",
            "Epoch: 52\n",
            "Train:  0.6959439899047576\n",
            "Test 0.6090891361236572\n",
            "0.6267466044552541\n",
            "Epoch: 53\n",
            "Train:  0.6880258928596704\n",
            "Test 0.6034529209136963\n",
            "0.622087840511778\n",
            "Epoch: 54\n",
            "Train:  0.6811483502388\n",
            "Test 0.5951765775680542\n",
            "0.6167055627511684\n",
            "Epoch: 55\n",
            "Train:  0.6721274359193863\n",
            "Test 0.5880786180496216\n",
            "0.6109801523895557\n",
            "Epoch: 56\n",
            "Train:  0.662515218441303\n",
            "Test 0.5826091170310974\n",
            "0.6053059283340299\n",
            "Epoch: 57\n",
            "Train:  0.6517531046500573\n",
            "Test 0.5739854574203491\n",
            "0.5990418191517323\n",
            "Epoch: 58\n",
            "Train:  0.646672353755295\n",
            "Test 0.5679990649223328\n",
            "0.5928332564126089\n",
            "Epoch: 59\n",
            "Train:  0.6403319085884958\n",
            "Test 0.5604604482650757\n",
            "0.5863586848608503\n",
            "Epoch: 60\n",
            "Train:  0.6274960914887994\n",
            "Test 0.5517665147781372\n",
            "0.5794402423623218\n",
            "Epoch: 61\n",
            "Train:  0.6192211506053873\n",
            "Test 0.5451237559318542\n",
            "0.5725769383447192\n",
            "Epoch: 62\n",
            "Train:  0.6033401773795822\n",
            "Test 0.5365048050880432\n",
            "0.5653625060326684\n",
            "Epoch: 63\n",
            "Train:  0.5966849262358377\n",
            "Test 0.5267811417579651\n",
            "0.5576462283341417\n",
            "Epoch: 64\n",
            "Train:  0.5858527074032779\n",
            "Test 0.5213313102722168\n",
            "0.550383241074516\n",
            "Epoch: 65\n",
            "Train:  0.5814222178308133\n",
            "Test 0.5124335289001465\n",
            "0.542793295590499\n",
            "Epoch: 66\n",
            "Train:  0.5618151112109827\n",
            "Test 0.5012303590774536\n",
            "0.5344807056163265\n",
            "Epoch: 67\n",
            "Train:  0.5532584941495058\n",
            "Test 0.4922173023223877\n",
            "0.5260280227842687\n",
            "Epoch: 68\n",
            "Train:  0.5480110607535591\n",
            "Test 0.48270735144615173\n",
            "0.517363886734536\n",
            "Epoch: 69\n",
            "Train:  0.5309889814163226\n",
            "Test 0.4760987162590027\n",
            "0.5091108512813886\n",
            "Epoch: 70\n",
            "Train:  0.5218214870038616\n",
            "Test 0.47532254457473755\n",
            "0.5023531890504772\n",
            "Epoch: 71\n",
            "Train:  0.5095797453530774\n",
            "Test 0.45760419964790344\n",
            "0.4934033902274386\n",
            "Epoch: 72\n",
            "Train:  0.5036931400655081\n",
            "Test 0.45004358887672424\n",
            "0.48473142922668444\n",
            "Epoch: 73\n",
            "Train:  0.48733578763935903\n",
            "Test 0.4449842572212219\n",
            "0.4767819942898011\n",
            "Epoch: 74\n",
            "Train:  0.47018166526949784\n",
            "Test 0.436270534992218\n",
            "0.4686797019934098\n",
            "Epoch: 75\n",
            "Train:  0.46478822301415834\n",
            "Test 0.4281177520751953\n",
            "0.4605673116598315\n",
            "Epoch: 76\n",
            "Train:  0.44848338389828196\n",
            "Test 0.4205922484397888\n",
            "0.45257229873992527\n",
            "Epoch: 77\n",
            "Train:  0.4423805297230164\n",
            "Test 0.41473469138145447\n",
            "0.4450047770593146\n",
            "Epoch: 78\n",
            "Train:  0.4237772923519169\n",
            "Test 0.40864789485931396\n",
            "0.43773340045872194\n",
            "Epoch: 79\n",
            "Train:  0.4341466929847838\n",
            "Test 0.4007250666618347\n",
            "0.4303317335685683\n",
            "Epoch: 80\n",
            "Train:  0.42237728308228883\n",
            "Test 0.39520275592803955\n",
            "0.4233059379411545\n",
            "Epoch: 81\n",
            "Train:  0.40010990087802595\n",
            "Test 0.39018696546554565\n",
            "0.4166821433711321\n",
            "Epoch: 82\n",
            "Train:  0.3972963582876041\n",
            "Test 0.3817611336708069\n",
            "0.4096979413678862\n",
            "Epoch: 83\n",
            "Train:  0.4029785857211411\n",
            "Test 0.37622618675231934\n",
            "0.4030035903963257\n",
            "Epoch: 84\n",
            "Train:  0.3934766930692336\n",
            "Test 0.3703943192958832\n",
            "0.39648173613847826\n",
            "Epoch: 85\n",
            "Train:  0.39187016247084777\n",
            "Test 0.37374284863471985\n",
            "0.3919339586166627\n",
            "Epoch: 86\n",
            "Train:  0.36365573937536905\n",
            "Test 0.3613678812980652\n",
            "0.3858207431302916\n",
            "Epoch: 87\n",
            "Train:  0.3802616457324222\n",
            "Test 0.35448604822158813\n",
            "0.37955380412997397\n",
            "Epoch: 88\n",
            "Train:  0.36353603517847366\n",
            "Test 0.3548572063446045\n",
            "0.3746144845611868\n",
            "Epoch: 89\n",
            "Train:  0.3677074689401221\n",
            "Test 0.35662052035331726\n",
            "0.3710156917127855\n",
            "Epoch: 90\n",
            "Train:  0.3467266296639162\n",
            "Test 0.3433152735233307\n",
            "0.36547560806648627\n",
            "Epoch: 91\n",
            "Train:  0.33435181632840255\n",
            "Test 0.33973121643066406\n",
            "0.36032672973307023\n",
            "Epoch: 92\n",
            "Train:  0.33102284054950354\n",
            "Test 0.3369906544685364\n",
            "0.35565951467563\n",
            "Epoch: 93\n",
            "Train:  0.3064791880446861\n",
            "Test 0.33369308710098267\n",
            "0.3512662291572867\n",
            "Epoch: 94\n",
            "Train:  0.3200643889235156\n",
            "Test 0.33279141783714294\n",
            "0.347571266890961\n",
            "Epoch: 95\n",
            "Train:  0.31958317210501674\n",
            "Test 0.3277553915977478\n",
            "0.34360809183034735\n",
            "Epoch: 96\n",
            "Train:  0.3066435286362247\n",
            "Test 0.3249129354953766\n",
            "0.3398690605618656\n",
            "Epoch: 97\n",
            "Train:  0.30159147346720977\n",
            "Test 0.3280297517776489\n",
            "0.33750119880426854\n",
            "Epoch: 98\n",
            "Train:  0.29895210502104524\n",
            "Test 0.31890201568603516\n",
            "0.3337813621796747\n",
            "Epoch: 99\n",
            "Train:  0.2786368199063642\n",
            "Test 0.31669020652770996\n",
            "0.33036313104858545\n",
            "Epoch: 100\n",
            "Train:  0.2804203707033692\n",
            "Test 0.3138407766819\n",
            "0.3270586601747098\n",
            "Epoch: 101\n",
            "Train:  0.29966009323952963\n",
            "Test 0.31140610575675964\n",
            "0.3239281492907117\n",
            "Epoch: 102\n",
            "Train:  0.28192348454602706\n",
            "Test 0.31031152606010437\n",
            "0.32120482464430616\n",
            "Epoch: 103\n",
            "Train:  0.2650247272862568\n",
            "Test 0.3080093562602997\n",
            "0.31856573096728463\n",
            "Epoch: 104\n",
            "Train:  0.2463803795667795\n",
            "Test 0.30494263768196106\n",
            "0.315841112310038\n",
            "Epoch: 105\n",
            "Train:  0.27365043725632976\n",
            "Test 0.30288514494895935\n",
            "0.3132499188376839\n",
            "Epoch: 106\n",
            "Train:  0.26039947104130395\n",
            "Test 0.3008236289024353\n",
            "0.31076466085052795\n",
            "Epoch: 107\n",
            "Train:  0.25988815421432393\n",
            "Test 0.2977425456047058\n",
            "0.3081602378012745\n",
            "Epoch: 108\n",
            "Train:  0.2662168878101116\n",
            "Test 0.2968760132789612\n",
            "0.3059033928967501\n",
            "Epoch: 109\n",
            "Train:  0.24559994075632743\n",
            "Test 0.2941764295101166\n",
            "0.3035580002193721\n",
            "Epoch: 110\n",
            "Train:  0.25549423364222856\n",
            "Test 0.2928937077522278\n",
            "0.30142514172590595\n",
            "Epoch: 111\n",
            "Train:  0.251203018480836\n",
            "Test 0.29044193029403687\n",
            "0.29922849943950136\n",
            "Epoch: 112\n",
            "Train:  0.23768763945383184\n",
            "Test 0.2890971004962921\n",
            "0.2972022196508368\n",
            "Epoch: 113\n",
            "Train:  0.22444906412746032\n",
            "Test 0.28775420784950256\n",
            "0.295312617290553\n",
            "Epoch: 114\n",
            "Train:  0.22083283563005437\n",
            "Test 0.28639304637908936\n",
            "0.29352870310824747\n",
            "Epoch: 115\n",
            "Train:  0.23636595689063697\n",
            "Test 0.28515133261680603\n",
            "0.2918532290099496\n",
            "Epoch: 116\n",
            "Train:  0.23577282466500055\n",
            "Test 0.28441914916038513\n",
            "0.29036641304002986\n",
            "Epoch: 117\n",
            "Train:  0.21148662107292884\n",
            "Test 0.2833543121814728\n",
            "0.28896399286831326\n",
            "Epoch: 118\n",
            "Train:  0.2270351531009329\n",
            "Test 0.28390300273895264\n",
            "0.28795179484243816\n",
            "Epoch: 119\n",
            "Train:  0.22008483418646982\n",
            "Test 0.28139346837997437\n",
            "0.28664012954994234\n",
            "Epoch: 120\n",
            "Train:  0.21984820351072026\n",
            "Test 0.2817685902118683\n",
            "0.28566582168232574\n",
            "Epoch: 121\n",
            "Train:  0.2167942562389158\n",
            "Test 0.2844223082065582\n",
            "0.2854171189871718\n",
            "Epoch: 122\n",
            "Train:  0.20331489554357746\n",
            "Test 0.2818496823310852\n",
            "0.28470363165595364\n",
            "Epoch: 123\n",
            "Train:  0.19805691472140913\n",
            "Test 0.2780912518501282\n",
            "0.2833811556947873\n",
            "Epoch: 124\n",
            "Train:  0.19264891177280996\n",
            "Test 0.2772650420665741\n",
            "0.2821579329691437\n",
            "Epoch: 125\n",
            "Train:  0.19703657051961346\n",
            "Test 0.2762643098831177\n",
            "0.28097920835193774\n",
            "Epoch: 126\n",
            "Train:  0.20122644065866643\n",
            "Test 0.27686333656311035\n",
            "0.2801560339941719\n",
            "Epoch: 127\n",
            "Train:  0.18664677151188053\n",
            "Test 0.2756885290145874\n",
            "0.2792625329982546\n",
            "Epoch: 128\n",
            "Train:  0.1823521177558338\n",
            "Test 0.27523645758628845\n",
            "0.2784573179158611\n",
            "Epoch: 129\n",
            "Train:  0.19708745019737953\n",
            "Test 0.2755115330219269\n",
            "0.2778681609370741\n",
            "Epoch: 130\n",
            "Train:  0.2003918395322912\n",
            "Test 0.28391265869140625\n",
            "0.2790770604879408\n",
            "Epoch: 131\n",
            "Train:  0.1819983658073175\n",
            "Test 0.2742723822593689\n",
            "0.27811612484222625\n",
            "Epoch: 132\n",
            "Train:  0.1855852085405885\n",
            "Test 0.27508750557899475\n",
            "0.27751040098957985\n",
            "Epoch: 133\n",
            "Train:  0.18371592097962064\n",
            "Test 0.2772613763809204\n",
            "0.27746059606784795\n",
            "Epoch: 134\n",
            "Train:  0.18218979347345515\n",
            "Test 0.28140029311180115\n",
            "0.2782485354766387\n",
            "Epoch: 135\n",
            "Train:  0.18828718681141263\n",
            "Test 0.2793150544166565\n",
            "0.27846183926464224\n",
            "Epoch: 136\n",
            "Train:  0.1610234085790712\n",
            "Test 0.2774577736854553\n",
            "0.2782610261488049\n",
            "Epoch: 137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:13:57,273]\u001b[0m Trial 2 finished with value: 0.2790117843189192 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 1.433675124157315e-05, 'b1': 0.9286759768614723}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.1765466347269343\n",
            "Test 0.2820148169994354\n",
            "0.27901178431893103\n",
            "Epoch: 0\n",
            "Train:  0.6929551112166358\n",
            "Test 0.6905836462974548\n",
            "0.6905836462974547\n",
            "Epoch: 1\n",
            "Train:  0.7900893672019648\n",
            "Test 0.6905653476715088\n",
            "0.6905734803941516\n",
            "Epoch: 2\n",
            "Train:  0.7898065348016731\n",
            "Test 0.6905441880226135\n",
            "0.690561475323849\n",
            "Epoch: 3\n",
            "Train:  0.7892397437160371\n",
            "Test 0.6905303597450256\n",
            "0.6905509348161175\n",
            "Epoch: 4\n",
            "Train:  0.7892829503408924\n",
            "Test 0.6905169486999512\n",
            "0.6905408247149257\n",
            "Epoch: 5\n",
            "Train:  0.7889391766414383\n",
            "Test 0.6905049681663513\n",
            "0.6905311055966766\n",
            "Epoch: 6\n",
            "Train:  0.7895898071888885\n",
            "Test 0.6904933452606201\n",
            "0.6905215494629747\n",
            "Epoch: 7\n",
            "Train:  0.7894493020497836\n",
            "Test 0.6904816627502441\n",
            "0.6905119639358357\n",
            "Epoch: 8\n",
            "Train:  0.7887193811425256\n",
            "Test 0.6904691457748413\n",
            "0.6905020727282241\n",
            "Epoch: 9\n",
            "Train:  0.789761210998259\n",
            "Test 0.6904563903808594\n",
            "0.6904918372305577\n",
            "Epoch: 10\n",
            "Train:  0.7910024326850925\n",
            "Test 0.6904468536376953\n",
            "0.6904819950774728\n",
            "Epoch: 11\n",
            "Train:  0.7888948429224178\n",
            "Test 0.6904332637786865\n",
            "0.6904715296382052\n",
            "Epoch: 12\n",
            "Train:  0.7894361911855672\n",
            "Test 0.6904163360595703\n",
            "0.6904598487593733\n",
            "Epoch: 13\n",
            "Train:  0.789807385988365\n",
            "Test 0.6904007792472839\n",
            "0.6904474913733716\n",
            "Epoch: 14\n",
            "Train:  0.790147901660177\n",
            "Test 0.690385639667511\n",
            "0.690434669917319\n",
            "Epoch: 15\n",
            "Train:  0.7896922459969153\n",
            "Test 0.6903700828552246\n",
            "0.6904213783814245\n",
            "Epoch: 16\n",
            "Train:  0.789352898953727\n",
            "Test 0.6903568506240845\n",
            "0.6904081755281306\n",
            "Epoch: 17\n",
            "Train:  0.7902109660713921\n",
            "Test 0.6903433799743652\n",
            "0.6903949786841714\n",
            "Epoch: 18\n",
            "Train:  0.7898632622412427\n",
            "Test 0.6903294920921326\n",
            "0.6903816898535311\n",
            "Epoch: 19\n",
            "Train:  0.7896693694106055\n",
            "Test 0.6903175115585327\n",
            "0.6903687044834067\n",
            "Epoch: 20\n",
            "Train:  0.7900614727676184\n",
            "Test 0.6903039216995239\n",
            "0.6903556273110039\n",
            "Epoch: 21\n",
            "Train:  0.7898781458716587\n",
            "Test 0.690288782119751\n",
            "0.690342158893372\n",
            "Epoch: 22\n",
            "Train:  0.7886589064317591\n",
            "Test 0.6902742981910706\n",
            "0.6903285061614068\n",
            "Epoch: 23\n",
            "Train:  0.7889871357253234\n",
            "Test 0.6902595162391663\n",
            "0.6903146427086541\n",
            "Epoch: 24\n",
            "Train:  0.7901267964915453\n",
            "Test 0.6902446150779724\n",
            "0.6903005840704844\n",
            "Epoch: 25\n",
            "Train:  0.7893957010221697\n",
            "Test 0.6902300715446472\n",
            "0.6902864388139023\n",
            "Epoch: 26\n",
            "Train:  0.7899544835090637\n",
            "Test 0.6902143359184265\n",
            "0.6902719832834792\n",
            "Epoch: 27\n",
            "Train:  0.7897387653993805\n",
            "Test 0.6902016997337341\n",
            "0.6902578993312045\n",
            "Epoch: 28\n",
            "Train:  0.7893359226878413\n",
            "Test 0.6901878714561462\n",
            "0.6902438720500266\n",
            "Epoch: 29\n",
            "Train:  0.7888865139149972\n",
            "Test 0.6901733875274658\n",
            "0.6902297576727616\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:14:58,211]\u001b[0m Trial 3 finished with value: 0.6895324694106032 and parameters: {'layer_size1': 512, 'layer_size2': 256, 'layer_size3': 512, 'layer_size4': 128, 'layer_size5': 64, 'layer_size6': 128, 'learning_rate': 1.1114098894037444e-06, 'b1': 0.9813140635868588}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7891203868982479\n",
            "Test 0.6901611685752869\n",
            "0.6902160262543285\n",
            "Epoch: 0\n",
            "Train:  0.6941598907315354\n",
            "Test 0.6966660618782043\n",
            "0.6966660618782043\n",
            "Epoch: 1\n",
            "Train:  0.7913661084024075\n",
            "Test 0.6962376236915588\n",
            "0.6964280406634015\n",
            "Epoch: 2\n",
            "Train:  0.7909447353889499\n",
            "Test 0.6959473490715027\n",
            "0.6962310359126233\n",
            "Epoch: 3\n",
            "Train:  0.7914824599054604\n",
            "Test 0.6957036852836609\n",
            "0.696052394100644\n",
            "Epoch: 4\n",
            "Train:  0.7908830569880041\n",
            "Test 0.6955990791320801\n",
            "0.6959175431461688\n",
            "Epoch: 5\n",
            "Train:  0.7908518098598152\n",
            "Test 0.6954175233840942\n",
            "0.6957820099900858\n",
            "Epoch: 6\n",
            "Train:  0.7906538132089296\n",
            "Test 0.6952435374259949\n",
            "0.6956457369492551\n",
            "Epoch: 7\n",
            "Train:  0.79118294063197\n",
            "Test 0.6951126456260681\n",
            "0.6955176250795088\n",
            "Epoch: 8\n",
            "Train:  0.7902704784773055\n",
            "Test 0.6948968172073364\n",
            "0.6953742153811933\n",
            "Epoch: 9\n",
            "Train:  0.7907562215403734\n",
            "Test 0.6946971416473389\n",
            "0.6952225115607336\n",
            "Epoch: 10\n",
            "Train:  0.7905264842024756\n",
            "Test 0.6945182085037231\n",
            "0.6950684140762023\n",
            "Epoch: 11\n",
            "Train:  0.7901101657168358\n",
            "Test 0.6941813826560974\n",
            "0.6948779169278251\n",
            "Epoch: 12\n",
            "Train:  0.7896670318836541\n",
            "Test 0.6939716935157776\n",
            "0.694686128568879\n",
            "Epoch: 13\n",
            "Train:  0.7887736219086798\n",
            "Test 0.6937323808670044\n",
            "0.6944866038380406\n",
            "Epoch: 14\n",
            "Train:  0.788864726664254\n",
            "Test 0.6935276389122009\n",
            "0.6942878166504999\n",
            "Epoch: 15\n",
            "Train:  0.7888171662032873\n",
            "Test 0.6933013796806335\n",
            "0.6940848152758051\n",
            "Epoch: 16\n",
            "Train:  0.7895673027944781\n",
            "Test 0.6930460929870605\n",
            "0.6938722850631229\n",
            "Epoch: 17\n",
            "Train:  0.7881546314485472\n",
            "Test 0.692636251449585\n",
            "0.6936205433651466\n",
            "Epoch: 18\n",
            "Train:  0.7876332353682539\n",
            "Test 0.6922532320022583\n",
            "0.6933430824595088\n",
            "Epoch: 19\n",
            "Train:  0.7870785777385418\n",
            "Test 0.6919424533843994\n",
            "0.6930596893443194\n",
            "Epoch: 20\n",
            "Train:  0.7878417467100048\n",
            "Test 0.6914961338043213\n",
            "0.6927440671352625\n",
            "Epoch: 21\n",
            "Train:  0.7873455307602343\n",
            "Test 0.6911311745643616\n",
            "0.6924190907183653\n",
            "Epoch: 22\n",
            "Train:  0.7867361549878013\n",
            "Test 0.6907970309257507\n",
            "0.6920927523984122\n",
            "Epoch: 23\n",
            "Train:  0.7873651083238524\n",
            "Test 0.6904708743095398\n",
            "0.6917668376919516\n",
            "Epoch: 24\n",
            "Train:  0.7867619308950674\n",
            "Test 0.69012451171875\n",
            "0.6914371268851006\n",
            "Epoch: 25\n",
            "Train:  0.7860175654359532\n",
            "Test 0.6896216869354248\n",
            "0.6910729382024112\n",
            "Epoch: 26\n",
            "Train:  0.7861045367038089\n",
            "Test 0.6890377402305603\n",
            "0.690664912061367\n",
            "Epoch: 27\n",
            "Train:  0.784665052977083\n",
            "Test 0.6885040998458862\n",
            "0.6902319120744884\n",
            "Epoch: 28\n",
            "Train:  0.785585484083961\n",
            "Test 0.6882956624031067\n",
            "0.689844061971247\n",
            "Epoch: 29\n",
            "Train:  0.7847998606133785\n",
            "Test 0.6876011490821838\n",
            "0.6893949233867975\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:16:00,575]\u001b[0m Trial 4 finished with value: 0.6882540877620597 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 128, 'layer_size6': 512, 'learning_rate': 1.1082085854454556e-05, 'b1': 0.9406785882237053}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7822493560173932\n",
            "Test 0.6871044635772705\n",
            "0.6889363773028464\n",
            "Epoch: 0\n",
            "Train:  0.6966933084828821\n",
            "Test 0.7026292085647583\n",
            "0.7026292085647583\n",
            "Epoch: 1\n",
            "Train:  0.7945135564825654\n",
            "Test 0.7016547918319702\n",
            "0.7020878659354316\n",
            "Epoch: 2\n",
            "Train:  0.7924756100274858\n",
            "Test 0.7004048824310303\n",
            "0.7013981185975622\n",
            "Epoch: 3\n",
            "Train:  0.7929556952342728\n",
            "Test 0.6986330151557922\n",
            "0.700461430439781\n",
            "Epoch: 4\n",
            "Train:  0.7907782358821162\n",
            "Test 0.6973510980606079\n",
            "0.6995361768762478\n",
            "Epoch: 5\n",
            "Train:  0.7901678290302397\n",
            "Test 0.6963449120521545\n",
            "0.6986711666780269\n",
            "Epoch: 6\n",
            "Train:  0.7890705827674175\n",
            "Test 0.6947957277297974\n",
            "0.6976903964675332\n",
            "Epoch: 7\n",
            "Train:  0.7887627471626074\n",
            "Test 0.6931386590003967\n",
            "0.6965965283587383\n",
            "Epoch: 8\n",
            "Train:  0.7874870532238645\n",
            "Test 0.6919353008270264\n",
            "0.6955197616744438\n",
            "Epoch: 9\n",
            "Train:  0.786861439905555\n",
            "Test 0.6913861632347107\n",
            "0.6945935956688835\n",
            "Epoch: 10\n",
            "Train:  0.7851277158271134\n",
            "Test 0.6900880336761475\n",
            "0.693607804449366\n",
            "Epoch: 11\n",
            "Train:  0.7831612507142632\n",
            "Test 0.6885835528373718\n",
            "0.6925288059120864\n",
            "Epoch: 12\n",
            "Train:  0.7826465845647441\n",
            "Test 0.6860525012016296\n",
            "0.6911581948288353\n",
            "Epoch: 13\n",
            "Train:  0.777869021730725\n",
            "Test 0.6845845580101013\n",
            "0.6897829851016624\n",
            "Epoch: 14\n",
            "Train:  0.7765332155637612\n",
            "Test 0.6785007119178772\n",
            "0.6874442433032503\n",
            "Epoch: 15\n",
            "Train:  0.7694556553978725\n",
            "Test 0.6774691939353943\n",
            "0.6853914525054796\n",
            "Epoch: 16\n",
            "Train:  0.7638911847615134\n",
            "Test 0.6741270422935486\n",
            "0.6830866714066032\n",
            "Epoch: 17\n",
            "Train:  0.761355429213511\n",
            "Test 0.6645129919052124\n",
            "0.679303789157984\n",
            "Epoch: 18\n",
            "Train:  0.7533110053830556\n",
            "Test 0.6614742875099182\n",
            "0.6756857473508203\n",
            "Epoch: 19\n",
            "Train:  0.744499255898851\n",
            "Test 0.6584635972976685\n",
            "0.672201142582465\n",
            "Epoch: 20\n",
            "Train:  0.7287217637532437\n",
            "Test 0.6389127373695374\n",
            "0.6654814836253573\n",
            "Epoch: 21\n",
            "Train:  0.7176432210395779\n",
            "Test 0.6280491948127747\n",
            "0.6579393749232392\n",
            "Epoch: 22\n",
            "Train:  0.7025999178713803\n",
            "Test 0.6248672008514404\n",
            "0.6512856635292876\n",
            "Epoch: 23\n",
            "Train:  0.6814740542912375\n",
            "Test 0.5959353446960449\n",
            "0.6401630748225274\n",
            "Epoch: 24\n",
            "Train:  0.6530984769579512\n",
            "Test 0.5755500197410583\n",
            "0.6271914584250304\n",
            "Epoch: 25\n",
            "Train:  0.6247366949983312\n",
            "Test 0.5652731657028198\n",
            "0.6147702591091795\n",
            "Epoch: 26\n",
            "Train:  0.5989561666190893\n",
            "Test 0.5243632197380066\n",
            "0.5966450271130517\n",
            "Epoch: 27\n",
            "Train:  0.5572494027032032\n",
            "Test 0.5022976994514465\n",
            "0.5777389919903204\n",
            "Epoch: 28\n",
            "Train:  0.5103019257476427\n",
            "Test 0.4691373407840729\n",
            "0.5559849990755165\n",
            "Epoch: 29\n",
            "Train:  0.4787524791594544\n",
            "Test 0.44187667965888977\n",
            "0.5331350483232591\n",
            "Epoch: 30\n",
            "Train:  0.43990252608627217\n",
            "Test 0.41885873675346375\n",
            "0.5102571288152404\n",
            "Epoch: 31\n",
            "Train:  0.4080613936504088\n",
            "Test 0.40429386496543884\n",
            "0.4890476721825107\n",
            "Epoch: 32\n",
            "Train:  0.39178368395270263\n",
            "Test 0.38226208090782166\n",
            "0.4676770086603524\n",
            "Epoch: 33\n",
            "Train:  0.3777655694819144\n",
            "Test 0.3719050884246826\n",
            "0.4485129072593609\n",
            "Epoch: 34\n",
            "Train:  0.3674579123835758\n",
            "Test 0.40832313895225525\n",
            "0.44047169169338335\n",
            "Epoch: 35\n",
            "Train:  0.3434278537515062\n",
            "Test 0.36299368739128113\n",
            "0.42497106059057665\n",
            "Epoch: 36\n",
            "Train:  0.3137416680474087\n",
            "Test 0.3590169847011566\n",
            "0.41177681999199206\n",
            "Epoch: 37\n",
            "Train:  0.2948198319155706\n",
            "Test 0.3428567945957184\n",
            "0.39798995149217736\n",
            "Epoch: 38\n",
            "Train:  0.27741492785749394\n",
            "Test 0.34777581691741943\n",
            "0.38794545564909105\n",
            "Epoch: 39\n",
            "Train:  0.28114076032897467\n",
            "Test 0.33762696385383606\n",
            "0.3778804194172462\n",
            "Epoch: 40\n",
            "Train:  0.25868785893755264\n",
            "Test 0.33000531792640686\n",
            "0.368304380819991\n",
            "Epoch: 41\n",
            "Train:  0.2601343560205326\n",
            "Test 0.3306547701358795\n",
            "0.36077381805373815\n",
            "Epoch: 42\n",
            "Train:  0.23181975939694574\n",
            "Test 0.3221343457698822\n",
            "0.3530453976279279\n",
            "Epoch: 43\n",
            "Train:  0.23731200842026673\n",
            "Test 0.31959423422813416\n",
            "0.3463548006772225\n",
            "Epoch: 44\n",
            "Train:  0.20714581194776216\n",
            "Test 0.3173021078109741\n",
            "0.3405440090083002\n",
            "Epoch: 45\n",
            "Train:  0.20699955367934111\n",
            "Test 0.3179951012134552\n",
            "0.3360340703009031\n",
            "Epoch: 46\n",
            "Train:  0.196685486991481\n",
            "Test 0.31548893451690674\n",
            "0.33192492859795114\n",
            "Epoch: 47\n",
            "Train:  0.1937712407017725\n",
            "Test 0.31853213906288147\n",
            "0.3292463109557677\n",
            "Epoch: 48\n",
            "Train:  0.18542641465345658\n",
            "Test 0.3270983397960663\n",
            "0.32881670905947347\n",
            "Epoch: 49\n",
            "Train:  0.1825469455837664\n",
            "Test 0.34144872426986694\n",
            "0.3313431481600959\n",
            "Epoch: 50\n",
            "Train:  0.2033330618795766\n",
            "Test 0.32984188199043274\n",
            "0.33104289149783833\n",
            "Epoch: 51\n",
            "Train:  0.16199713817279263\n",
            "Test 0.33097565174102783\n",
            "0.3310294434236363\n",
            "Epoch: 52\n",
            "Train:  0.15673567830032892\n",
            "Test 0.3308160901069641\n",
            "0.33098677244848335\n",
            "Epoch: 53\n",
            "Train:  0.17113716576703533\n",
            "Test 0.3632217347621918\n",
            "0.3374338026006055\n",
            "Epoch: 54\n",
            "Train:  0.1532529264149083\n",
            "Test 0.3490316867828369\n",
            "0.33975339028531165\n",
            "Epoch: 55\n",
            "Train:  0.15707641509593342\n",
            "Test 0.37850725650787354\n",
            "0.3475041925290181\n",
            "Epoch: 56\n",
            "Train:  0.163200698388378\n",
            "Test 0.3459952175617218\n",
            "0.34720239663223684\n",
            "Epoch: 57\n",
            "Train:  0.1322369556621189\n",
            "Test 0.34987205266952515\n",
            "0.34773632911820884\n",
            "Epoch: 58\n",
            "Train:  0.13137356973173123\n",
            "Test 0.3670590817928314\n",
            "0.3516008870561556\n",
            "Epoch: 59\n",
            "Train:  0.14083536919964923\n",
            "Test 0.3540802299976349\n",
            "0.352096756404369\n",
            "Epoch: 60\n",
            "Train:  0.13407455587009498\n",
            "Test 0.3786974549293518\n",
            "0.3574169026318459\n",
            "Epoch: 61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:17:57,653]\u001b[0m Trial 5 finished with value: 0.35793847922640654 and parameters: {'layer_size1': 512, 'layer_size2': 256, 'layer_size3': 512, 'layer_size4': 512, 'layer_size5': 64, 'layer_size6': 64, 'learning_rate': 4.7445917141963744e-05, 'b1': 0.9457654566117144}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.1307939602508804\n",
            "Test 0.3600265383720398\n",
            "0.3579388302917898\n",
            "Epoch: 0\n",
            "Train:  0.6922823020775394\n",
            "Test 0.6914861798286438\n",
            "0.6914861798286438\n",
            "Epoch: 1\n",
            "Train:  0.78967601735128\n",
            "Test 0.6912868618965149\n",
            "0.6913754476441278\n",
            "Epoch: 2\n",
            "Train:  0.7895051934061007\n",
            "Test 0.6908828020095825\n",
            "0.6911735436955437\n",
            "Epoch: 3\n",
            "Train:  0.7893934260666101\n",
            "Test 0.6905848383903503\n",
            "0.6909741177791503\n",
            "Epoch: 4\n",
            "Train:  0.7886843899795912\n",
            "Test 0.690210223197937\n",
            "0.6907468764115833\n",
            "Epoch: 5\n",
            "Train:  0.789394633262945\n",
            "Test 0.6898946166038513\n",
            "0.6905158666189594\n",
            "Epoch: 6\n",
            "Train:  0.7885188638354859\n",
            "Test 0.689533531665802\n",
            "0.690267263848627\n",
            "Epoch: 7\n",
            "Train:  0.7887693619296562\n",
            "Test 0.6892380118370056\n",
            "0.690019915235146\n",
            "Epoch: 8\n",
            "Train:  0.7882731858961183\n",
            "Test 0.6889065504074097\n",
            "0.6897627224365075\n",
            "Epoch: 9\n",
            "Train:  0.7885542609033541\n",
            "Test 0.6888906359672546\n",
            "0.6895673244428447\n",
            "Epoch: 10\n",
            "Train:  0.7871627837284658\n",
            "Test 0.6888219118118286\n",
            "0.6894042324184185\n",
            "Epoch: 11\n",
            "Train:  0.7880563682021059\n",
            "Test 0.6887317895889282\n",
            "0.6892598198998073\n",
            "Epoch: 12\n",
            "Train:  0.7870938208847563\n",
            "Test 0.6885951161384583\n",
            "0.6891191454900759\n",
            "Epoch: 13\n",
            "Train:  0.7867573666896216\n",
            "Test 0.6883974671363831\n",
            "0.6889681698400277\n",
            "Epoch: 14\n",
            "Train:  0.7867848994505352\n",
            "Test 0.688209056854248\n",
            "0.6888108106588913\n",
            "Epoch: 15\n",
            "Train:  0.7865976491664869\n",
            "Test 0.6879156231880188\n",
            "0.6886265877508413\n",
            "Epoch: 16\n",
            "Train:  0.7859788364414716\n",
            "Test 0.687602162361145\n",
            "0.6884169827887564\n",
            "Epoch: 17\n",
            "Train:  0.784364047363333\n",
            "Test 0.6871495842933655\n",
            "0.6881588530374715\n",
            "Epoch: 18\n",
            "Train:  0.7846232067405908\n",
            "Test 0.6867160201072693\n",
            "0.6878660669593598\n",
            "Epoch: 19\n",
            "Train:  0.7845176714577826\n",
            "Test 0.6862361431121826\n",
            "0.6875362800052107\n",
            "Epoch: 20\n",
            "Train:  0.7832944064118743\n",
            "Test 0.6856878995895386\n",
            "0.6871631625207031\n",
            "Epoch: 21\n",
            "Train:  0.7820094818443195\n",
            "Test 0.6850225925445557\n",
            "0.6867318661197422\n",
            "Epoch: 22\n",
            "Train:  0.7825621423138752\n",
            "Test 0.6847123503684998\n",
            "0.6863255645885779\n",
            "Epoch: 23\n",
            "Train:  0.7804728239370148\n",
            "Test 0.6846073269844055\n",
            "0.6859802865382513\n",
            "Epoch: 24\n",
            "Train:  0.7803470891525303\n",
            "Test 0.6840826272964478\n",
            "0.6855993154217052\n",
            "Epoch: 25\n",
            "Train:  0.7794085420095004\n",
            "Test 0.6834996938705444\n",
            "0.6851781181207423\n",
            "Epoch: 26\n",
            "Train:  0.7794062627386723\n",
            "Test 0.682895302772522\n",
            "0.684720448473792\n",
            "Epoch: 27\n",
            "Train:  0.7763961362083573\n",
            "Test 0.6819791197776794\n",
            "0.6841711201791152\n",
            "Epoch: 28\n",
            "Train:  0.7769115583389593\n",
            "Test 0.6811327934265137\n",
            "0.6835625130546855\n",
            "Epoch: 29\n",
            "Train:  0.774952103379625\n",
            "Test 0.6799450516700745\n",
            "0.6828381240275826\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:19:17,931]\u001b[0m Trial 6 finished with value: 0.6812489846639842 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 128, 'layer_size5': 64, 'layer_size6': 512, 'learning_rate': 1.7680260867478474e-05, 'b1': 0.97013953283949}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7750785256403064\n",
            "Test 0.6782736778259277\n",
            "0.6819243298092912\n",
            "Epoch: 0\n",
            "Train:  0.6924498795923604\n",
            "Test 0.6855696439743042\n",
            "0.6855696439743042\n",
            "Epoch: 1\n",
            "Train:  0.7897987991436575\n",
            "Test 0.6856481432914734\n",
            "0.685613254706065\n",
            "Epoch: 2\n",
            "Train:  0.7872579345875735\n",
            "Test 0.6843004822731018\n",
            "0.6850752332171456\n",
            "Epoch: 3\n",
            "Train:  0.782085365839134\n",
            "Test 0.6797608137130737\n",
            "0.6832749556073651\n",
            "Epoch: 4\n",
            "Train:  0.780123884052173\n",
            "Test 0.6733354926109314\n",
            "0.6803181900801061\n",
            "Epoch: 5\n",
            "Train:  0.7664670696085935\n",
            "Test 0.6616972088813782\n",
            "0.675270868868724\n",
            "Epoch: 6\n",
            "Train:  0.7470242154544295\n",
            "Test 0.6251369118690491\n",
            "0.6625833018043757\n",
            "Epoch: 7\n",
            "Train:  0.7188456387541413\n",
            "Test 0.596762478351593\n",
            "0.6467653201678126\n",
            "Epoch: 8\n",
            "Train:  0.6723496094008916\n",
            "Test 0.5764519572257996\n",
            "0.6305225845018153\n",
            "Epoch: 9\n",
            "Train:  0.6080541822435629\n",
            "Test 0.5008062720298767\n",
            "0.6014586004864959\n",
            "Epoch: 10\n",
            "Train:  0.5318050304958724\n",
            "Test 0.4479025602340698\n",
            "0.5678614162886632\n",
            "Epoch: 11\n",
            "Train:  0.44462709607581746\n",
            "Test 0.40037617087364197\n",
            "0.5318926096334976\n",
            "Epoch: 12\n",
            "Train:  0.41166534696229445\n",
            "Test 0.38378697633743286\n",
            "0.5005483119879998\n",
            "Epoch: 13\n",
            "Train:  0.3422731227195101\n",
            "Test 0.36896437406539917\n",
            "0.47302085399774363\n",
            "Epoch: 14\n",
            "Train:  0.320673435521881\n",
            "Test 0.37675726413726807\n",
            "0.4530660383675518\n",
            "Epoch: 15\n",
            "Train:  0.33704431982061983\n",
            "Test 0.3739054501056671\n",
            "0.43677537943068945\n",
            "Epoch: 16\n",
            "Train:  0.27524760729586917\n",
            "Test 0.32181957364082336\n",
            "0.4132545768848109\n",
            "Epoch: 17\n",
            "Train:  0.24716314848731546\n",
            "Test 0.3285791873931885\n",
            "0.39600882717868546\n",
            "Epoch: 18\n",
            "Train:  0.209428651570195\n",
            "Test 0.322582870721817\n",
            "0.38110890538413816\n",
            "Epoch: 19\n",
            "Train:  0.17951259340635792\n",
            "Test 0.30943846702575684\n",
            "0.36660762938300245\n",
            "Epoch: 20\n",
            "Train:  0.16642683063427247\n",
            "Test 0.3095206618309021\n",
            "0.35508394867820814\n",
            "Epoch: 21\n",
            "Train:  0.13762817685218418\n",
            "Test 0.31789639592170715\n",
            "0.3475911510387273\n",
            "Epoch: 22\n",
            "Train:  0.1480100764914066\n",
            "Test 0.32626140117645264\n",
            "0.3432998698129861\n",
            "Epoch: 23\n",
            "Train:  0.14151433801354327\n",
            "Test 0.38254058361053467\n",
            "0.3511852502286755\n",
            "Epoch: 24\n",
            "Train:  0.14968786270640014\n",
            "Test 0.37632229924201965\n",
            "0.35623172507427836\n",
            "Epoch: 25\n",
            "Train:  0.12986261899924387\n",
            "Test 0.32421138882637024\n",
            "0.349808244044525\n",
            "Epoch: 26\n",
            "Train:  0.10513265371929466\n",
            "Test 0.34043413400650024\n",
            "0.34792887800866956\n",
            "Epoch: 27\n",
            "Train:  0.10487479333896443\n",
            "Test 0.3957480490207672\n",
            "0.3575112472087516\n",
            "Epoch: 28\n",
            "Train:  0.10591016866148732\n",
            "Test 0.35477593541145325\n",
            "0.3569633369993092\n",
            "Epoch: 29\n",
            "Train:  0.09684155678283844\n",
            "Test 0.413228303194046\n",
            "0.36823027803568575\n",
            "Epoch: 30\n",
            "Train:  0.09586420195910456\n",
            "Test 0.3662926256656647\n",
            "0.3678423633896238\n",
            "Epoch: 31\n",
            "Train:  0.08463970680480899\n",
            "Test 0.37315136194229126\n",
            "0.36890500501158846\n",
            "Epoch: 32\n",
            "Train:  0.08380347396335586\n",
            "Test 0.36538034677505493\n",
            "0.3681996262773939\n",
            "Epoch: 33\n",
            "Train:  0.08601810211922585\n",
            "Test 0.34846705198287964\n",
            "0.3642511092825156\n",
            "Epoch: 34\n",
            "Train:  0.0757353314308121\n",
            "Test 0.3364352285861969\n",
            "0.3586856755351141\n",
            "Epoch: 35\n",
            "Train:  0.07020056703679972\n",
            "Test 0.3424206078052521\n",
            "0.35543160598319723\n",
            "Epoch: 36\n",
            "Train:  0.06265690998802155\n",
            "Test 0.40551984310150146\n",
            "0.36545185481218384\n",
            "Epoch: 37\n",
            "Train:  0.0704659652133468\n",
            "Test 0.33555638790130615\n",
            "0.3594715193629297\n",
            "Epoch: 38\n",
            "Train:  0.05651880145005511\n",
            "Test 0.40846818685531616\n",
            "0.36927248132553525\n",
            "Epoch: 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:20:18,941]\u001b[0m Trial 7 finished with value: 0.37627496960712015 and parameters: {'layer_size1': 256, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 128, 'layer_size5': 64, 'layer_size6': 128, 'learning_rate': 0.00020324253025713507, 'b1': 0.9140465480985822}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.06523119697423138\n",
            "Test 0.4045303463935852\n",
            "0.3763249917785787\n",
            "Epoch: 0\n",
            "Train:  0.6937751980388865\n",
            "Test 0.6974778771400452\n",
            "0.6974778771400452\n",
            "Epoch: 1\n",
            "Train:  0.7920378754041854\n",
            "Test 0.6971756219863892\n",
            "0.6973099576102364\n",
            "Epoch: 2\n",
            "Train:  0.7912340452768144\n",
            "Test 0.6968055367469788\n",
            "0.6971032277482455\n",
            "Epoch: 3\n",
            "Train:  0.7904740790436171\n",
            "Test 0.6964698433876038\n",
            "0.696888666650467\n",
            "Epoch: 4\n",
            "Train:  0.7898422881488887\n",
            "Test 0.6962651610374451\n",
            "0.6967031878269836\n",
            "Epoch: 5\n",
            "Train:  0.7903114214741807\n",
            "Test 0.6959933638572693\n",
            "0.6965107860657417\n",
            "Epoch: 6\n",
            "Train:  0.789794320704171\n",
            "Test 0.6957293152809143\n",
            "0.6963130166578453\n",
            "Epoch: 7\n",
            "Train:  0.7899230702430414\n",
            "Test 0.6955587863922119\n",
            "0.6961317609417104\n",
            "Epoch: 8\n",
            "Train:  0.7911432412415068\n",
            "Test 0.6953796744346619\n",
            "0.6959580252274619\n",
            "Epoch: 9\n",
            "Train:  0.7910090817045842\n",
            "Test 0.6952970623970032\n",
            "0.6958099311845994\n",
            "Epoch: 10\n",
            "Train:  0.789607606052813\n",
            "Test 0.6950994729995728\n",
            "0.6956544869932282\n",
            "Epoch: 11\n",
            "Train:  0.7901209368964666\n",
            "Test 0.694976806640625\n",
            "0.6955089496743057\n",
            "Epoch: 12\n",
            "Train:  0.7914085253331457\n",
            "Test 0.694794774055481\n",
            "0.6953578053007297\n",
            "Epoch: 13\n",
            "Train:  0.7897308587488545\n",
            "Test 0.6945578455924988\n",
            "0.6951904531329053\n",
            "Epoch: 14\n",
            "Train:  0.7898610663090356\n",
            "Test 0.6942506432533264\n",
            "0.6949956366617804\n",
            "Epoch: 15\n",
            "Train:  0.7893003497727856\n",
            "Test 0.6939942240715027\n",
            "0.6947895534160455\n",
            "Epoch: 16\n",
            "Train:  0.7896930130358735\n",
            "Test 0.6937127709388733\n",
            "0.6945692358091494\n",
            "Epoch: 17\n",
            "Train:  0.7894763353183798\n",
            "Test 0.693444550037384\n",
            "0.6943401722118709\n",
            "Epoch: 18\n",
            "Train:  0.7883567896363962\n",
            "Test 0.6932443380355835\n",
            "0.6941178006648817\n",
            "Epoch: 19\n",
            "Train:  0.7880757030318765\n",
            "Test 0.693034827709198\n",
            "0.6938986797819648\n",
            "Epoch: 20\n",
            "Train:  0.7882418424835033\n",
            "Test 0.6929883360862732\n",
            "0.6937149161222258\n",
            "Epoch: 21\n",
            "Train:  0.7874330794649427\n",
            "Test 0.692467987537384\n",
            "0.6934636765846773\n",
            "Epoch: 22\n",
            "Train:  0.7873396258548374\n",
            "Test 0.6921257376670837\n",
            "0.6931944998622436\n",
            "Epoch: 23\n",
            "Train:  0.788805689207569\n",
            "Test 0.6917496919631958\n",
            "0.6929041672253204\n",
            "Epoch: 24\n",
            "Train:  0.7870096024884358\n",
            "Test 0.6913110613822937\n",
            "0.6925843377752148\n",
            "Epoch: 25\n",
            "Train:  0.7860634504939636\n",
            "Test 0.6905233263969421\n",
            "0.6921708859179968\n",
            "Epoch: 26\n",
            "Train:  0.7875359314599188\n",
            "Test 0.6900938749313354\n",
            "0.691754476905451\n",
            "Epoch: 27\n",
            "Train:  0.7857361187762265\n",
            "Test 0.6900729537010193\n",
            "0.6914175204960793\n",
            "Epoch: 28\n",
            "Train:  0.7856722207090974\n",
            "Test 0.6894263625144958\n",
            "0.691018671711162\n",
            "Epoch: 29\n",
            "Train:  0.7850963373529426\n",
            "Test 0.6889449954032898\n",
            "0.6906034223958338\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:23:35,016]\u001b[0m Trial 8 finished with value: 0.689516139791095 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 512, 'layer_size4': 64, 'layer_size5': 64, 'layer_size6': 512, 'learning_rate': 1.103876874557472e-05, 'b1': 0.9174687258295642}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7850164466314187\n",
            "Test 0.6885867118835449\n",
            "0.6901996804467166\n",
            "Epoch: 0\n",
            "Train:  0.6942713889061596\n",
            "Test 0.696393609046936\n",
            "0.6963936090469359\n",
            "Epoch: 1\n",
            "Train:  0.7903133571417623\n",
            "Test 0.6915722489356995\n",
            "0.6937150756518047\n",
            "Epoch: 2\n",
            "Train:  0.7884661630807419\n",
            "Test 0.6875930428504944\n",
            "0.6912060458152021\n",
            "Epoch: 3\n",
            "Train:  0.7856934820365042\n",
            "Test 0.6824374198913574\n",
            "0.6882356440794823\n",
            "Epoch: 4\n",
            "Train:  0.7815385282848755\n",
            "Test 0.6761739253997803\n",
            "0.6846475554670055\n",
            "Epoch: 5\n",
            "Train:  0.7746300087794998\n",
            "Test 0.6733623147010803\n",
            "0.6815886277721911\n",
            "Epoch: 6\n",
            "Train:  0.7652297586337473\n",
            "Test 0.6545498967170715\n",
            "0.6747458462698467\n",
            "Epoch: 7\n",
            "Train:  0.7440653335454777\n",
            "Test 0.6408110857009888\n",
            "0.6665906851618362\n",
            "Epoch: 8\n",
            "Train:  0.7105004771262812\n",
            "Test 0.5906859636306763\n",
            "0.6490563179228729\n",
            "Epoch: 9\n",
            "Train:  0.6570930173494157\n",
            "Test 0.5889896750450134\n",
            "0.6355979030543986\n",
            "Epoch: 10\n",
            "Train:  0.6090544336791491\n",
            "Test 0.5083735585212708\n",
            "0.6077619434223221\n",
            "Epoch: 11\n",
            "Train:  0.5222016451585346\n",
            "Test 0.45734572410583496\n",
            "0.5754588477258072\n",
            "Epoch: 12\n",
            "Train:  0.4291083646845494\n",
            "Test 0.4272107183933258\n",
            "0.5440843929682581\n",
            "Epoch: 13\n",
            "Train:  0.37471564943434427\n",
            "Test 0.42959463596343994\n",
            "0.5201330503802395\n",
            "Epoch: 14\n",
            "Train:  0.32126747779716736\n",
            "Test 0.39790621399879456\n",
            "0.49479622273243984\n",
            "Epoch: 15\n",
            "Train:  0.31474382507855\n",
            "Test 0.4818771183490753\n",
            "0.492137567359859\n",
            "Epoch: 16\n",
            "Train:  0.2881424654258322\n",
            "Test 0.3831968605518341\n",
            "0.4698474982638805\n",
            "Epoch: 17\n",
            "Train:  0.24621394802542293\n",
            "Test 0.3631618022918701\n",
            "0.44811893201808706\n",
            "Epoch: 18\n",
            "Train:  0.21655517971623536\n",
            "Test 0.40524980425834656\n",
            "0.43941973786600536\n",
            "Epoch: 19\n",
            "Train:  0.23292665527417108\n",
            "Test 0.43905648589134216\n",
            "0.4393462400995175\n",
            "Epoch: 20\n",
            "Train:  0.18873773835498284\n",
            "Test 0.37647145986557007\n",
            "0.42665422083717247\n",
            "Epoch: 21\n",
            "Train:  0.17026659714825013\n",
            "Test 0.3720722794532776\n",
            "0.4156566850692929\n",
            "Epoch: 22\n",
            "Train:  0.15591147194755564\n",
            "Test 0.32401159405708313\n",
            "0.3972188289748052\n",
            "Epoch: 23\n",
            "Train:  0.12713929730977408\n",
            "Test 0.33604174852371216\n",
            "0.384925358612188\n",
            "Epoch: 24\n",
            "Train:  0.11866562530330943\n",
            "Test 0.3667224943637848\n",
            "0.3812709799101087\n",
            "Epoch: 25\n",
            "Train:  0.1144576089085834\n",
            "Test 0.3853068947792053\n",
            "0.38208060984024517\n",
            "Epoch: 26\n",
            "Train:  0.11246824320152873\n",
            "Test 0.414602667093277\n",
            "0.38860078610973275\n",
            "Epoch: 27\n",
            "Train:  0.10681279122458594\n",
            "Test 0.38443365693092346\n",
            "0.3877657450696929\n",
            "Epoch: 28\n",
            "Train:  0.09658083802603219\n",
            "Test 0.4257923662662506\n",
            "0.39538285621759656\n",
            "Epoch: 29\n",
            "Train:  0.09938411057265097\n",
            "Test 0.38900279998779297\n",
            "0.3941052633883185\n",
            "Epoch: 30\n",
            "Train:  0.0858758682578937\n",
            "Test 0.401676207780838\n",
            "0.39562095333343866\n",
            "Epoch: 31\n",
            "Train:  0.09640742808883816\n",
            "Test 0.43016141653060913\n",
            "0.4025345234674541\n",
            "Epoch: 32\n",
            "Train:  0.08542974268579308\n",
            "Test 0.4350948929786682\n",
            "0.4090507275046764\n",
            "Epoch: 33\n",
            "Train:  0.08452852212769144\n",
            "Test 0.40736424922943115\n",
            "0.40871326073364544\n",
            "Epoch: 34\n",
            "Train:  0.08122196567678883\n",
            "Test 0.41813012957572937\n",
            "0.4105973987992621\n",
            "Epoch: 35\n",
            "Train:  0.06923687906000274\n",
            "Test 0.44113823771476746\n",
            "0.4167075494396117\n",
            "Epoch: 36\n",
            "Train:  0.06893853136291939\n",
            "Test 0.43022605776786804\n",
            "0.4194119532086226\n",
            "Epoch: 37\n",
            "Train:  0.06856318828502324\n",
            "Test 0.4731009006500244\n",
            "0.4301519733118087\n",
            "Epoch: 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:24:43,047]\u001b[0m Trial 9 finished with value: 0.43348518410733794 and parameters: {'layer_size1': 512, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 512, 'layer_size5': 512, 'layer_size6': 128, 'learning_rate': 0.00011894513623782998, 'b1': 0.9252783541292425}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.0734538331636274\n",
            "Test 0.44717538356781006\n",
            "0.433557221156855\n",
            "Epoch: 0\n",
            "Train:  0.6940247082062976\n",
            "Test 0.6848345398902893\n",
            "0.6848345398902893\n",
            "Epoch: 1\n",
            "Train:  0.7867193737181064\n",
            "Test 0.6882060766220093\n",
            "0.6867076158523561\n",
            "Epoch: 2\n",
            "Train:  0.7866339535195364\n",
            "Test 0.6836200952529907\n",
            "0.6854422385575343\n",
            "Epoch: 3\n",
            "Train:  0.7781717561488777\n",
            "Test 0.678288996219635\n",
            "0.6830190534837202\n",
            "Epoch: 4\n",
            "Train:  0.7706274657227874\n",
            "Test 0.6623867154121399\n",
            "0.6768813993691378\n",
            "Epoch: 5\n",
            "Train:  0.7366670887934137\n",
            "Test 0.6143487691879272\n",
            "0.6599315798430486\n",
            "Epoch: 6\n",
            "Train:  0.6691307870511016\n",
            "Test 0.5564542412757874\n",
            "0.6337442259758702\n",
            "Epoch: 7\n",
            "Train:  0.566562975693612\n",
            "Test 0.4801374673843384\n",
            "0.5968296333105895\n",
            "Epoch: 8\n",
            "Train:  0.4639205172051132\n",
            "Test 0.42237114906311035\n",
            "0.5565288579564185\n",
            "Epoch: 9\n",
            "Train:  0.4160425536907636\n",
            "Test 0.4156738221645355\n",
            "0.5249691533154303\n",
            "Epoch: 10\n",
            "Train:  0.38599201753668116\n",
            "Test 0.4317680895328522\n",
            "0.5045772931047228\n",
            "Epoch: 11\n",
            "Train:  0.34216304656067587\n",
            "Test 0.386111319065094\n",
            "0.47913577015212777\n",
            "Epoch: 12\n",
            "Train:  0.343691439245621\n",
            "Test 0.36589962244033813\n",
            "0.4551710671282206\n",
            "Epoch: 13\n",
            "Train:  0.3165923009765634\n",
            "Test 0.3483307659626007\n",
            "0.4328199964091418\n",
            "Epoch: 14\n",
            "Train:  0.29284774225491744\n",
            "Test 0.3704170882701874\n",
            "0.41988427971183734\n",
            "Epoch: 15\n",
            "Train:  0.2766250428570881\n",
            "Test 0.40292486548423767\n",
            "0.41639415869306995\n",
            "Epoch: 16\n",
            "Train:  0.22371663682466178\n",
            "Test 0.40743255615234375\n",
            "0.4145605489644725\n",
            "Epoch: 17\n",
            "Train:  0.2407029240446932\n",
            "Test 0.2874895930290222\n",
            "0.3886801377359221\n",
            "Epoch: 18\n",
            "Train:  0.1792795382338951\n",
            "Test 0.29618868231773376\n",
            "0.36991136005980974\n",
            "Epoch: 19\n",
            "Train:  0.16459037532094378\n",
            "Test 0.3651899993419647\n",
            "0.36895607422036736\n",
            "Epoch: 20\n",
            "Train:  0.13973826204523512\n",
            "Test 0.29323098063468933\n",
            "0.3536700669714165\n",
            "Epoch: 21\n",
            "Train:  0.13895717654292938\n",
            "Test 0.3411158323287964\n",
            "0.3511405555429113\n",
            "Epoch: 22\n",
            "Train:  0.1304930518655216\n",
            "Test 0.39019715785980225\n",
            "0.35899825970474714\n",
            "Epoch: 23\n",
            "Train:  0.11520456202429344\n",
            "Test 0.3300783634185791\n",
            "0.35318683677866963\n",
            "Epoch: 24\n",
            "Train:  0.10363676739915599\n",
            "Test 0.35523393750190735\n",
            "0.35359780953449116\n",
            "Epoch: 25\n",
            "Train:  0.11144435087143026\n",
            "Test 0.34243983030319214\n",
            "0.35135944865758706\n",
            "Epoch: 26\n",
            "Train:  0.09539248640080113\n",
            "Test 0.30482134222984314\n",
            "0.34202926838036063\n",
            "Epoch: 27\n",
            "Train:  0.08957692150161412\n",
            "Test 0.31711751222610474\n",
            "0.3370372612033206\n",
            "Epoch: 28\n",
            "Train:  0.09239109381359087\n",
            "Test 0.3167724311351776\n",
            "0.3329780138085929\n",
            "Epoch: 29\n",
            "Train:  0.08842471994805187\n",
            "Test 0.29210007190704346\n",
            "0.3247922919955001\n",
            "Epoch: 30\n",
            "Train:  0.08193636436133363\n",
            "Test 0.2917274236679077\n",
            "0.3181727626656627\n",
            "Epoch: 31\n",
            "Train:  0.08307762224884595\n",
            "Test 0.3527792990207672\n",
            "0.32509955790927625\n",
            "Epoch: 32\n",
            "Train:  0.07915036791590005\n",
            "Test 0.40365344285964966\n",
            "0.3408202991028559\n",
            "Epoch: 33\n",
            "Train:  0.08900922245232228\n",
            "Test 0.2578771710395813\n",
            "0.3242232577904483\n",
            "Epoch: 34\n",
            "Train:  0.06948472301968757\n",
            "Test 0.3375031352043152\n",
            "0.3268803111020938\n",
            "Epoch: 35\n",
            "Train:  0.07253654031462259\n",
            "Test 0.2366546392440796\n",
            "0.30882931884859277\n",
            "Epoch: 36\n",
            "Train:  0.0629045159336837\n",
            "Test 0.279328316450119\n",
            "0.30292758619150123\n",
            "Epoch: 37\n",
            "Train:  0.06837557514547897\n",
            "Test 0.23352135717868805\n",
            "0.2890434567680751\n",
            "Epoch: 38\n",
            "Train:  0.05130177829836019\n",
            "Test 0.2674154043197632\n",
            "0.28471712744365557\n",
            "Epoch: 39\n",
            "Train:  0.06133072604994159\n",
            "Test 0.32198765873908997\n",
            "0.2921722246551349\n",
            "Epoch: 40\n",
            "Train:  0.06845903504698862\n",
            "Test 0.2860395610332489\n",
            "0.290945561489556\n",
            "Epoch: 41\n",
            "Train:  0.06323784835872608\n",
            "Test 0.32087987661361694\n",
            "0.2969329338636789\n",
            "Epoch: 42\n",
            "Train:  0.061962420044980974\n",
            "Test 0.3128703832626343\n",
            "0.3001206406875546\n",
            "Epoch: 43\n",
            "Train:  0.05558035118459003\n",
            "Test 0.3241408169269562\n",
            "0.3049249375062338\n",
            "Epoch: 44\n",
            "Train:  0.06251980199128794\n",
            "Test 0.3917849659919739\n",
            "0.3222976998939041\n",
            "Epoch: 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:25:30,587]\u001b[0m Trial 10 finished with value: 0.3487206331784799 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 0.0004097714918306327, 'b1': 0.9618672469663216}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.07342927415362176\n",
            "Test 0.4544685184955597\n",
            "0.3487327847425032\n",
            "Epoch: 0\n",
            "Train:  0.7053411678491135\n",
            "Test 0.6866016983985901\n",
            "0.6866016983985901\n",
            "Epoch: 1\n",
            "Train:  0.7885140070009016\n",
            "Test 0.6913492679595947\n",
            "0.6892392370435927\n",
            "Epoch: 2\n",
            "Train:  0.7898275250762836\n",
            "Test 0.6893279552459717\n",
            "0.6892755969626004\n",
            "Epoch: 3\n",
            "Train:  0.7838789016952342\n",
            "Test 0.6755098700523376\n",
            "0.6846124103398826\n",
            "Epoch: 4\n",
            "Train:  0.7407686834421633\n",
            "Test 0.5909382104873657\n",
            "0.6567464536964638\n",
            "Epoch: 5\n",
            "Train:  0.6142810735497539\n",
            "Test 0.4730195105075836\n",
            "0.6069462370718433\n",
            "Epoch: 6\n",
            "Train:  0.4169749976283285\n",
            "Test 0.6729933023452759\n",
            "0.6236609873171808\n",
            "Epoch: 7\n",
            "Train:  0.6082988914051747\n",
            "Test 0.49980515241622925\n",
            "0.5938961010809906\n",
            "Epoch: 8\n",
            "Train:  0.3846497497946968\n",
            "Test 0.41221338510513306\n",
            "0.5519264923579658\n",
            "Epoch: 9\n",
            "Train:  0.32173600213020637\n",
            "Test 0.426584929227829\n",
            "0.5238427061490608\n",
            "Epoch: 10\n",
            "Train:  0.25122288756106237\n",
            "Test 0.3953838646411896\n",
            "0.49573664563274733\n",
            "Epoch: 11\n",
            "Train:  0.22336403344551362\n",
            "Test 0.4154342710971832\n",
            "0.47849106333406116\n",
            "Epoch: 12\n",
            "Train:  0.19826600319659549\n",
            "Test 0.4250577390193939\n",
            "0.46718271547291434\n",
            "Epoch: 13\n",
            "Train:  0.1572256360152458\n",
            "Test 0.49034082889556885\n",
            "0.4720274100795668\n",
            "Epoch: 14\n",
            "Train:  0.15059546655996353\n",
            "Test 0.45010650157928467\n",
            "0.46748334842340544\n",
            "Epoch: 15\n",
            "Train:  0.13654294272893155\n",
            "Test 0.39557111263275146\n",
            "0.4526843463900054\n",
            "Epoch: 16\n",
            "Train:  0.10445832188043119\n",
            "Test 0.3817465007305145\n",
            "0.4381699419254161\n",
            "Epoch: 17\n",
            "Train:  0.11980037583350056\n",
            "Test 0.5865445137023926\n",
            "0.4683892387362379\n",
            "Epoch: 18\n",
            "Train:  0.15548981486807043\n",
            "Test 0.39919513463974\n",
            "0.454348063252372\n",
            "Epoch: 19\n",
            "Train:  0.09781435378131824\n",
            "Test 0.3463321328163147\n",
            "0.4324929043381727\n",
            "Epoch: 20\n",
            "Train:  0.08675536307679042\n",
            "Test 0.3925994038581848\n",
            "0.4244399286514124\n",
            "Epoch: 21\n",
            "Train:  0.09159069597653674\n",
            "Test 0.7108712196350098\n",
            "0.48215202697119663\n",
            "Epoch: 22\n",
            "Train:  0.2688093019219545\n",
            "Test 0.9403934478759766\n",
            "0.574344519573449\n",
            "Epoch: 23\n",
            "Train:  0.3711285796774998\n",
            "Test 0.39120420813560486\n",
            "0.5375426654434291\n",
            "Epoch: 24\n",
            "Train:  0.3109621542476421\n",
            "Test 0.41156062483787537\n",
            "0.5122507070048652\n",
            "Epoch: 25\n",
            "Train:  0.19825013396561955\n",
            "Test 0.40296444296836853\n",
            "0.490327194446595\n",
            "Epoch: 26\n",
            "Train:  0.1347135532377667\n",
            "Test 0.4438022971153259\n",
            "0.48099966239167224\n",
            "Epoch: 27\n",
            "Train:  0.09808673899898904\n",
            "Test 0.6727917790412903\n",
            "0.5194324254970154\n",
            "Epoch: 28\n",
            "Train:  0.11578329209432141\n",
            "Test 0.6973537802696228\n",
            "0.5550718457832233\n",
            "Epoch: 29\n",
            "Train:  0.11146398232533382\n",
            "Test 0.7731450200080872\n",
            "0.5987405398530484\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:26:01,756]\u001b[0m Trial 11 finished with value: 0.6459231500899589 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 0.0009942964278178714, 'b1': 0.9004201022501528}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.11240926501235811\n",
            "Test 0.8376184105873108\n",
            "0.6465634755414087\n",
            "Epoch: 0\n",
            "Train:  0.694074345659886\n",
            "Test 0.6896198987960815\n",
            "0.6896198987960815\n",
            "Epoch: 1\n",
            "Train:  0.7910327164296111\n",
            "Test 0.6862584948539734\n",
            "0.6877524521615771\n",
            "Epoch: 2\n",
            "Train:  0.7883264039436616\n",
            "Test 0.687175452709198\n",
            "0.6875159769761758\n",
            "Epoch: 3\n",
            "Train:  0.7811543588185202\n",
            "Test 0.6789257526397705\n",
            "0.6846060093825426\n",
            "Epoch: 4\n",
            "Train:  0.7570395043532773\n",
            "Test 0.6422778964042664\n",
            "0.6720143527374104\n",
            "Epoch: 5\n",
            "Train:  0.6751612012202923\n",
            "Test 0.5305999517440796\n",
            "0.6336831875796206\n",
            "Epoch: 6\n",
            "Train:  0.5422514172700735\n",
            "Test 0.43532484769821167\n",
            "0.5834839834746172\n",
            "Epoch: 7\n",
            "Train:  0.416750683099436\n",
            "Test 0.4225960671901703\n",
            "0.5448195916935282\n",
            "Epoch: 8\n",
            "Train:  0.38452465444533535\n",
            "Test 0.4658878743648529\n",
            "0.5265859734083354\n",
            "Epoch: 9\n",
            "Train:  0.3562059858266045\n",
            "Test 0.44191795587539673\n",
            "0.5076154225092845\n",
            "Epoch: 10\n",
            "Train:  0.33603786060173585\n",
            "Test 0.42409756779670715\n",
            "0.4893421931138555\n",
            "Epoch: 11\n",
            "Train:  0.2588685908064044\n",
            "Test 0.3963835835456848\n",
            "0.4693785823121683\n",
            "Epoch: 12\n",
            "Train:  0.23101671835685747\n",
            "Test 0.4082601070404053\n",
            "0.4564437895029873\n",
            "Epoch: 13\n",
            "Train:  0.20667135560404662\n",
            "Test 0.45052823424339294\n",
            "0.4552062509289912\n",
            "Epoch: 14\n",
            "Train:  0.18519623577594757\n",
            "Test 0.4086605906486511\n",
            "0.44555763850319297\n",
            "Epoch: 15\n",
            "Train:  0.18493085411878732\n",
            "Test 0.6046465635299683\n",
            "0.47829695329541433\n",
            "Epoch: 16\n",
            "Train:  0.21766145851007954\n",
            "Test 0.45073971152305603\n",
            "0.4726585391415307\n",
            "Epoch: 17\n",
            "Train:  0.18529159413878196\n",
            "Test 0.7544357776641846\n",
            "0.5300478202256841\n",
            "Epoch: 18\n",
            "Train:  0.2202835707373209\n",
            "Test 0.40231120586395264\n",
            "0.5041269380726805\n",
            "Epoch: 19\n",
            "Train:  0.16793663732336658\n",
            "Test 0.6257563233375549\n",
            "0.5287365445711757\n",
            "Epoch: 20\n",
            "Train:  0.23401199638573833\n",
            "Test 0.33597812056541443\n",
            "0.4898259730931146\n",
            "Epoch: 21\n",
            "Train:  0.16651791276840064\n",
            "Test 0.33574992418289185\n",
            "0.4587816969845387\n",
            "Epoch: 22\n",
            "Train:  0.12040253752227283\n",
            "Test 0.578230082988739\n",
            "0.48281323132575993\n",
            "Epoch: 23\n",
            "Train:  0.15580448285756607\n",
            "Test 0.44718149304389954\n",
            "0.4756530707671539\n",
            "Epoch: 24\n",
            "Train:  0.12731163187702588\n",
            "Test 0.38623058795928955\n",
            "0.4577007522638291\n",
            "Epoch: 25\n",
            "Train:  0.10676081932878873\n",
            "Test 0.4401918947696686\n",
            "0.45418836522655237\n",
            "Epoch: 26\n",
            "Train:  0.1067536599866675\n",
            "Test 0.7067074775695801\n",
            "0.5048145944063682\n",
            "Epoch: 27\n",
            "Train:  0.154102803797912\n",
            "Test 0.39662110805511475\n",
            "0.48313396069150394\n",
            "Epoch: 28\n",
            "Train:  0.1056801331360146\n",
            "Test 0.4299888014793396\n",
            "0.4724884557280302\n",
            "Epoch: 29\n",
            "Train:  0.10563089735637424\n",
            "Test 0.32831546664237976\n",
            "0.44361811816408203\n",
            "Epoch: 30\n",
            "Train:  0.06494689947289174\n",
            "Test 0.5239811539649963\n",
            "0.45970665864301086\n",
            "Epoch: 31\n",
            "Train:  0.10202157318726923\n",
            "Test 0.3614518344402313\n",
            "0.4400401123592043\n",
            "Epoch: 32\n",
            "Train:  0.06672512098611648\n",
            "Test 0.282263845205307\n",
            "0.4084648457255746\n",
            "Epoch: 33\n",
            "Train:  0.05962329732077154\n",
            "Test 0.40111416578292847\n",
            "0.4069939639113595\n",
            "Epoch: 34\n",
            "Train:  0.06746801411747781\n",
            "Test 0.2941010594367981\n",
            "0.38440622033910415\n",
            "Epoch: 35\n",
            "Train:  0.04809727677780444\n",
            "Test 0.3327254056930542\n",
            "0.3740667020443716\n",
            "Epoch: 36\n",
            "Train:  0.05430416315709709\n",
            "Test 0.5318924784660339\n",
            "0.40564005423957483\n",
            "Epoch: 37\n",
            "Train:  0.087915041616157\n",
            "Test 0.4228520095348358\n",
            "0.40908316040379983\n",
            "Epoch: 38\n",
            "Train:  0.07365072285545762\n",
            "Test 0.4036839008331299\n",
            "0.4080031290386749\n",
            "Epoch: 39\n",
            "Train:  0.08523283706126182\n",
            "Test 0.7240751385688782\n",
            "0.4712259346970409\n",
            "Epoch: 40\n",
            "Train:  0.1102546247615736\n",
            "Test 0.5065876245498657\n",
            "0.4782990248075572\n",
            "Epoch: 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:26:43,960]\u001b[0m Trial 12 finished with value: 0.49154194535309176 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 0.0008112495009036293, 'b1': 0.9656543396440461}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.07964293141923086\n",
            "Test 0.5447170734405518\n",
            "0.4915837646748376\n",
            "Epoch: 0\n",
            "Train:  0.6933524740767155\n",
            "Test 0.6893916726112366\n",
            "0.6893916726112366\n",
            "Epoch: 1\n",
            "Train:  0.7885456818800706\n",
            "Test 0.6862981915473938\n",
            "0.6876730720202129\n",
            "Epoch: 2\n",
            "Train:  0.7875125834305362\n",
            "Test 0.6833593845367432\n",
            "0.685905167313873\n",
            "Epoch: 3\n",
            "Train:  0.7850641880639538\n",
            "Test 0.6851909160614014\n",
            "0.6856632122825478\n",
            "Epoch: 4\n",
            "Train:  0.7821752113454482\n",
            "Test 0.6771149635314941\n",
            "0.6831203015403257\n",
            "Epoch: 5\n",
            "Train:  0.7758737972419186\n",
            "Test 0.6682924628257751\n",
            "0.6791011328367981\n",
            "Epoch: 6\n",
            "Train:  0.7658417934206276\n",
            "Test 0.6599764823913574\n",
            "0.6742611940083046\n",
            "Epoch: 7\n",
            "Train:  0.751148357110865\n",
            "Test 0.6389813423156738\n",
            "0.6657827822088074\n",
            "Epoch: 8\n",
            "Train:  0.7273178079009596\n",
            "Test 0.6097890138626099\n",
            "0.6528479439343184\n",
            "Epoch: 9\n",
            "Train:  0.6910367837319007\n",
            "Test 0.5751355886459351\n",
            "0.635435865153687\n",
            "Epoch: 10\n",
            "Train:  0.6308805675528169\n",
            "Test 0.5222429633140564\n",
            "0.6106699050081811\n",
            "Epoch: 11\n",
            "Train:  0.5641329605385189\n",
            "Test 0.4625282883644104\n",
            "0.5788552985726129\n",
            "Epoch: 12\n",
            "Train:  0.49723798144457027\n",
            "Test 0.4648289382457733\n",
            "0.5547233591049722\n",
            "Epoch: 13\n",
            "Train:  0.4242911712346573\n",
            "Test 0.43698960542678833\n",
            "0.5300933699924886\n",
            "Epoch: 14\n",
            "Train:  0.3823995327248293\n",
            "Test 0.39741069078445435\n",
            "0.5025891141803615\n",
            "Epoch: 15\n",
            "Train:  0.3109892546388898\n",
            "Test 0.35709285736083984\n",
            "0.4726470691743845\n",
            "Epoch: 16\n",
            "Train:  0.2963388934394353\n",
            "Test 0.3418763279914856\n",
            "0.44589041464087065\n",
            "Epoch: 17\n",
            "Train:  0.25008325848509283\n",
            "Test 0.32026389241218567\n",
            "0.42030418974347883\n",
            "Epoch: 18\n",
            "Train:  0.23379590149918292\n",
            "Test 0.28222447633743286\n",
            "0.3922844399105688\n",
            "Epoch: 19\n",
            "Train:  0.20076083894229044\n",
            "Test 0.3082745373249054\n",
            "0.3752864863312774\n",
            "Epoch: 20\n",
            "Train:  0.1782484027595002\n",
            "Test 0.3112269639968872\n",
            "0.36235531284042005\n",
            "Epoch: 21\n",
            "Train:  0.17958822040671138\n",
            "Test 0.2926277220249176\n",
            "0.3483061300052374\n",
            "Epoch: 22\n",
            "Train:  0.17357464728042551\n",
            "Test 0.2838234603404999\n",
            "0.33533301632576695\n",
            "Epoch: 23\n",
            "Train:  0.14148740480523303\n",
            "Test 0.3056475818157196\n",
            "0.3293677592938884\n",
            "Epoch: 24\n",
            "Train:  0.14062603343935573\n",
            "Test 0.30824699997901917\n",
            "0.32512758851863405\n",
            "Epoch: 25\n",
            "Train:  0.13807410173691237\n",
            "Test 0.3034113347530365\n",
            "0.32077117130238014\n",
            "Epoch: 26\n",
            "Train:  0.11602010196959811\n",
            "Test 0.3089551627635956\n",
            "0.3184022418747234\n",
            "Epoch: 27\n",
            "Train:  0.11235921775998034\n",
            "Test 0.304142564535141\n",
            "0.3155447792702932\n",
            "Epoch: 28\n",
            "Train:  0.09544043890221626\n",
            "Test 0.3102312684059143\n",
            "0.3144804300968454\n",
            "Epoch: 29\n",
            "Train:  0.08471644221387838\n",
            "Test 0.32504427433013916\n",
            "0.31659581766647515\n",
            "Epoch: 30\n",
            "Train:  0.08034390462267453\n",
            "Test 0.33226636052131653\n",
            "0.3197330331852054\n",
            "Epoch: 31\n",
            "Train:  0.0868502584248107\n",
            "Test 0.32412442564964294\n",
            "0.32061200807374607\n",
            "Epoch: 32\n",
            "Train:  0.07853498481787168\n",
            "Test 0.3101077377796173\n",
            "0.3185098215959451\n",
            "Epoch: 33\n",
            "Train:  0.07503653067370615\n",
            "Test 0.2945871353149414\n",
            "0.3137228570603576\n",
            "Epoch: 34\n",
            "Train:  0.06808299615524069\n",
            "Test 0.3391393721103668\n",
            "0.3188082229398344\n",
            "Epoch: 35\n",
            "Train:  0.07431689865583747\n",
            "Test 0.29635584354400635\n",
            "0.3143162893448755\n",
            "Epoch: 36\n",
            "Train:  0.059623831592297935\n",
            "Test 0.27637675404548645\n",
            "0.30672641184014143\n",
            "Epoch: 37\n",
            "Train:  0.05097306593494043\n",
            "Test 0.2869393229484558\n",
            "0.30276817196754613\n",
            "Epoch: 38\n",
            "Train:  0.05227770055412437\n",
            "Test 0.4288169741630554\n",
            "0.3279821217926469\n",
            "Epoch: 39\n",
            "Train:  0.06937964509211784\n",
            "Test 0.34214669466018677\n",
            "0.3308154129751509\n",
            "Epoch: 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:27:32,754]\u001b[0m Trial 13 finished with value: 0.3701955626769094 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 0.00019769349839011765, 'b1': 0.9559616469041498}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.056993891742097305\n",
            "Test 0.5278920531272888\n",
            "0.37023493280792585\n",
            "Epoch: 0\n",
            "Train:  0.6947798844915709\n",
            "Test 0.700781524181366\n",
            "0.700781524181366\n",
            "Epoch: 1\n",
            "Train:  0.7926282656138839\n",
            "Test 0.7006068229675293\n",
            "0.7006844679514569\n",
            "Epoch: 2\n",
            "Train:  0.7926157777665427\n",
            "Test 0.7004784345626831\n",
            "0.7006000280380249\n",
            "Epoch: 3\n",
            "Train:  0.793483190407041\n",
            "Test 0.7003237009048462\n",
            "0.700506421285593\n",
            "Epoch: 4\n",
            "Train:  0.7926166807364554\n",
            "Test 0.7001786231994629\n",
            "0.7004089087659208\n",
            "Epoch: 5\n",
            "Train:  0.7926684644966643\n",
            "Test 0.7000365853309631\n",
            "0.7003079884142648\n",
            "Epoch: 6\n",
            "Train:  0.7932855913002567\n",
            "Test 0.6999138593673706\n",
            "0.7002082448669021\n",
            "Epoch: 7\n",
            "Train:  0.792667112738838\n",
            "Test 0.6997736096382141\n",
            "0.700103793848131\n",
            "Epoch: 8\n",
            "Train:  0.7925623374826768\n",
            "Test 0.6996486186981201\n",
            "0.6999986461303429\n",
            "Epoch: 9\n",
            "Train:  0.7926350993808039\n",
            "Test 0.6995511054992676\n",
            "0.6998983710492841\n",
            "Epoch: 10\n",
            "Train:  0.7923330260078292\n",
            "Test 0.6994416117668152\n",
            "0.6997984347281733\n",
            "Epoch: 11\n",
            "Train:  0.7925654560732086\n",
            "Test 0.6993260383605957\n",
            "0.6996969837999563\n",
            "Epoch: 12\n",
            "Train:  0.7923478608217714\n",
            "Test 0.6992139220237732\n",
            "0.699594751145081\n",
            "Epoch: 13\n",
            "Train:  0.7918273067582247\n",
            "Test 0.6990916132926941\n",
            "0.6994894943309626\n",
            "Epoch: 14\n",
            "Train:  0.7921034229826603\n",
            "Test 0.6989790201187134\n",
            "0.6993836763492712\n",
            "Epoch: 15\n",
            "Train:  0.7918788601909827\n",
            "Test 0.6988350749015808\n",
            "0.6992707782610581\n",
            "Epoch: 16\n",
            "Train:  0.7918556329891153\n",
            "Test 0.6986687779426575\n",
            "0.6991476045724884\n",
            "Epoch: 17\n",
            "Train:  0.7916581962443046\n",
            "Test 0.6984812021255493\n",
            "0.6990118790698074\n",
            "Epoch: 18\n",
            "Train:  0.7917149118168861\n",
            "Test 0.698329508304596\n",
            "0.6988734093579079\n",
            "Epoch: 19\n",
            "Train:  0.7913217841230367\n",
            "Test 0.6981824040412903\n",
            "0.6987335963604707\n",
            "Epoch: 20\n",
            "Train:  0.7922383320277633\n",
            "Test 0.6980697512626648\n",
            "0.6985995913629626\n",
            "Epoch: 21\n",
            "Train:  0.7908330093142134\n",
            "Test 0.6979495882987976\n",
            "0.6984686243843953\n",
            "Epoch: 22\n",
            "Train:  0.79176088479849\n",
            "Test 0.6978548765182495\n",
            "0.6983451459229816\n",
            "Epoch: 23\n",
            "Train:  0.7914434401697703\n",
            "Test 0.6977643370628357\n",
            "0.6982284329897082\n",
            "Epoch: 24\n",
            "Train:  0.7913125561912675\n",
            "Test 0.6976485252380371\n",
            "0.6981120116118439\n",
            "Epoch: 25\n",
            "Train:  0.7909750091544104\n",
            "Test 0.6975308656692505\n",
            "0.6979954300772571\n",
            "Epoch: 26\n",
            "Train:  0.7913509278275848\n",
            "Test 0.6974271535873413\n",
            "0.6978814993115854\n",
            "Epoch: 27\n",
            "Train:  0.7904490666691534\n",
            "Test 0.697314441204071\n",
            "0.6977678678949569\n",
            "Epoch: 28\n",
            "Train:  0.7912571610908163\n",
            "Test 0.6971871852874756\n",
            "0.6976515513823754\n",
            "Epoch: 29\n",
            "Train:  0.7911300505448251\n",
            "Test 0.6970682740211487\n",
            "0.6975347513186547\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:28:38,784]\u001b[0m Trial 14 finished with value: 0.6967297892755219 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 4.203567531921537e-06, 'b1': 0.9334744590317524}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7905596672679505\n",
            "Test 0.6969639658927917\n",
            "0.697420481065705\n",
            "Epoch: 0\n",
            "Train:  0.6934091413722319\n",
            "Test 0.694399356842041\n",
            "0.694399356842041\n",
            "Epoch: 1\n",
            "Train:  0.7908756859701683\n",
            "Test 0.6933620572090149\n",
            "0.6938230792681377\n",
            "Epoch: 2\n",
            "Train:  0.7900560028952172\n",
            "Test 0.6924806237220764\n",
            "0.6932728925689322\n",
            "Epoch: 3\n",
            "Train:  0.7896345732438618\n",
            "Test 0.6918781399726868\n",
            "0.6928004154021823\n",
            "Epoch: 4\n",
            "Train:  0.7882285528053525\n",
            "Test 0.6914405226707458\n",
            "0.6923958780594179\n",
            "Epoch: 5\n",
            "Train:  0.7871909513732427\n",
            "Test 0.6902108192443848\n",
            "0.691803605633624\n",
            "Epoch: 6\n",
            "Train:  0.7870345695525812\n",
            "Test 0.6881992220878601\n",
            "0.6908914323143941\n",
            "Epoch: 7\n",
            "Train:  0.784353168032288\n",
            "Test 0.6860122084617615\n",
            "0.6897188630687816\n",
            "Epoch: 8\n",
            "Train:  0.7829963656572195\n",
            "Test 0.6840127110481262\n",
            "0.6884007136436441\n",
            "Epoch: 9\n",
            "Train:  0.7788644821395702\n",
            "Test 0.6804237365722656\n",
            "0.6866134077167718\n",
            "Epoch: 10\n",
            "Train:  0.7760722305440255\n",
            "Test 0.6743294596672058\n",
            "0.6839257500732207\n",
            "Epoch: 11\n",
            "Train:  0.7677393118720249\n",
            "Test 0.6703035831451416\n",
            "0.6810002799086916\n",
            "Epoch: 12\n",
            "Train:  0.7600107910406536\n",
            "Test 0.6633155345916748\n",
            "0.6772575733777839\n",
            "Epoch: 13\n",
            "Train:  0.7520703886429109\n",
            "Test 0.6511656045913696\n",
            "0.6717991140400373\n",
            "Epoch: 14\n",
            "Train:  0.7387732809485353\n",
            "Test 0.6388123631477356\n",
            "0.6649611752799204\n",
            "Epoch: 15\n",
            "Train:  0.7156169781318078\n",
            "Test 0.6239522099494934\n",
            "0.6565218359291128\n",
            "Epoch: 16\n",
            "Train:  0.7006938789225272\n",
            "Test 0.6017496585845947\n",
            "0.6453150459851305\n",
            "Epoch: 17\n",
            "Train:  0.6718101922203513\n",
            "Test 0.5761810541152954\n",
            "0.6312345967881362\n",
            "Epoch: 18\n",
            "Train:  0.6399750016393705\n",
            "Test 0.5454912781715393\n",
            "0.6138351810577778\n",
            "Epoch: 19\n",
            "Train:  0.5960117014824535\n",
            "Test 0.510834813117981\n",
            "0.5929948346342672\n",
            "Epoch: 20\n",
            "Train:  0.5606342837281896\n",
            "Test 0.48331737518310547\n",
            "0.5708551401046054\n",
            "Epoch: 21\n",
            "Train:  0.5050919823247383\n",
            "Test 0.4431230127811432\n",
            "0.5451188140720247\n",
            "Epoch: 22\n",
            "Train:  0.457245487972622\n",
            "Test 0.4203299582004547\n",
            "0.5200128434047118\n",
            "Epoch: 23\n",
            "Train:  0.41896673146955565\n",
            "Test 0.3880360424518585\n",
            "0.4934922432207266\n",
            "Epoch: 24\n",
            "Train:  0.37889959706979637\n",
            "Test 0.37279823422431946\n",
            "0.4692619017796132\n",
            "Epoch: 25\n",
            "Train:  0.3528422184659345\n",
            "Test 0.3611222207546234\n",
            "0.447568400991564\n",
            "Epoch: 26\n",
            "Train:  0.309054747156428\n",
            "Test 0.3552861511707306\n",
            "0.4290672179115596\n",
            "Epoch: 27\n",
            "Train:  0.3167650174366403\n",
            "Test 0.3481537699699402\n",
            "0.412853165785226\n",
            "Epoch: 28\n",
            "Train:  0.2884594551982923\n",
            "Test 0.3446659445762634\n",
            "0.39919458591480683\n",
            "Epoch: 29\n",
            "Train:  0.2948369145797928\n",
            "Test 0.3497036099433899\n",
            "0.38928412216063424\n",
            "Epoch: 30\n",
            "Train:  0.2697014013701435\n",
            "Test 0.36872056126594543\n",
            "0.3851673329111002\n",
            "Epoch: 31\n",
            "Train:  0.25973834468228785\n",
            "Test 0.3467518389225006\n",
            "0.37747814210879543\n",
            "Epoch: 32\n",
            "Train:  0.2346284192611729\n",
            "Test 0.3245942294597626\n",
            "0.36689465149487427\n",
            "Epoch: 33\n",
            "Train:  0.24408246826262495\n",
            "Test 0.3262888789176941\n",
            "0.35876937697578976\n",
            "Epoch: 34\n",
            "Train:  0.20730685800044246\n",
            "Test 0.3303121030330658\n",
            "0.3530756125219882\n",
            "Epoch: 35\n",
            "Train:  0.19427457788950717\n",
            "Test 0.3203941285610199\n",
            "0.34653719389163745\n",
            "Epoch: 36\n",
            "Train:  0.1717910264951611\n",
            "Test 0.3256625831127167\n",
            "0.34236118758263134\n",
            "Epoch: 37\n",
            "Train:  0.18090051807024898\n",
            "Test 0.33079737424850464\n",
            "0.340047944474009\n",
            "Epoch: 38\n",
            "Train:  0.18574377004377443\n",
            "Test 0.3319327235221863\n",
            "0.3384246305643576\n",
            "Epoch: 39\n",
            "Train:  0.1649387158891734\n",
            "Test 0.3336832821369171\n",
            "0.33747623481545147\n",
            "Epoch: 40\n",
            "Train:  0.16471856724622563\n",
            "Test 0.34068384766578674\n",
            "0.3381178256111543\n",
            "Epoch: 41\n",
            "Train:  0.16051466607939605\n",
            "Test 0.3479136824607849\n",
            "0.3400771636631279\n",
            "Epoch: 42\n",
            "Train:  0.1446006166921482\n",
            "Test 0.3587304949760437\n",
            "0.3438080838389807\n",
            "Epoch: 43\n",
            "Train:  0.13302602838067448\n",
            "Test 0.3706858456134796\n",
            "0.3491839288827246\n",
            "Epoch: 44\n",
            "Train:  0.15230570441929464\n",
            "Test 0.3779141306877136\n",
            "0.35493021952997933\n",
            "Epoch: 45\n",
            "Train:  0.15632499842082753\n",
            "Test 0.37842780351638794\n",
            "0.3596299000872276\n",
            "Epoch: 46\n",
            "Train:  0.13151443560873222\n",
            "Test 0.39036935567855835\n",
            "0.36577796258846285\n",
            "Epoch: 47\n",
            "Train:  0.13514792851732868\n",
            "Test 0.3864481747150421\n",
            "0.36991209720806145\n",
            "Epoch: 48\n",
            "Train:  0.13605861598415073\n",
            "Test 0.37894150614738464\n",
            "0.3717180112145086\n",
            "Epoch: 49\n",
            "Train:  0.12831210915622937\n",
            "Test 0.3985203504562378\n",
            "0.3770785555711001\n",
            "Epoch: 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:29:33,790]\u001b[0m Trial 15 finished with value: 0.3782459894851957 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 7.641068189900952e-05, 'b1': 0.9544040641892478}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.1332781308275812\n",
            "Test 0.3829372525215149\n",
            "0.3782503083402345\n",
            "Epoch: 0\n",
            "Train:  0.6933290537665872\n",
            "Test 0.6847971081733704\n",
            "0.6847971081733704\n",
            "Epoch: 1\n",
            "Train:  0.7892771471140072\n",
            "Test 0.6912344098091125\n",
            "0.6883733868598939\n",
            "Epoch: 2\n",
            "Train:  0.7863641200561868\n",
            "Test 0.6748375296592712\n",
            "0.6828259044006223\n",
            "Epoch: 3\n",
            "Train:  0.7725000810299524\n",
            "Test 0.662320613861084\n",
            "0.6758796677679874\n",
            "Epoch: 4\n",
            "Train:  0.7624812964939963\n",
            "Test 0.6404320001602173\n",
            "0.6653347880655333\n",
            "Epoch: 5\n",
            "Train:  0.7148138311653655\n",
            "Test 0.5815446376800537\n",
            "0.6426229986688272\n",
            "Epoch: 6\n",
            "Train:  0.640746202404143\n",
            "Test 0.5359676480293274\n",
            "0.6156313747278125\n",
            "Epoch: 7\n",
            "Train:  0.5317660196604233\n",
            "Test 0.4394279718399048\n",
            "0.5732863835081226\n",
            "Epoch: 8\n",
            "Train:  0.44834511973199803\n",
            "Test 0.5089653730392456\n",
            "0.5584279109916336\n",
            "Epoch: 9\n",
            "Train:  0.46640854102993445\n",
            "Test 0.47204843163490295\n",
            "0.5390738932938307\n",
            "Epoch: 10\n",
            "Train:  0.40666651847135965\n",
            "Test 0.43404144048690796\n",
            "0.5160933927683972\n",
            "Epoch: 11\n",
            "Train:  0.36009564281049355\n",
            "Test 0.46092870831489563\n",
            "0.5042463320653219\n",
            "Epoch: 12\n",
            "Train:  0.32858280304869913\n",
            "Test 0.40255284309387207\n",
            "0.48272445665793345\n",
            "Epoch: 13\n",
            "Train:  0.2844262781860602\n",
            "Test 0.418082058429718\n",
            "0.46920121871787956\n",
            "Epoch: 14\n",
            "Train:  0.28404643302319815\n",
            "Test 0.3601439893245697\n",
            "0.44659436488210175\n",
            "Epoch: 15\n",
            "Train:  0.2608582401032901\n",
            "Test 0.3574546277523041\n",
            "0.428250071500918\n",
            "Epoch: 16\n",
            "Train:  0.23063429906896876\n",
            "Test 0.40432509779930115\n",
            "0.4233548460837719\n",
            "Epoch: 17\n",
            "Train:  0.22745769899085636\n",
            "Test 0.30813777446746826\n",
            "0.39988870331316095\n",
            "Epoch: 18\n",
            "Train:  0.1823821311555297\n",
            "Test 0.3563237488269806\n",
            "0.3910483089056547\n",
            "Epoch: 19\n",
            "Train:  0.15086422476293815\n",
            "Test 0.3071448802947998\n",
            "0.37407189849717404\n",
            "Epoch: 20\n",
            "Train:  0.13581718045931596\n",
            "Test 0.2979743778705597\n",
            "0.3587107124378991\n",
            "Epoch: 21\n",
            "Train:  0.11918447213036712\n",
            "Test 0.5043238997459412\n",
            "0.3880498344092487\n",
            "Epoch: 22\n",
            "Train:  0.18961791182358273\n",
            "Test 0.30388733744621277\n",
            "0.3711173834681451\n",
            "Epoch: 23\n",
            "Train:  0.13558543591477754\n",
            "Test 0.30570197105407715\n",
            "0.3579722247282864\n",
            "Epoch: 24\n",
            "Train:  0.09967842466109411\n",
            "Test 0.5635251402854919\n",
            "0.39923870820648216\n",
            "Epoch: 25\n",
            "Train:  0.17914768184505706\n",
            "Test 0.37171047925949097\n",
            "0.39371637218056815\n",
            "Epoch: 26\n",
            "Train:  0.16121331032584696\n",
            "Test 0.5135656595230103\n",
            "0.4177443256758023\n",
            "Epoch: 27\n",
            "Train:  0.16010051958970895\n",
            "Test 0.30943620204925537\n",
            "0.3960407200717834\n",
            "Epoch: 28\n",
            "Train:  0.10172113971742569\n",
            "Test 0.7553263902664185\n",
            "0.46800921997019984\n",
            "Epoch: 29\n",
            "Train:  0.17409922362503288\n",
            "Test 0.28892597556114197\n",
            "0.43214817726777643\n",
            "Epoch: 30\n",
            "Train:  0.09485853849425575\n",
            "Test 0.37570297718048096\n",
            "0.4208479460433686\n",
            "Epoch: 31\n",
            "Train:  0.12086910804590353\n",
            "Test 0.31413376331329346\n",
            "0.3994881865523915\n",
            "Epoch: 32\n",
            "Train:  0.0835466658300404\n",
            "Test 0.4753321707248688\n",
            "0.414666603851787\n",
            "Epoch: 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:30:55,112]\u001b[0m Trial 16 finished with value: 0.47500363298119574 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 0.0004610776312706641, 'b1': 0.9839734285946085}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.12994842747083077\n",
            "Test 0.7174030542373657\n",
            "0.4752446106275626\n",
            "Epoch: 0\n",
            "Train:  0.6922305420513066\n",
            "Test 0.6873655915260315\n",
            "0.6873655915260316\n",
            "Epoch: 1\n",
            "Train:  0.7890662521258738\n",
            "Test 0.6873241662979126\n",
            "0.6873425775104101\n",
            "Epoch: 2\n",
            "Train:  0.7893806415445664\n",
            "Test 0.6873167753219604\n",
            "0.6873320028430127\n",
            "Epoch: 3\n",
            "Train:  0.7893548343516044\n",
            "Test 0.6873056888580322\n",
            "0.687323088891461\n",
            "Epoch: 4\n",
            "Train:  0.7893361640731673\n",
            "Test 0.6872690320014954\n",
            "0.6873070081888296\n",
            "Epoch: 5\n",
            "Train:  0.7900434869986314\n",
            "Test 0.6872238516807556\n",
            "0.6872844681517293\n",
            "Epoch: 6\n",
            "Train:  0.7896151612786686\n",
            "Test 0.6872207522392273\n",
            "0.6872683433540608\n",
            "Epoch: 7\n",
            "Train:  0.7898218550833103\n",
            "Test 0.6872209310531616\n",
            "0.6872569492865662\n",
            "Epoch: 8\n",
            "Train:  0.7899199880625867\n",
            "Test 0.687215268611908\n",
            "0.6872473208439412\n",
            "Epoch: 9\n",
            "Train:  0.7893611378799197\n",
            "Test 0.6872087717056274\n",
            "0.6872386835991846\n",
            "Epoch: 10\n",
            "Train:  0.7895686801742104\n",
            "Test 0.6871941089630127\n",
            "0.6872289309234868\n",
            "Epoch: 11\n",
            "Train:  0.7894782213603749\n",
            "Test 0.6871736645698547\n",
            "0.6872170620285079\n",
            "Epoch: 12\n",
            "Train:  0.7891730779436379\n",
            "Test 0.6871510744094849\n",
            "0.6872030967557121\n",
            "Epoch: 13\n",
            "Train:  0.7891623725718503\n",
            "Test 0.6871588826179504\n",
            "0.6871938471251036\n",
            "Epoch: 14\n",
            "Train:  0.7892539509281313\n",
            "Test 0.6871484518051147\n",
            "0.6871844369707331\n",
            "Epoch: 15\n",
            "Train:  0.788612279147584\n",
            "Test 0.687161922454834\n",
            "0.6871798036512026\n",
            "Epoch: 16\n",
            "Train:  0.7882300024658306\n",
            "Test 0.6871520280838013\n",
            "0.6871741205660208\n",
            "Epoch: 17\n",
            "Train:  0.7888313127319198\n",
            "Test 0.6871290802955627\n",
            "0.6871649472603452\n",
            "Epoch: 18\n",
            "Train:  0.789062222474301\n",
            "Test 0.6870900392532349\n",
            "0.6871497465942376\n",
            "Epoch: 19\n",
            "Train:  0.7890394672549148\n",
            "Test 0.6870574355125427\n",
            "0.6871310690403634\n",
            "Epoch: 20\n",
            "Train:  0.7885941765966459\n",
            "Test 0.6870366930961609\n",
            "0.6871120181379624\n",
            "Epoch: 21\n",
            "Train:  0.7889947284400732\n",
            "Test 0.6869896650314331\n",
            "0.68708736561313\n",
            "Epoch: 22\n",
            "Train:  0.78868431230476\n",
            "Test 0.6869590282440186\n",
            "0.6870615457255934\n",
            "Epoch: 23\n",
            "Train:  0.7881690687183881\n",
            "Test 0.6869162917137146\n",
            "0.6870323570837537\n",
            "Epoch: 24\n",
            "Train:  0.7886240541665263\n",
            "Test 0.6868841648101807\n",
            "0.6870026062335046\n",
            "Epoch: 25\n",
            "Train:  0.7877689710569598\n",
            "Test 0.6868761777877808\n",
            "0.6869772438913844\n",
            "Epoch: 26\n",
            "Train:  0.788806191396929\n",
            "Test 0.6868351697921753\n",
            "0.6869487602022081\n",
            "Epoch: 27\n",
            "Train:  0.7876645812082075\n",
            "Test 0.686805248260498\n",
            "0.6869200021877762\n",
            "Epoch: 28\n",
            "Train:  0.787852630086614\n",
            "Test 0.6868253350257874\n",
            "0.6869010394119042\n",
            "Epoch: 29\n",
            "Train:  0.7878355184291822\n",
            "Test 0.6867985129356384\n",
            "0.6868805087008617\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:31:42,271]\u001b[0m Trial 17 finished with value: 0.6861717205350004 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 6.036105733188118e-06, 'b1': 0.9385328748287135}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7875428744570702\n",
            "Test 0.6867378354072571\n",
            "0.6868519457547692\n",
            "Epoch: 0\n",
            "Train:  0.6935001552374654\n",
            "Test 0.6922928094863892\n",
            "0.692292809486389\n",
            "Epoch: 1\n",
            "Train:  0.7897564747214857\n",
            "Test 0.6922046542167664\n",
            "0.6922438343365988\n",
            "Epoch: 2\n",
            "Train:  0.7901997231789843\n",
            "Test 0.6921393871307373\n",
            "0.6922010281046883\n",
            "Epoch: 3\n",
            "Train:  0.7904406111704279\n",
            "Test 0.6920675039291382\n",
            "0.6921557963379029\n",
            "Epoch: 4\n",
            "Train:  0.7900807541959426\n",
            "Test 0.6920143961906433\n",
            "0.6921137329909076\n",
            "Epoch: 5\n",
            "Train:  0.7903552797045643\n",
            "Test 0.6919589042663574\n",
            "0.6920717657982439\n",
            "Epoch: 6\n",
            "Train:  0.78963512074354\n",
            "Test 0.6919039487838745\n",
            "0.6920292957888574\n",
            "Epoch: 7\n",
            "Train:  0.7890638506790092\n",
            "Test 0.6918521523475647\n",
            "0.6919867248887316\n",
            "Epoch: 8\n",
            "Train:  0.7907449360346902\n",
            "Test 0.6918059587478638\n",
            "0.6919449670134058\n",
            "Epoch: 9\n",
            "Train:  0.7893236522221457\n",
            "Test 0.6917601823806763\n",
            "0.6919035645289411\n",
            "Epoch: 10\n",
            "Train:  0.7892234789300289\n",
            "Test 0.6917248368263245\n",
            "0.6918644599288472\n",
            "Epoch: 11\n",
            "Train:  0.7906335252442511\n",
            "Test 0.6916911602020264\n",
            "0.6918272424154359\n",
            "Epoch: 12\n",
            "Train:  0.7895573056661166\n",
            "Test 0.6916648745536804\n",
            "0.6917928797347352\n",
            "Epoch: 13\n",
            "Train:  0.7892816959463094\n",
            "Test 0.6916424632072449\n",
            "0.6917614124849562\n",
            "Epoch: 14\n",
            "Train:  0.7897467869439276\n",
            "Test 0.6916187405586243\n",
            "0.691731837523232\n",
            "Epoch: 15\n",
            "Train:  0.7903484605016752\n",
            "Test 0.6915848255157471\n",
            "0.6917015835480387\n",
            "Epoch: 16\n",
            "Train:  0.7901938293314628\n",
            "Test 0.6915595531463623\n",
            "0.6916725230842335\n",
            "Epoch: 17\n",
            "Train:  0.790762992196493\n",
            "Test 0.6915262937545776\n",
            "0.6916427407066846\n",
            "Epoch: 18\n",
            "Train:  0.7899829874750716\n",
            "Test 0.6914984583854675\n",
            "0.691613462296076\n",
            "Epoch: 19\n",
            "Train:  0.7897496606429778\n",
            "Test 0.691464364528656\n",
            "0.6915832949366174\n",
            "Epoch: 20\n",
            "Train:  0.7892900449118463\n",
            "Test 0.6914369463920593\n",
            "0.6915537527491197\n",
            "Epoch: 21\n",
            "Train:  0.7894293558004215\n",
            "Test 0.6914119720458984\n",
            "0.6915251858217563\n",
            "Epoch: 22\n",
            "Train:  0.7900443373762105\n",
            "Test 0.6913849711418152\n",
            "0.6914969763665358\n",
            "Epoch: 23\n",
            "Train:  0.7893999788016756\n",
            "Test 0.6913517713546753\n",
            "0.6914677975711986\n",
            "Epoch: 24\n",
            "Train:  0.7889327236969547\n",
            "Test 0.6913213133811951\n",
            "0.6914383896331492\n",
            "Epoch: 25\n",
            "Train:  0.7894036203487966\n",
            "Test 0.6912912130355835\n",
            "0.6914088650811531\n",
            "Epoch: 26\n",
            "Train:  0.7890997345630939\n",
            "Test 0.6912494897842407\n",
            "0.6913769127658125\n",
            "Epoch: 27\n",
            "Train:  0.7896195989928094\n",
            "Test 0.6912121772766113\n",
            "0.691343901815508\n",
            "Epoch: 28\n",
            "Train:  0.7889543779834902\n",
            "Test 0.6911763548851013\n",
            "0.6913103404958\n",
            "Epoch: 29\n",
            "Train:  0.7885719715739807\n",
            "Test 0.691148579120636\n",
            "0.6912779481209492\n",
            "Epoch: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:32:55,591]\u001b[0m Trial 18 finished with value: 0.6905613861269163 and parameters: {'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 1.4311615466794701e-06, 'b1': 0.9646660218382047}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.7904403625030862\n",
            "Test 0.6911181807518005\n",
            "0.6912459629705608\n",
            "Epoch: 0\n",
            "Train:  0.6935211811130403\n",
            "Test 0.6897590160369873\n",
            "0.6897590160369872\n",
            "Epoch: 1\n",
            "Train:  0.7879167530871085\n",
            "Test 0.6816650032997131\n",
            "0.6852623422940572\n",
            "Epoch: 2\n",
            "Train:  0.7799315436393427\n",
            "Test 0.6670640110969543\n",
            "0.6778040098362281\n",
            "Epoch: 3\n",
            "Train:  0.7470941958923685\n",
            "Test 0.6163678765296936\n",
            "0.6569923115616568\n",
            "Epoch: 4\n",
            "Train:  0.6605068450599774\n",
            "Test 0.6511454582214355\n",
            "0.6552530048802487\n",
            "Epoch: 5\n",
            "Train:  0.5715967074238877\n",
            "Test 0.44535091519355774\n",
            "0.5983578682447288\n",
            "Epoch: 6\n",
            "Train:  0.41976819736925186\n",
            "Test 0.3894140422344208\n",
            "0.545479759995574\n",
            "Epoch: 7\n",
            "Train:  0.32175163850525385\n",
            "Test 0.3692602515220642\n",
            "0.5031308983007983\n",
            "Epoch: 8\n",
            "Train:  0.24299196895565922\n",
            "Test 0.35753270983695984\n",
            "0.469496994449314\n",
            "Epoch: 9\n",
            "Train:  0.22621386771288393\n",
            "Test 0.7635085582733154\n",
            "0.5353726520937476\n",
            "Epoch: 10\n",
            "Train:  0.3582560838077942\n",
            "Test 0.40734291076660156\n",
            "0.5073604762451258\n",
            "Epoch: 11\n",
            "Train:  0.23517954760816842\n",
            "Test 0.45363256335258484\n",
            "0.4958219738174722\n",
            "Epoch: 12\n",
            "Train:  0.2165747375914414\n",
            "Test 0.2899600565433502\n",
            "0.4522544398532906\n",
            "Epoch: 13\n",
            "Train:  0.12571014184083334\n",
            "Test 0.305094838142395\n",
            "0.4214685413772751\n",
            "Epoch: 14\n",
            "Train:  0.11544591490583857\n",
            "Test 0.33072131872177124\n",
            "0.4026572327644633\n",
            "Epoch: 15\n",
            "Train:  0.1134913351336216\n",
            "Test 0.3465411961078644\n",
            "0.391108970755131\n",
            "Epoch: 16\n",
            "Train:  0.09272733202617092\n",
            "Test 0.35725265741348267\n",
            "0.3841817202733593\n",
            "Epoch: 17\n",
            "Train:  0.08758885371266986\n",
            "Test 0.32370471954345703\n",
            "0.37186443158030585\n",
            "Epoch: 18\n",
            "Train:  0.07938108346066795\n",
            "Test 0.3366658687591553\n",
            "0.36472178259590105\n",
            "Epoch: 19\n",
            "Train:  0.07343241373157103\n",
            "Test 0.34066763520240784\n",
            "0.35985484110210375\n",
            "Epoch: 20\n",
            "Train:  0.07016554274975545\n",
            "Test 0.2970534861087799\n",
            "0.3471776435944411\n",
            "Epoch: 21\n",
            "Train:  0.05328811713540716\n",
            "Test 0.32689744234085083\n",
            "0.34309145257573975\n",
            "Epoch: 22\n",
            "Train:  0.054415288461346974\n",
            "Test 0.4133061170578003\n",
            "0.3572177725471007\n",
            "Epoch: 23\n",
            "Train:  0.06182039341134993\n",
            "Test 0.3936387896537781\n",
            "0.3645365378604479\n",
            "Epoch: 24\n",
            "Train:  0.06213130935504021\n",
            "Test 0.6428859233856201\n",
            "0.4204175273750129\n",
            "Epoch: 25\n",
            "Train:  0.09630588909144391\n",
            "Test 0.6389035582542419\n",
            "0.46424720060995617\n",
            "Epoch: 26\n",
            "Train:  0.1351812223132649\n",
            "Test 0.6347214579582214\n",
            "0.4984246881739405\n",
            "Epoch: 27\n",
            "Train:  0.4929139280009054\n",
            "Test 0.4311419725418091\n",
            "0.48494206586315003\n",
            "Epoch: 28\n",
            "Train:  0.4090863497447374\n",
            "Test 0.4652155935764313\n",
            "0.48099065689659215\n",
            "Epoch: 29\n",
            "Train:  0.28582995658007143\n",
            "Test 0.3081856071949005\n",
            "0.4463868094680087\n",
            "Epoch: 30\n",
            "Train:  0.16849937765307016\n",
            "Test 0.32265013456344604\n",
            "0.4216149416174481\n",
            "Epoch: 31\n",
            "Train:  0.12000802233478063\n",
            "Test 0.30099350214004517\n",
            "0.3974715253368803\n",
            "Epoch: 32\n",
            "Train:  0.07793990947828708\n",
            "Test 0.32548972964286804\n",
            "0.3830660356342438\n",
            "Epoch: 33\n",
            "Train:  0.06639125914738526\n",
            "Test 0.3955288529396057\n",
            "0.3855598636163315\n",
            "Epoch: 34\n",
            "Train:  0.06562309344172713\n",
            "Test 0.39641281962394714\n",
            "0.3877313356715679\n",
            "Epoch: 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-16 06:33:39,852]\u001b[0m Trial 19 finished with value: 0.3955437770270236 and parameters: {'layer_size1': 256, 'layer_size2': 256, 'layer_size3': 256, 'layer_size4': 512, 'layer_size5': 512, 'layer_size6': 512, 'learning_rate': 0.0003105756012985766, 'b1': 0.9041500573520839}. Best is trial 2 with value: 0.2790117843189192.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:  0.06016777981848591\n",
            "Test 0.4274226725101471\n",
            "0.395672179990597\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(),pruner=optuna.pruners.HyperbandPruner())\n",
        "study.optimize(objective, n_trials=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "C57DthBtjHEL"
      },
      "outputs": [],
      "source": [
        "best_trial = study.best_trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "B41PbOc5zuNx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_size1': 512, 'layer_size2': 512, 'layer_size3': 256, 'layer_size4': 64, 'layer_size5': 512, 'layer_size6': 64, 'learning_rate': 1.433675124157315e-05, 'b1': 0.9286759768614723}\n"
          ]
        }
      ],
      "source": [
        "print(best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kmV1JcJfzxaO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number</th>\n",
              "      <th>value</th>\n",
              "      <th>datetime_start</th>\n",
              "      <th>datetime_complete</th>\n",
              "      <th>duration</th>\n",
              "      <th>params_b1</th>\n",
              "      <th>params_layer_size1</th>\n",
              "      <th>params_layer_size2</th>\n",
              "      <th>params_layer_size3</th>\n",
              "      <th>params_layer_size4</th>\n",
              "      <th>params_layer_size5</th>\n",
              "      <th>params_layer_size6</th>\n",
              "      <th>params_learning_rate</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.387419</td>\n",
              "      <td>2022-10-16 05:56:23.942941</td>\n",
              "      <td>2022-10-16 06:08:00.122371</td>\n",
              "      <td>0 days 00:11:36.179430</td>\n",
              "      <td>0.998112</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.695399</td>\n",
              "      <td>2022-10-16 06:08:00.124011</td>\n",
              "      <td>2022-10-16 06:08:53.052468</td>\n",
              "      <td>0 days 00:00:52.928457</td>\n",
              "      <td>0.925050</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.279012</td>\n",
              "      <td>2022-10-16 06:08:53.055273</td>\n",
              "      <td>2022-10-16 06:13:57.272932</td>\n",
              "      <td>0 days 00:05:04.217659</td>\n",
              "      <td>0.928676</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.689532</td>\n",
              "      <td>2022-10-16 06:13:57.274491</td>\n",
              "      <td>2022-10-16 06:14:58.211351</td>\n",
              "      <td>0 days 00:01:00.936860</td>\n",
              "      <td>0.981314</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.688254</td>\n",
              "      <td>2022-10-16 06:14:58.214101</td>\n",
              "      <td>2022-10-16 06:16:00.574613</td>\n",
              "      <td>0 days 00:01:02.360512</td>\n",
              "      <td>0.940679</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>128</td>\n",
              "      <td>512</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.357938</td>\n",
              "      <td>2022-10-16 06:16:00.576132</td>\n",
              "      <td>2022-10-16 06:17:57.652447</td>\n",
              "      <td>0 days 00:01:57.076315</td>\n",
              "      <td>0.945765</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.681249</td>\n",
              "      <td>2022-10-16 06:17:57.654074</td>\n",
              "      <td>2022-10-16 06:19:17.928335</td>\n",
              "      <td>0 days 00:01:20.274261</td>\n",
              "      <td>0.970140</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.376275</td>\n",
              "      <td>2022-10-16 06:19:17.933509</td>\n",
              "      <td>2022-10-16 06:20:18.940857</td>\n",
              "      <td>0 days 00:01:01.007348</td>\n",
              "      <td>0.914047</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>128</td>\n",
              "      <td>64</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.689516</td>\n",
              "      <td>2022-10-16 06:20:18.942500</td>\n",
              "      <td>2022-10-16 06:23:35.016432</td>\n",
              "      <td>0 days 00:03:16.073932</td>\n",
              "      <td>0.917469</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.433485</td>\n",
              "      <td>2022-10-16 06:23:35.017992</td>\n",
              "      <td>2022-10-16 06:24:43.046858</td>\n",
              "      <td>0 days 00:01:08.028866</td>\n",
              "      <td>0.925278</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>128</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.348721</td>\n",
              "      <td>2022-10-16 06:24:43.048266</td>\n",
              "      <td>2022-10-16 06:25:30.587044</td>\n",
              "      <td>0 days 00:00:47.538778</td>\n",
              "      <td>0.961867</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.645923</td>\n",
              "      <td>2022-10-16 06:25:30.588464</td>\n",
              "      <td>2022-10-16 06:26:01.755694</td>\n",
              "      <td>0 days 00:00:31.167230</td>\n",
              "      <td>0.900420</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000994</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.491542</td>\n",
              "      <td>2022-10-16 06:26:01.756661</td>\n",
              "      <td>2022-10-16 06:26:43.960269</td>\n",
              "      <td>0 days 00:00:42.203608</td>\n",
              "      <td>0.965654</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.370196</td>\n",
              "      <td>2022-10-16 06:26:43.961143</td>\n",
              "      <td>2022-10-16 06:27:32.754119</td>\n",
              "      <td>0 days 00:00:48.792976</td>\n",
              "      <td>0.955962</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.696730</td>\n",
              "      <td>2022-10-16 06:27:32.755507</td>\n",
              "      <td>2022-10-16 06:28:38.783934</td>\n",
              "      <td>0 days 00:01:06.028427</td>\n",
              "      <td>0.933474</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.378246</td>\n",
              "      <td>2022-10-16 06:28:38.785494</td>\n",
              "      <td>2022-10-16 06:29:33.790299</td>\n",
              "      <td>0 days 00:00:55.004805</td>\n",
              "      <td>0.954404</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>0.475004</td>\n",
              "      <td>2022-10-16 06:29:33.791763</td>\n",
              "      <td>2022-10-16 06:30:55.111695</td>\n",
              "      <td>0 days 00:01:21.319932</td>\n",
              "      <td>0.983973</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.686172</td>\n",
              "      <td>2022-10-16 06:30:55.113557</td>\n",
              "      <td>2022-10-16 06:31:42.271462</td>\n",
              "      <td>0 days 00:00:47.157905</td>\n",
              "      <td>0.938533</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>0.690561</td>\n",
              "      <td>2022-10-16 06:31:42.272766</td>\n",
              "      <td>2022-10-16 06:32:55.591073</td>\n",
              "      <td>0 days 00:01:13.318307</td>\n",
              "      <td>0.964666</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>256</td>\n",
              "      <td>64</td>\n",
              "      <td>512</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>0.395544</td>\n",
              "      <td>2022-10-16 06:32:55.592480</td>\n",
              "      <td>2022-10-16 06:33:39.852111</td>\n",
              "      <td>0 days 00:00:44.259631</td>\n",
              "      <td>0.904150</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>COMPLETE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    number     value             datetime_start          datetime_complete  \\\n",
              "0        0  0.387419 2022-10-16 05:56:23.942941 2022-10-16 06:08:00.122371   \n",
              "1        1  0.695399 2022-10-16 06:08:00.124011 2022-10-16 06:08:53.052468   \n",
              "2        2  0.279012 2022-10-16 06:08:53.055273 2022-10-16 06:13:57.272932   \n",
              "3        3  0.689532 2022-10-16 06:13:57.274491 2022-10-16 06:14:58.211351   \n",
              "4        4  0.688254 2022-10-16 06:14:58.214101 2022-10-16 06:16:00.574613   \n",
              "5        5  0.357938 2022-10-16 06:16:00.576132 2022-10-16 06:17:57.652447   \n",
              "6        6  0.681249 2022-10-16 06:17:57.654074 2022-10-16 06:19:17.928335   \n",
              "7        7  0.376275 2022-10-16 06:19:17.933509 2022-10-16 06:20:18.940857   \n",
              "8        8  0.689516 2022-10-16 06:20:18.942500 2022-10-16 06:23:35.016432   \n",
              "9        9  0.433485 2022-10-16 06:23:35.017992 2022-10-16 06:24:43.046858   \n",
              "10      10  0.348721 2022-10-16 06:24:43.048266 2022-10-16 06:25:30.587044   \n",
              "11      11  0.645923 2022-10-16 06:25:30.588464 2022-10-16 06:26:01.755694   \n",
              "12      12  0.491542 2022-10-16 06:26:01.756661 2022-10-16 06:26:43.960269   \n",
              "13      13  0.370196 2022-10-16 06:26:43.961143 2022-10-16 06:27:32.754119   \n",
              "14      14  0.696730 2022-10-16 06:27:32.755507 2022-10-16 06:28:38.783934   \n",
              "15      15  0.378246 2022-10-16 06:28:38.785494 2022-10-16 06:29:33.790299   \n",
              "16      16  0.475004 2022-10-16 06:29:33.791763 2022-10-16 06:30:55.111695   \n",
              "17      17  0.686172 2022-10-16 06:30:55.113557 2022-10-16 06:31:42.271462   \n",
              "18      18  0.690561 2022-10-16 06:31:42.272766 2022-10-16 06:32:55.591073   \n",
              "19      19  0.395544 2022-10-16 06:32:55.592480 2022-10-16 06:33:39.852111   \n",
              "\n",
              "                 duration  params_b1  params_layer_size1  params_layer_size2  \\\n",
              "0  0 days 00:11:36.179430   0.998112                 256                 512   \n",
              "1  0 days 00:00:52.928457   0.925050                 256                 512   \n",
              "2  0 days 00:05:04.217659   0.928676                 512                 512   \n",
              "3  0 days 00:01:00.936860   0.981314                 512                 256   \n",
              "4  0 days 00:01:02.360512   0.940679                 512                 512   \n",
              "5  0 days 00:01:57.076315   0.945765                 512                 256   \n",
              "6  0 days 00:01:20.274261   0.970140                 512                 512   \n",
              "7  0 days 00:01:01.007348   0.914047                 256                 512   \n",
              "8  0 days 00:03:16.073932   0.917469                 512                 512   \n",
              "9  0 days 00:01:08.028866   0.925278                 512                 256   \n",
              "10 0 days 00:00:47.538778   0.961867                 256                 256   \n",
              "11 0 days 00:00:31.167230   0.900420                 256                 256   \n",
              "12 0 days 00:00:42.203608   0.965654                 256                 256   \n",
              "13 0 days 00:00:48.792976   0.955962                 256                 256   \n",
              "14 0 days 00:01:06.028427   0.933474                 512                 512   \n",
              "15 0 days 00:00:55.004805   0.954404                 256                 256   \n",
              "16 0 days 00:01:21.319932   0.983973                 512                 512   \n",
              "17 0 days 00:00:47.157905   0.938533                 256                 256   \n",
              "18 0 days 00:01:13.318307   0.964666                 512                 512   \n",
              "19 0 days 00:00:44.259631   0.904150                 256                 256   \n",
              "\n",
              "    params_layer_size3  params_layer_size4  params_layer_size5  \\\n",
              "0                  256                 512                 128   \n",
              "1                  512                 128                 128   \n",
              "2                  256                  64                 512   \n",
              "3                  512                 128                  64   \n",
              "4                  256                  64                 128   \n",
              "5                  512                 512                  64   \n",
              "6                  512                 128                  64   \n",
              "7                  256                 128                  64   \n",
              "8                  512                  64                  64   \n",
              "9                  256                 512                 512   \n",
              "10                 256                  64                 512   \n",
              "11                 256                  64                 512   \n",
              "12                 256                  64                 512   \n",
              "13                 256                  64                 512   \n",
              "14                 256                  64                 512   \n",
              "15                 256                  64                 512   \n",
              "16                 256                  64                 512   \n",
              "17                 256                  64                 512   \n",
              "18                 256                  64                 512   \n",
              "19                 256                 512                 512   \n",
              "\n",
              "    params_layer_size6  params_learning_rate     state  \n",
              "0                   64              0.000033  COMPLETE  \n",
              "1                  128              0.000003  COMPLETE  \n",
              "2                   64              0.000014  COMPLETE  \n",
              "3                  128              0.000001  COMPLETE  \n",
              "4                  512              0.000011  COMPLETE  \n",
              "5                   64              0.000047  COMPLETE  \n",
              "6                  512              0.000018  COMPLETE  \n",
              "7                  128              0.000203  COMPLETE  \n",
              "8                  512              0.000011  COMPLETE  \n",
              "9                  128              0.000119  COMPLETE  \n",
              "10                  64              0.000410  COMPLETE  \n",
              "11                  64              0.000994  COMPLETE  \n",
              "12                  64              0.000811  COMPLETE  \n",
              "13                  64              0.000198  COMPLETE  \n",
              "14                  64              0.000004  COMPLETE  \n",
              "15                  64              0.000076  COMPLETE  \n",
              "16                  64              0.000461  COMPLETE  \n",
              "17                  64              0.000006  COMPLETE  \n",
              "18                  64              0.000001  COMPLETE  \n",
              "19                 512              0.000311  COMPLETE  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.trials_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open('best_model_pol.json', 'w') as f:\n",
        "    json.dump(best_trial.params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# with open('best_model_pol.json') as f:\n",
        "#     best_param = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# best_trial={}\n",
        "# best_trial['params'] = best_param\n",
        "# best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(test_data_pol.num_features,[best_trial.params['layer_size1'],best_trial.params['layer_size2'],best_trial.params['layer_size3'],\n",
        "                                        best_trial.params['layer_size4'],best_trial.params['layer_size5'],best_trial.params['layer_size6']],1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_trial.params['learning_rate'],betas=(best_trial.params['b1'],0.99))\n",
        "lossff = torch.nn.BCELoss()\n",
        "print(device)\n",
        "\n",
        "print(\"Number of Parameters of the model :\",sum([param.nelement() for param in model.parameters()]))\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in test_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(test_loader.dataset), accuracy, f1\n",
        "\n",
        "@torch.no_grad()\n",
        "def val(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in val_loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        # print(out)\n",
        "        loss = lossff(torch.reshape(out,(-1,)), data.y.float())\n",
        "        # print(loss)\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "        all_preds.append(torch.reshape(out, (-1,)))\n",
        "        all_labels.append(data.y.float())\n",
        "    # print(all_preds)\n",
        "    accuracy, f1 = metrics(all_preds, all_labels)\n",
        "    return total_loss / len(val_loader.dataset), accuracy, f1\n",
        "\n",
        "\n",
        "def metrics(preds, gts):\n",
        "    preds = torch.round(torch.cat(preds))\n",
        "    gts = torch.cat(gts)\n",
        "    # print(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    f1 = f1_score(preds.cpu().numpy(), gts.cpu().numpy())\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 |  TrainLoss: 0.69363 | ValLoss: 0.68620 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70272 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69328 | ValLoss: 0.68613 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70254 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69196 | ValLoss: 0.68601 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70245 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69300 | ValLoss: 0.68593 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70227 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69297 | ValLoss: 0.68583 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70209 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69256 | ValLoss: 0.68570 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70197 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69292 | ValLoss: 0.68572 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70161 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69292 | ValLoss: 0.68568 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70131 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69288 | ValLoss: 0.68551 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70116 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69193 | ValLoss: 0.68527 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70104 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69231 | ValLoss: 0.68505 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70087 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69169 | ValLoss: 0.68483 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70072 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69152 | ValLoss: 0.68453 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70067 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69161 | ValLoss: 0.68422 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70049 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69025 | ValLoss: 0.68401 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70009 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.68961 | ValLoss: 0.68379 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69961 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.69031 | ValLoss: 0.68331 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69934 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.69007 | ValLoss: 0.68298 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69880 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.68881 | ValLoss: 0.68276 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69813 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.68890 | ValLoss: 0.68231 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69768 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.68710 | ValLoss: 0.68199 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69687 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.68666 | ValLoss: 0.68163 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69593 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.68619 | ValLoss: 0.68049 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69576 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.68533 | ValLoss: 0.67987 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69465 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.68364 | ValLoss: 0.67919 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69329 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.68331 | ValLoss: 0.67833 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69200 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.68267 | ValLoss: 0.67730 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69065 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.68121 | ValLoss: 0.67580 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68950 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 28 |  TrainLoss: 0.67933 | ValLoss: 0.67461 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68774 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 29 |  TrainLoss: 0.67871 | ValLoss: 0.67377 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68533 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 30 |  TrainLoss: 0.67693 | ValLoss: 0.67223 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68352 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 31 |  TrainLoss: 0.67636 | ValLoss: 0.67019 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.68228 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 32 |  TrainLoss: 0.67254 | ValLoss: 0.66920 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67914 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 33 |  TrainLoss: 0.67141 | ValLoss: 0.66746 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67680 | TestAcc: 0.54839 | TestF1: 0.65\n",
            "Epoch: 34 |  TrainLoss: 0.66690 | ValLoss: 0.66523 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67470 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 35 |  TrainLoss: 0.66742 | ValLoss: 0.66337 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67161 | TestAcc: 0.64516 | TestF1: 0.70\n",
            "Epoch: 36 |  TrainLoss: 0.66668 | ValLoss: 0.66199 | ValAcc: 0.83871 | ValF1: 0.88 | TestLoss: 0.66797 | TestAcc: 0.72581 | TestF1: 0.75\n",
            "Epoch: 37 |  TrainLoss: 0.66319 | ValLoss: 0.65987 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.66482 | TestAcc: 0.77419 | TestF1: 0.79\n",
            "Epoch: 38 |  TrainLoss: 0.66038 | ValLoss: 0.65684 | ValAcc: 0.83871 | ValF1: 0.88 | TestLoss: 0.66250 | TestAcc: 0.75806 | TestF1: 0.78\n",
            "Epoch: 39 |  TrainLoss: 0.65567 | ValLoss: 0.65395 | ValAcc: 0.83871 | ValF1: 0.88 | TestLoss: 0.65966 | TestAcc: 0.77419 | TestF1: 0.79\n",
            "Epoch: 40 |  TrainLoss: 0.65468 | ValLoss: 0.65270 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.65404 | TestAcc: 0.80645 | TestF1: 0.80\n",
            "Epoch: 41 |  TrainLoss: 0.65118 | ValLoss: 0.65012 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.64971 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 42 |  TrainLoss: 0.64703 | ValLoss: 0.64631 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.64610 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 43 |  TrainLoss: 0.64467 | ValLoss: 0.64328 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.64138 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 44 |  TrainLoss: 0.63817 | ValLoss: 0.64059 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.63596 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 45 |  TrainLoss: 0.63322 | ValLoss: 0.63657 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.63086 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 46 |  TrainLoss: 0.62694 | ValLoss: 0.63145 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.62623 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 47 |  TrainLoss: 0.62890 | ValLoss: 0.62734 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.62003 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 48 |  TrainLoss: 0.61565 | ValLoss: 0.62409 | ValAcc: 0.74194 | ValF1: 0.76 | TestLoss: 0.61288 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 49 |  TrainLoss: 0.61355 | ValLoss: 0.62195 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.60479 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 50 |  TrainLoss: 0.60551 | ValLoss: 0.61278 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.60021 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 51 |  TrainLoss: 0.59816 | ValLoss: 0.60598 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.59479 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 52 |  TrainLoss: 0.59151 | ValLoss: 0.60327 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.58331 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 53 |  TrainLoss: 0.58347 | ValLoss: 0.59701 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.57497 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 54 |  TrainLoss: 0.58575 | ValLoss: 0.58828 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.56862 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 55 |  TrainLoss: 0.57326 | ValLoss: 0.58198 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.55928 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 56 |  TrainLoss: 0.56135 | ValLoss: 0.57560 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.54950 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 57 |  TrainLoss: 0.55287 | ValLoss: 0.57158 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.53756 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 58 |  TrainLoss: 0.54049 | ValLoss: 0.56203 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.52854 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 59 |  TrainLoss: 0.53649 | ValLoss: 0.55088 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.52343 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 60 |  TrainLoss: 0.52655 | ValLoss: 0.54411 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.51127 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 61 |  TrainLoss: 0.51252 | ValLoss: 0.54169 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.49612 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 62 |  TrainLoss: 0.51075 | ValLoss: 0.53094 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.48685 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 63 |  TrainLoss: 0.49532 | ValLoss: 0.51905 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.48110 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 64 |  TrainLoss: 0.49051 | ValLoss: 0.51086 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.47020 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 65 |  TrainLoss: 0.47661 | ValLoss: 0.50479 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.45630 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 66 |  TrainLoss: 0.47113 | ValLoss: 0.49656 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.44615 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 67 |  TrainLoss: 0.45797 | ValLoss: 0.48939 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.43484 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 68 |  TrainLoss: 0.45023 | ValLoss: 0.47903 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.42913 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 69 |  TrainLoss: 0.43843 | ValLoss: 0.47194 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41742 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 70 |  TrainLoss: 0.41851 | ValLoss: 0.46970 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.40131 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 71 |  TrainLoss: 0.42235 | ValLoss: 0.45772 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.39524 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 72 |  TrainLoss: 0.40452 | ValLoss: 0.45067 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.38493 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 73 |  TrainLoss: 0.40177 | ValLoss: 0.44260 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37677 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 74 |  TrainLoss: 0.38684 | ValLoss: 0.43616 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36624 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 75 |  TrainLoss: 0.38136 | ValLoss: 0.43151 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.35452 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 76 |  TrainLoss: 0.36846 | ValLoss: 0.42076 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35124 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 77 |  TrainLoss: 0.35652 | ValLoss: 0.41456 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34187 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 78 |  TrainLoss: 0.34440 | ValLoss: 0.41143 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32934 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 79 |  TrainLoss: 0.33079 | ValLoss: 0.40419 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32230 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 80 |  TrainLoss: 0.33146 | ValLoss: 0.39693 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31706 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 81 |  TrainLoss: 0.33947 | ValLoss: 0.39100 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31578 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 82 |  TrainLoss: 0.33270 | ValLoss: 0.39315 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.29759 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 83 |  TrainLoss: 0.32607 | ValLoss: 0.38333 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29619 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 84 |  TrainLoss: 0.30547 | ValLoss: 0.37850 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30265 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 85 |  TrainLoss: 0.30308 | ValLoss: 0.37419 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29588 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 86 |  TrainLoss: 0.29971 | ValLoss: 0.37292 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.27703 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 87 |  TrainLoss: 0.27966 | ValLoss: 0.36703 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27412 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 88 |  TrainLoss: 0.28403 | ValLoss: 0.36285 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27428 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 89 |  TrainLoss: 0.27958 | ValLoss: 0.36248 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28211 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 90 |  TrainLoss: 0.26322 | ValLoss: 0.36121 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.25645 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 91 |  TrainLoss: 0.26665 | ValLoss: 0.35335 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25679 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 92 |  TrainLoss: 0.27170 | ValLoss: 0.35190 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26232 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 93 |  TrainLoss: 0.24135 | ValLoss: 0.34802 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.24978 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 94 |  TrainLoss: 0.25006 | ValLoss: 0.34608 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.24650 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 95 |  TrainLoss: 0.22612 | ValLoss: 0.34408 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24941 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 96 |  TrainLoss: 0.21834 | ValLoss: 0.34070 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24262 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 97 |  TrainLoss: 0.24509 | ValLoss: 0.33884 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.23737 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 98 |  TrainLoss: 0.20681 | ValLoss: 0.33778 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24580 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 99 |  TrainLoss: 0.21916 | ValLoss: 0.33538 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24139 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 100 |  TrainLoss: 0.23144 | ValLoss: 0.33222 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23319 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 101 |  TrainLoss: 0.21715 | ValLoss: 0.33105 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23197 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 102 |  TrainLoss: 0.19343 | ValLoss: 0.32982 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22983 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 103 |  TrainLoss: 0.21305 | ValLoss: 0.33232 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.23986 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 104 |  TrainLoss: 0.18155 | ValLoss: 0.32724 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22444 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 105 |  TrainLoss: 0.19768 | ValLoss: 0.32552 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22318 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 106 |  TrainLoss: 0.20627 | ValLoss: 0.32445 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22671 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 107 |  TrainLoss: 0.17534 | ValLoss: 0.32396 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22826 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 108 |  TrainLoss: 0.19115 | ValLoss: 0.32376 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22936 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 109 |  TrainLoss: 0.19329 | ValLoss: 0.32130 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22255 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 110 |  TrainLoss: 0.20027 | ValLoss: 0.32237 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21693 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 111 |  TrainLoss: 0.17893 | ValLoss: 0.32563 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23073 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 112 |  TrainLoss: 0.17577 | ValLoss: 0.32293 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22448 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 113 |  TrainLoss: 0.17238 | ValLoss: 0.32067 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21652 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 114 |  TrainLoss: 0.15898 | ValLoss: 0.31973 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21842 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 115 |  TrainLoss: 0.16760 | ValLoss: 0.32269 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22850 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 116 |  TrainLoss: 0.15131 | ValLoss: 0.32160 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22620 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 117 |  TrainLoss: 0.16266 | ValLoss: 0.31809 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21544 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 118 |  TrainLoss: 0.16246 | ValLoss: 0.31817 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21475 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 119 |  TrainLoss: 0.15629 | ValLoss: 0.32649 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23060 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 120 |  TrainLoss: 0.14030 | ValLoss: 0.31951 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21910 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 121 |  TrainLoss: 0.14107 | ValLoss: 0.31868 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21546 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 122 |  TrainLoss: 0.12828 | ValLoss: 0.32105 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21963 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 123 |  TrainLoss: 0.13687 | ValLoss: 0.32331 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22295 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 124 |  TrainLoss: 0.14508 | ValLoss: 0.32140 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21455 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 125 |  TrainLoss: 0.12729 | ValLoss: 0.32899 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22534 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 126 |  TrainLoss: 0.13870 | ValLoss: 0.32902 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22454 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 127 |  TrainLoss: 0.12098 | ValLoss: 0.32584 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21537 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 128 |  TrainLoss: 0.14484 | ValLoss: 0.32908 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22040 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 129 |  TrainLoss: 0.12817 | ValLoss: 0.32929 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22052 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 130 |  TrainLoss: 0.12149 | ValLoss: 0.33113 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21863 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 131 |  TrainLoss: 0.11351 | ValLoss: 0.33820 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22805 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 132 |  TrainLoss: 0.09988 | ValLoss: 0.33791 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22708 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 133 |  TrainLoss: 0.10644 | ValLoss: 0.33623 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22181 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 134 |  TrainLoss: 0.10594 | ValLoss: 0.33775 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22330 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 135 |  TrainLoss: 0.11051 | ValLoss: 0.35276 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23728 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 136 |  TrainLoss: 0.10662 | ValLoss: 0.34220 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22813 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 137 |  TrainLoss: 0.09961 | ValLoss: 0.34491 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22805 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 138 |  TrainLoss: 0.09396 | ValLoss: 0.34703 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23282 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 139 |  TrainLoss: 0.10647 | ValLoss: 0.34981 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23597 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 140 |  TrainLoss: 0.09417 | ValLoss: 0.34807 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23313 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 141 |  TrainLoss: 0.09613 | ValLoss: 0.35143 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23357 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 142 |  TrainLoss: 0.09621 | ValLoss: 0.35407 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23719 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 143 |  TrainLoss: 0.08920 | ValLoss: 0.35583 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23708 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 144 |  TrainLoss: 0.08630 | ValLoss: 0.36444 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24079 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 145 |  TrainLoss: 0.08260 | ValLoss: 0.36009 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24019 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 146 |  TrainLoss: 0.07904 | ValLoss: 0.36112 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24153 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 147 |  TrainLoss: 0.08594 | ValLoss: 0.36148 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24302 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 148 |  TrainLoss: 0.07378 | ValLoss: 0.36216 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24444 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 149 |  TrainLoss: 0.08933 | ValLoss: 0.36522 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24696 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 150 |  TrainLoss: 0.08100 | ValLoss: 0.37362 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25292 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 151 |  TrainLoss: 0.07914 | ValLoss: 0.36274 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24918 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 152 |  TrainLoss: 0.07852 | ValLoss: 0.36143 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24977 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 153 |  TrainLoss: 0.08367 | ValLoss: 0.37749 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25912 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 154 |  TrainLoss: 0.06899 | ValLoss: 0.36703 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25462 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 155 |  TrainLoss: 0.06961 | ValLoss: 0.36431 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25500 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.06988 | ValLoss: 0.36249 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25563 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 157 |  TrainLoss: 0.07986 | ValLoss: 0.36384 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25777 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 158 |  TrainLoss: 0.07536 | ValLoss: 0.36247 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25824 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 159 |  TrainLoss: 0.06010 | ValLoss: 0.37635 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26773 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 160 |  TrainLoss: 0.06815 | ValLoss: 0.37025 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26677 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 161 |  TrainLoss: 0.05763 | ValLoss: 0.35473 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25998 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 162 |  TrainLoss: 0.06685 | ValLoss: 0.36148 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26533 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 163 |  TrainLoss: 0.05397 | ValLoss: 0.38439 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28153 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 164 |  TrainLoss: 0.06955 | ValLoss: 0.36330 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27117 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 165 |  TrainLoss: 0.06736 | ValLoss: 0.35156 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.26567 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 166 |  TrainLoss: 0.05200 | ValLoss: 0.36613 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27618 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 167 |  TrainLoss: 0.05278 | ValLoss: 0.38171 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28784 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 168 |  TrainLoss: 0.05117 | ValLoss: 0.35475 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26828 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 169 |  TrainLoss: 0.05527 | ValLoss: 0.35757 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26925 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 170 |  TrainLoss: 0.06103 | ValLoss: 0.37913 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28603 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 171 |  TrainLoss: 0.04506 | ValLoss: 0.37060 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27915 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 172 |  TrainLoss: 0.04559 | ValLoss: 0.36795 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27669 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 173 |  TrainLoss: 0.05588 | ValLoss: 0.36829 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27764 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 174 |  TrainLoss: 0.05968 | ValLoss: 0.40204 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30464 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 175 |  TrainLoss: 0.04933 | ValLoss: 0.37370 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28423 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 176 |  TrainLoss: 0.05258 | ValLoss: 0.36182 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27412 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 177 |  TrainLoss: 0.04147 | ValLoss: 0.37425 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28721 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 178 |  TrainLoss: 0.03912 | ValLoss: 0.38052 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29450 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 179 |  TrainLoss: 0.04097 | ValLoss: 0.37465 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29193 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 180 |  TrainLoss: 0.04216 | ValLoss: 0.36577 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28491 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 181 |  TrainLoss: 0.03901 | ValLoss: 0.36255 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.28043 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 182 |  TrainLoss: 0.04646 | ValLoss: 0.38202 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29961 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 183 |  TrainLoss: 0.04197 | ValLoss: 0.38041 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29838 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 184 |  TrainLoss: 0.03724 | ValLoss: 0.36688 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28658 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 185 |  TrainLoss: 0.04024 | ValLoss: 0.37425 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29498 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 186 |  TrainLoss: 0.04380 | ValLoss: 0.37185 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29437 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 187 |  TrainLoss: 0.03813 | ValLoss: 0.39184 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31454 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 188 |  TrainLoss: 0.03865 | ValLoss: 0.36903 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29426 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 189 |  TrainLoss: 0.04039 | ValLoss: 0.36453 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.28860 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 190 |  TrainLoss: 0.03487 | ValLoss: 0.38142 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30724 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 191 |  TrainLoss: 0.04527 | ValLoss: 0.39382 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32042 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 192 |  TrainLoss: 0.03922 | ValLoss: 0.36914 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29145 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 193 |  TrainLoss: 0.03726 | ValLoss: 0.37135 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29353 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 194 |  TrainLoss: 0.04198 | ValLoss: 0.39905 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32514 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 195 |  TrainLoss: 0.04024 | ValLoss: 0.40240 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32971 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 196 |  TrainLoss: 0.04064 | ValLoss: 0.37230 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30056 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 197 |  TrainLoss: 0.04037 | ValLoss: 0.37598 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30578 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 198 |  TrainLoss: 0.03529 | ValLoss: 0.41317 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33952 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 199 |  TrainLoss: 0.03669 | ValLoss: 0.39052 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32184 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 200 |  TrainLoss: 0.03021 | ValLoss: 0.38644 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31957 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 201 |  TrainLoss: 0.03056 | ValLoss: 0.38153 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31539 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 202 |  TrainLoss: 0.03424 | ValLoss: 0.38090 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31491 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 203 |  TrainLoss: 0.03167 | ValLoss: 0.39264 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33211 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 204 |  TrainLoss: 0.02948 | ValLoss: 0.39158 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33558 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 205 |  TrainLoss: 0.03505 | ValLoss: 0.38482 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32999 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 206 |  TrainLoss: 0.03361 | ValLoss: 0.38929 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33842 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 207 |  TrainLoss: 0.03003 | ValLoss: 0.39036 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34061 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 208 |  TrainLoss: 0.03473 | ValLoss: 0.38259 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33100 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 209 |  TrainLoss: 0.03215 | ValLoss: 0.39432 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34800 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 210 |  TrainLoss: 0.02670 | ValLoss: 0.39019 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34520 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 211 |  TrainLoss: 0.02680 | ValLoss: 0.38553 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34193 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 212 |  TrainLoss: 0.03378 | ValLoss: 0.38529 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34492 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 213 |  TrainLoss: 0.02677 | ValLoss: 0.38634 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34902 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 214 |  TrainLoss: 0.03448 | ValLoss: 0.38007 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34104 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 215 |  TrainLoss: 0.03469 | ValLoss: 0.39823 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.36454 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 216 |  TrainLoss: 0.03280 | ValLoss: 0.39571 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.36630 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 217 |  TrainLoss: 0.03158 | ValLoss: 0.37691 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.33053 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 218 |  TrainLoss: 0.02795 | ValLoss: 0.38474 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.36007 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 219 |  TrainLoss: 0.03653 | ValLoss: 0.40148 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.38608 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 220 |  TrainLoss: 0.02705 | ValLoss: 0.39875 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.38562 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 221 |  TrainLoss: 0.03055 | ValLoss: 0.39191 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.38034 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 222 |  TrainLoss: 0.02743 | ValLoss: 0.37781 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.35336 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 223 |  TrainLoss: 0.02115 | ValLoss: 0.37768 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.35748 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 224 |  TrainLoss: 0.02246 | ValLoss: 0.38751 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.38067 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 225 |  TrainLoss: 0.02972 | ValLoss: 0.42695 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.42992 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 226 |  TrainLoss: 0.02882 | ValLoss: 0.39746 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.40041 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 227 |  TrainLoss: 0.02135 | ValLoss: 0.38504 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.36692 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 228 |  TrainLoss: 0.02158 | ValLoss: 0.39785 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.40516 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 229 |  TrainLoss: 0.01808 | ValLoss: 0.41725 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.43519 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 230 |  TrainLoss: 0.02497 | ValLoss: 0.43894 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.46972 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 231 |  TrainLoss: 0.01736 | ValLoss: 0.40035 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.43478 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 232 |  TrainLoss: 0.01573 | ValLoss: 0.39083 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.39831 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 233 |  TrainLoss: 0.01505 | ValLoss: 0.43182 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.48291 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 234 |  TrainLoss: 0.01508 | ValLoss: 0.39332 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.43433 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 235 |  TrainLoss: 0.01009 | ValLoss: 0.39227 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.42445 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 236 |  TrainLoss: 0.00662 | ValLoss: 0.39383 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.42224 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 237 |  TrainLoss: 0.00753 | ValLoss: 0.39825 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.46033 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 238 |  TrainLoss: 0.00787 | ValLoss: 0.39576 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.45514 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 239 |  TrainLoss: 0.00790 | ValLoss: 0.39429 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.44096 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 240 |  TrainLoss: 0.00684 | ValLoss: 0.40430 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.47230 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 241 |  TrainLoss: 0.00688 | ValLoss: 0.40498 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.47251 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 242 |  TrainLoss: 0.01046 | ValLoss: 0.40324 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.44498 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 243 |  TrainLoss: 0.00636 | ValLoss: 0.40509 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.45627 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 244 |  TrainLoss: 0.00442 | ValLoss: 0.40815 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.46090 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 245 |  TrainLoss: 0.00486 | ValLoss: 0.41848 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.44328 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 246 |  TrainLoss: 0.00368 | ValLoss: 0.41597 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.45397 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 247 |  TrainLoss: 0.00405 | ValLoss: 0.41661 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.46372 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 248 |  TrainLoss: 0.00484 | ValLoss: 0.42354 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.48998 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 249 |  TrainLoss: 0.00394 | ValLoss: 0.42425 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.48733 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 250 |  TrainLoss: 0.00401 | ValLoss: 0.42378 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.47538 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 251 |  TrainLoss: 0.00718 | ValLoss: 0.42590 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.49680 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 252 |  TrainLoss: 0.00368 | ValLoss: 0.42410 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.48047 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 253 |  TrainLoss: 0.00288 | ValLoss: 0.43565 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.46456 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 254 |  TrainLoss: 0.00494 | ValLoss: 0.43035 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.48103 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 255 |  TrainLoss: 0.00291 | ValLoss: 0.43669 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.52099 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 256 |  TrainLoss: 0.00359 | ValLoss: 0.43663 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.51658 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 257 |  TrainLoss: 0.00412 | ValLoss: 0.43905 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.49107 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 258 |  TrainLoss: 0.00332 | ValLoss: 0.45791 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.47081 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 259 |  TrainLoss: 0.00404 | ValLoss: 0.44660 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.48789 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 260 |  TrainLoss: 0.00357 | ValLoss: 0.47514 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55894 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 261 |  TrainLoss: 0.00419 | ValLoss: 0.44937 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.52061 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 262 |  TrainLoss: 0.00315 | ValLoss: 0.44813 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.49046 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 263 |  TrainLoss: 0.00234 | ValLoss: 0.44838 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.50056 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 264 |  TrainLoss: 0.00332 | ValLoss: 0.46957 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.54137 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 265 |  TrainLoss: 0.00341 | ValLoss: 0.45100 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.50453 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 266 |  TrainLoss: 0.00275 | ValLoss: 0.45647 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.51835 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 267 |  TrainLoss: 0.00343 | ValLoss: 0.47732 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55712 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 268 |  TrainLoss: 0.00307 | ValLoss: 0.46790 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.50727 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 269 |  TrainLoss: 0.00305 | ValLoss: 0.47055 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.53063 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 270 |  TrainLoss: 0.00235 | ValLoss: 0.49014 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.56391 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 271 |  TrainLoss: 0.00282 | ValLoss: 0.47672 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.52040 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 272 |  TrainLoss: 0.00408 | ValLoss: 0.48467 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.51534 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 273 |  TrainLoss: 0.00236 | ValLoss: 0.50289 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.58412 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 274 |  TrainLoss: 0.00218 | ValLoss: 0.49729 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.57427 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 275 |  TrainLoss: 0.00200 | ValLoss: 0.49139 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55014 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 276 |  TrainLoss: 0.00192 | ValLoss: 0.49516 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.54082 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 277 |  TrainLoss: 0.00181 | ValLoss: 0.49804 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.57222 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 278 |  TrainLoss: 0.00143 | ValLoss: 0.49693 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55637 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 279 |  TrainLoss: 0.00153 | ValLoss: 0.49728 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55709 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 280 |  TrainLoss: 0.00105 | ValLoss: 0.49796 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55542 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 281 |  TrainLoss: 0.00116 | ValLoss: 0.49983 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.55045 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 282 |  TrainLoss: 0.00109 | ValLoss: 0.49834 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.56488 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 283 |  TrainLoss: 0.00228 | ValLoss: 0.50060 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.58030 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 284 |  TrainLoss: 0.00246 | ValLoss: 0.50675 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.53682 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 285 |  TrainLoss: 0.00239 | ValLoss: 0.48825 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.57622 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 286 |  TrainLoss: 0.00113 | ValLoss: 0.55414 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65227 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 287 |  TrainLoss: 0.00195 | ValLoss: 0.48626 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.58129 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 288 |  TrainLoss: 0.00156 | ValLoss: 0.51119 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.52199 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 289 |  TrainLoss: 0.00157 | ValLoss: 0.48556 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.56940 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 290 |  TrainLoss: 0.00092 | ValLoss: 0.52572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.62781 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 291 |  TrainLoss: 0.00134 | ValLoss: 0.51821 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.61693 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 292 |  TrainLoss: 0.00074 | ValLoss: 0.50132 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.55620 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 293 |  TrainLoss: 0.00104 | ValLoss: 0.50530 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.55705 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 294 |  TrainLoss: 0.00067 | ValLoss: 0.50465 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.58125 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 295 |  TrainLoss: 0.00089 | ValLoss: 0.51799 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.61274 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 296 |  TrainLoss: 0.00119 | ValLoss: 0.51151 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.59966 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 297 |  TrainLoss: 0.00103 | ValLoss: 0.53518 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.54976 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 298 |  TrainLoss: 0.00136 | ValLoss: 0.51709 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.58351 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 299 |  TrainLoss: 0.00055 | ValLoss: 0.55250 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66363 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 300 |  TrainLoss: 0.00097 | ValLoss: 0.55174 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66787 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 301 |  TrainLoss: 0.00113 | ValLoss: 0.53194 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.59003 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 302 |  TrainLoss: 0.00080 | ValLoss: 0.55696 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.57336 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 303 |  TrainLoss: 0.00081 | ValLoss: 0.54363 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.59148 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 304 |  TrainLoss: 0.00124 | ValLoss: 0.58229 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.69715 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 305 |  TrainLoss: 0.00085 | ValLoss: 0.57994 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.68381 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 306 |  TrainLoss: 0.00059 | ValLoss: 0.55203 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.63463 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 307 |  TrainLoss: 0.00064 | ValLoss: 0.55340 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.61120 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 308 |  TrainLoss: 0.00360 | ValLoss: 0.59855 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.68553 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 309 |  TrainLoss: 0.00268 | ValLoss: 0.71112 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.75820 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 310 |  TrainLoss: 0.00146 | ValLoss: 0.58950 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.56813 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 311 |  TrainLoss: 0.00293 | ValLoss: 0.61534 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.56667 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 312 |  TrainLoss: 0.00071 | ValLoss: 0.59280 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.69638 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 313 |  TrainLoss: 0.00083 | ValLoss: 0.69044 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.77395 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 314 |  TrainLoss: 0.00187 | ValLoss: 0.55966 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66449 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 315 |  TrainLoss: 0.00118 | ValLoss: 0.56913 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.61380 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 316 |  TrainLoss: 0.00057 | ValLoss: 0.57456 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.61163 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 317 |  TrainLoss: 0.00051 | ValLoss: 0.56492 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.62620 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 318 |  TrainLoss: 0.00040 | ValLoss: 0.56146 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.64500 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 319 |  TrainLoss: 0.00055 | ValLoss: 0.56441 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65895 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 320 |  TrainLoss: 0.00080 | ValLoss: 0.56572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65837 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 321 |  TrainLoss: 0.00035 | ValLoss: 0.56697 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.64557 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 322 |  TrainLoss: 0.00042 | ValLoss: 0.57117 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.63894 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 323 |  TrainLoss: 0.00060 | ValLoss: 0.57235 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65950 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 324 |  TrainLoss: 0.00054 | ValLoss: 0.57657 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65790 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 325 |  TrainLoss: 0.00044 | ValLoss: 0.58043 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65562 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 326 |  TrainLoss: 0.00063 | ValLoss: 0.59281 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.63632 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 327 |  TrainLoss: 0.00054 | ValLoss: 0.60367 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.63034 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 328 |  TrainLoss: 0.00067 | ValLoss: 0.59268 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.65049 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 329 |  TrainLoss: 0.00039 | ValLoss: 0.59381 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.68292 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 330 |  TrainLoss: 0.00052 | ValLoss: 0.59828 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.68966 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 331 |  TrainLoss: 0.00058 | ValLoss: 0.59722 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66416 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 332 |  TrainLoss: 0.00033 | ValLoss: 0.59960 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.65971 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 333 |  TrainLoss: 0.00031 | ValLoss: 0.60045 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66423 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 334 |  TrainLoss: 0.00032 | ValLoss: 0.60147 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66542 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 335 |  TrainLoss: 0.00029 | ValLoss: 0.60267 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66481 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 336 |  TrainLoss: 0.00038 | ValLoss: 0.60374 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.66405 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 337 |  TrainLoss: 0.00034 | ValLoss: 0.60330 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.67638 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 338 |  TrainLoss: 0.00049 | ValLoss: 0.64847 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.74769 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 339 |  TrainLoss: 0.00045 | ValLoss: 0.66303 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.76624 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 340 |  TrainLoss: 0.00043 | ValLoss: 0.60959 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.70274 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 341 |  TrainLoss: 0.00040 | ValLoss: 0.62401 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.66971 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 342 |  TrainLoss: 0.00025 | ValLoss: 0.61819 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.68611 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 343 |  TrainLoss: 0.00027 | ValLoss: 0.61585 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.71562 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 344 |  TrainLoss: 0.00041 | ValLoss: 0.62324 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.73929 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 345 |  TrainLoss: 0.00025 | ValLoss: 0.62352 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.69486 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 346 |  TrainLoss: 0.00020 | ValLoss: 0.65479 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.67025 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 347 |  TrainLoss: 0.00050 | ValLoss: 0.63317 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.69800 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 348 |  TrainLoss: 0.00021 | ValLoss: 0.62169 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73611 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 349 |  TrainLoss: 0.00046 | ValLoss: 0.64535 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.79621 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 350 |  TrainLoss: 0.00063 | ValLoss: 0.63734 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.78745 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 351 |  TrainLoss: 0.00016 | ValLoss: 0.67500 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.68510 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 352 |  TrainLoss: 0.00028 | ValLoss: 0.70168 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.67532 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 353 |  TrainLoss: 0.00020 | ValLoss: 0.64533 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.70854 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 354 |  TrainLoss: 0.00042 | ValLoss: 0.68894 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.85299 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 355 |  TrainLoss: 0.00042 | ValLoss: 0.73603 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.88684 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 356 |  TrainLoss: 0.00077 | ValLoss: 0.63252 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.70907 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 357 |  TrainLoss: 0.00068 | ValLoss: 0.80373 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.66621 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 358 |  TrainLoss: 0.00162 | ValLoss: 0.76593 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.90019 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 359 |  TrainLoss: 0.00172 | ValLoss: 0.63799 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.79908 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 360 |  TrainLoss: 0.00013 | ValLoss: 0.64824 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.66481 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 361 |  TrainLoss: 0.00045 | ValLoss: 0.72717 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.65379 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 362 |  TrainLoss: 0.00162 | ValLoss: 0.59248 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.70685 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 363 |  TrainLoss: 0.00110 | ValLoss: 0.77871 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.84783 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 364 |  TrainLoss: 0.00056 | ValLoss: 0.61702 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73890 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 365 |  TrainLoss: 0.00024 | ValLoss: 0.63233 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.67267 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 366 |  TrainLoss: 0.00014 | ValLoss: 0.69444 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.65225 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 367 |  TrainLoss: 0.00049 | ValLoss: 0.68460 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.65467 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 368 |  TrainLoss: 0.00046 | ValLoss: 0.61778 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.70182 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 369 |  TrainLoss: 0.00029 | ValLoss: 0.64582 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.76685 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 370 |  TrainLoss: 0.00065 | ValLoss: 0.63909 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.76083 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 371 |  TrainLoss: 0.00019 | ValLoss: 0.62542 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.71367 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 372 |  TrainLoss: 0.00023 | ValLoss: 0.63860 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.69485 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 373 |  TrainLoss: 0.00008 | ValLoss: 0.64587 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.69008 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 374 |  TrainLoss: 0.00014 | ValLoss: 0.65039 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.68833 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 375 |  TrainLoss: 0.00028 | ValLoss: 0.63997 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.70069 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 376 |  TrainLoss: 0.00011 | ValLoss: 0.63359 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.72429 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 377 |  TrainLoss: 0.00016 | ValLoss: 0.63755 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.74005 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 378 |  TrainLoss: 0.00018 | ValLoss: 0.63813 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.74118 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 379 |  TrainLoss: 0.00014 | ValLoss: 0.63866 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.74123 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 380 |  TrainLoss: 0.00013 | ValLoss: 0.63863 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73772 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 381 |  TrainLoss: 0.00011 | ValLoss: 0.63938 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73238 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 382 |  TrainLoss: 0.00023 | ValLoss: 0.64057 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.72921 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 383 |  TrainLoss: 0.00018 | ValLoss: 0.64274 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73070 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 384 |  TrainLoss: 0.00010 | ValLoss: 0.64542 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.73388 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 385 |  TrainLoss: 0.00009 | ValLoss: 0.64727 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.74099 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 386 |  TrainLoss: 0.00012 | ValLoss: 0.64967 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.74630 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 387 |  TrainLoss: 0.00011 | ValLoss: 0.65153 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.74928 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 388 |  TrainLoss: 0.00021 | ValLoss: 0.65577 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.76304 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 389 |  TrainLoss: 0.00012 | ValLoss: 0.65822 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.76826 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 390 |  TrainLoss: 0.00008 | ValLoss: 0.65855 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.76409 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 391 |  TrainLoss: 0.00011 | ValLoss: 0.66019 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.75858 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 392 |  TrainLoss: 0.00011 | ValLoss: 0.66203 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.76390 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 393 |  TrainLoss: 0.00009 | ValLoss: 0.66523 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.75704 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 394 |  TrainLoss: 0.00011 | ValLoss: 0.66938 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.75247 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 395 |  TrainLoss: 0.00027 | ValLoss: 0.69266 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.73029 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 396 |  TrainLoss: 0.00009 | ValLoss: 0.72214 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.71585 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 397 |  TrainLoss: 0.00029 | ValLoss: 0.67493 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.77310 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 398 |  TrainLoss: 0.00011 | ValLoss: 0.72784 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.86424 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 399 |  TrainLoss: 0.00020 | ValLoss: 0.71009 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.85062 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 400 |  TrainLoss: 0.00016 | ValLoss: 0.68360 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.79046 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 401 |  TrainLoss: 0.00008 | ValLoss: 0.71158 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.75730 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 402 |  TrainLoss: 0.00011 | ValLoss: 0.70705 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.76761 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 403 |  TrainLoss: 0.00009 | ValLoss: 0.70337 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.77759 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 404 |  TrainLoss: 0.00013 | ValLoss: 0.69647 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.79995 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 405 |  TrainLoss: 0.00014 | ValLoss: 0.70329 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.83105 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 406 |  TrainLoss: 0.00010 | ValLoss: 0.71090 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.84423 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 407 |  TrainLoss: 0.00010 | ValLoss: 0.71443 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.84704 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 408 |  TrainLoss: 0.00007 | ValLoss: 0.70935 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.83117 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 409 |  TrainLoss: 0.00008 | ValLoss: 0.70947 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.81323 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 410 |  TrainLoss: 0.00075 | ValLoss: 1.01723 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.01634 | TestAcc: 0.85484 | TestF1: 0.82\n",
            "Epoch: 411 |  TrainLoss: 0.00088 | ValLoss: 0.74715 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.85283 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 412 |  TrainLoss: 0.00032 | ValLoss: 0.95558 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.73366 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 413 |  TrainLoss: 0.00065 | ValLoss: 0.78053 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.76752 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 414 |  TrainLoss: 0.00005 | ValLoss: 0.87830 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.96822 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 415 |  TrainLoss: 0.00087 | ValLoss: 0.75416 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.83502 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 416 |  TrainLoss: 0.00100 | ValLoss: 0.79325 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.77378 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 417 |  TrainLoss: 0.00008 | ValLoss: 0.78715 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.89112 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 418 |  TrainLoss: 0.00011 | ValLoss: 0.87131 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.94634 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 419 |  TrainLoss: 0.00032 | ValLoss: 0.82398 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.91917 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 420 |  TrainLoss: 0.00014 | ValLoss: 0.76479 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.84405 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 421 |  TrainLoss: 0.00004 | ValLoss: 0.78445 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.78903 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 422 |  TrainLoss: 0.00005 | ValLoss: 0.81731 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.75950 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 423 |  TrainLoss: 0.00010 | ValLoss: 0.82615 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.75413 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 424 |  TrainLoss: 0.00027 | ValLoss: 0.75708 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.82135 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 425 |  TrainLoss: 0.00006 | ValLoss: 0.75853 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.87998 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 426 |  TrainLoss: 0.00034 | ValLoss: 0.74529 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.82992 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 427 |  TrainLoss: 0.00008 | ValLoss: 0.77064 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.78509 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 428 |  TrainLoss: 0.00008 | ValLoss: 0.79359 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.76710 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 429 |  TrainLoss: 0.00026 | ValLoss: 0.74332 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.83313 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 430 |  TrainLoss: 0.00010 | ValLoss: 0.75629 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.88682 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 431 |  TrainLoss: 0.00009 | ValLoss: 0.77703 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.90975 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 432 |  TrainLoss: 0.00010 | ValLoss: 0.77604 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.90865 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 433 |  TrainLoss: 0.00008 | ValLoss: 0.75829 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.88841 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 434 |  TrainLoss: 0.00006 | ValLoss: 0.74879 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.86795 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 435 |  TrainLoss: 0.00011 | ValLoss: 0.74596 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.84460 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 436 |  TrainLoss: 0.00004 | ValLoss: 0.75647 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.81713 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 437 |  TrainLoss: 0.00004 | ValLoss: 0.76955 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.80204 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 438 |  TrainLoss: 0.00005 | ValLoss: 0.78072 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.79307 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 439 |  TrainLoss: 0.00004 | ValLoss: 0.78921 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.78775 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 440 |  TrainLoss: 0.00008 | ValLoss: 0.78649 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.79333 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 441 |  TrainLoss: 0.00007 | ValLoss: 0.78173 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.80085 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 442 |  TrainLoss: 0.00003 | ValLoss: 0.77514 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.81068 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 443 |  TrainLoss: 0.00008 | ValLoss: 0.77571 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.81490 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 444 |  TrainLoss: 0.00089 | ValLoss: 0.92355 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.71501 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 445 |  TrainLoss: 0.00129 | ValLoss: 0.82797 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.93122 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 446 |  TrainLoss: 0.00067 | ValLoss: 1.06388 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.02769 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 447 |  TrainLoss: 0.00038 | ValLoss: 0.75135 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.89812 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 448 |  TrainLoss: 0.00007 | ValLoss: 0.79950 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.78570 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 449 |  TrainLoss: 0.00016 | ValLoss: 0.87216 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.75711 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 450 |  TrainLoss: 0.00009 | ValLoss: 0.87985 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.75638 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 451 |  TrainLoss: 0.00007 | ValLoss: 0.85981 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.76563 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 452 |  TrainLoss: 0.00004 | ValLoss: 0.83429 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.77688 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 453 |  TrainLoss: 0.00006 | ValLoss: 0.80465 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.79150 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 454 |  TrainLoss: 0.00005 | ValLoss: 0.77528 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.80898 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 455 |  TrainLoss: 0.00004 | ValLoss: 0.75393 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.82568 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 456 |  TrainLoss: 0.00005 | ValLoss: 0.74008 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.84091 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 457 |  TrainLoss: 0.00004 | ValLoss: 0.73299 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.85228 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 458 |  TrainLoss: 0.00004 | ValLoss: 0.73094 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.85813 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 459 |  TrainLoss: 0.00017 | ValLoss: 0.72661 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.87927 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 460 |  TrainLoss: 0.00006 | ValLoss: 0.73518 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.90723 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 461 |  TrainLoss: 0.00004 | ValLoss: 0.74085 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.91389 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 462 |  TrainLoss: 0.00014 | ValLoss: 0.74213 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.91465 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 463 |  TrainLoss: 0.00006 | ValLoss: 0.75467 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.93237 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 464 |  TrainLoss: 0.00007 | ValLoss: 0.75044 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.92937 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 465 |  TrainLoss: 0.00004 | ValLoss: 0.73798 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.91496 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 466 |  TrainLoss: 0.00004 | ValLoss: 0.72841 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.89798 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 467 |  TrainLoss: 0.00005 | ValLoss: 0.72450 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.87935 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 468 |  TrainLoss: 0.00009 | ValLoss: 0.72965 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.85985 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 469 |  TrainLoss: 0.00004 | ValLoss: 0.74653 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.84131 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 470 |  TrainLoss: 0.00006 | ValLoss: 0.75628 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.83610 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 471 |  TrainLoss: 0.00004 | ValLoss: 0.76183 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.83477 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 472 |  TrainLoss: 0.00006 | ValLoss: 0.76864 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.83294 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 473 |  TrainLoss: 0.00006 | ValLoss: 0.76075 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.84439 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 474 |  TrainLoss: 0.00002 | ValLoss: 0.74861 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.86406 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 475 |  TrainLoss: 0.00007 | ValLoss: 0.75350 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.86278 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 476 |  TrainLoss: 0.00005 | ValLoss: 0.75214 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.86777 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 477 |  TrainLoss: 0.00002 | ValLoss: 0.74992 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.87404 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 478 |  TrainLoss: 0.00005 | ValLoss: 0.75290 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.87401 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 479 |  TrainLoss: 0.00003 | ValLoss: 0.75919 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.87128 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 480 |  TrainLoss: 0.00003 | ValLoss: 0.76324 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.87142 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 481 |  TrainLoss: 0.00003 | ValLoss: 0.76035 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.88230 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 482 |  TrainLoss: 0.00002 | ValLoss: 0.75771 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.89407 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 483 |  TrainLoss: 0.00005 | ValLoss: 0.75950 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.89725 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 484 |  TrainLoss: 0.00003 | ValLoss: 0.76571 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.89349 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 485 |  TrainLoss: 0.00002 | ValLoss: 0.76921 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.89546 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 486 |  TrainLoss: 0.00003 | ValLoss: 0.77524 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.89231 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 487 |  TrainLoss: 0.00002 | ValLoss: 0.78386 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.88649 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 488 |  TrainLoss: 0.00004 | ValLoss: 0.77400 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.90598 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 489 |  TrainLoss: 0.00002 | ValLoss: 0.77286 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.92811 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 490 |  TrainLoss: 0.00002 | ValLoss: 0.77674 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.94181 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 491 |  TrainLoss: 0.00003 | ValLoss: 0.77823 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.94258 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 492 |  TrainLoss: 0.00002 | ValLoss: 0.77808 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.93437 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 493 |  TrainLoss: 0.00002 | ValLoss: 0.78002 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.92120 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 494 |  TrainLoss: 0.00001 | ValLoss: 0.78581 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.91091 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 495 |  TrainLoss: 0.00002 | ValLoss: 0.79021 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.90765 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 496 |  TrainLoss: 0.00003 | ValLoss: 0.79458 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.90408 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 497 |  TrainLoss: 0.00006 | ValLoss: 0.78273 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.94511 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 498 |  TrainLoss: 0.00007 | ValLoss: 0.79627 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.98442 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 499 |  TrainLoss: 0.00004 | ValLoss: 0.79523 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.99366 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 500 |  TrainLoss: 0.00002 | ValLoss: 0.78627 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.98339 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 501 |  TrainLoss: 0.00003 | ValLoss: 0.78526 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.97211 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 502 |  TrainLoss: 0.00004 | ValLoss: 0.79494 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.94699 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 503 |  TrainLoss: 0.00002 | ValLoss: 0.84074 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.91008 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 504 |  TrainLoss: 0.00004 | ValLoss: 0.80329 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.95001 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 505 |  TrainLoss: 0.00003 | ValLoss: 0.79786 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.96912 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 506 |  TrainLoss: 0.00002 | ValLoss: 0.80616 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.96031 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 507 |  TrainLoss: 0.00001 | ValLoss: 0.81959 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.94700 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 508 |  TrainLoss: 0.00003 | ValLoss: 0.80936 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.96485 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 509 |  TrainLoss: 0.00004 | ValLoss: 0.80379 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.99730 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 510 |  TrainLoss: 0.00001 | ValLoss: 0.80811 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.00591 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 511 |  TrainLoss: 0.00001 | ValLoss: 0.80998 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.99447 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 512 |  TrainLoss: 0.00002 | ValLoss: 0.81216 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.00167 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 513 |  TrainLoss: 0.00001 | ValLoss: 0.81948 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.02262 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 514 |  TrainLoss: 0.00002 | ValLoss: 0.81793 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.01373 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 515 |  TrainLoss: 0.00002 | ValLoss: 0.81863 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.99835 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 516 |  TrainLoss: 0.00001 | ValLoss: 0.83294 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.97078 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 517 |  TrainLoss: 0.00001 | ValLoss: 0.85724 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.94650 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 518 |  TrainLoss: 0.00001 | ValLoss: 0.88196 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.93241 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 519 |  TrainLoss: 0.00002 | ValLoss: 0.86209 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.94773 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 520 |  TrainLoss: 0.00001 | ValLoss: 0.83378 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.98405 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 521 |  TrainLoss: 0.00001 | ValLoss: 0.83233 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.01484 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 522 |  TrainLoss: 0.00001 | ValLoss: 0.83701 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.02539 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 523 |  TrainLoss: 0.00002 | ValLoss: 0.84150 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.03301 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 524 |  TrainLoss: 0.00003 | ValLoss: 0.90377 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10031 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 525 |  TrainLoss: 0.00002 | ValLoss: 0.89410 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09915 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 526 |  TrainLoss: 0.00005 | ValLoss: 0.88244 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10754 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 527 |  TrainLoss: 0.00001 | ValLoss: 0.87390 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11577 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 528 |  TrainLoss: 0.00002 | ValLoss: 0.86258 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.08219 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 529 |  TrainLoss: 0.00001 | ValLoss: 0.90607 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.02416 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 530 |  TrainLoss: 0.00002 | ValLoss: 0.91952 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.02260 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 531 |  TrainLoss: 0.00001 | ValLoss: 0.88782 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.06540 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 532 |  TrainLoss: 0.00001 | ValLoss: 0.87851 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10567 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 533 |  TrainLoss: 0.00001 | ValLoss: 0.88659 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.12343 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 534 |  TrainLoss: 0.00001 | ValLoss: 0.88679 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.12024 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 535 |  TrainLoss: 0.00001 | ValLoss: 0.88601 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11103 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 536 |  TrainLoss: 0.00001 | ValLoss: 0.88567 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11080 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 537 |  TrainLoss: 0.00001 | ValLoss: 0.88778 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09690 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 538 |  TrainLoss: 0.00001 | ValLoss: 0.89692 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.13250 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 539 |  TrainLoss: 0.00001 | ValLoss: 0.92113 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15437 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 540 |  TrainLoss: 0.00001 | ValLoss: 0.89925 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11204 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 541 |  TrainLoss: 0.00000 | ValLoss: 0.91767 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.06810 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 542 |  TrainLoss: 0.00000 | ValLoss: 0.94077 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.04421 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 543 |  TrainLoss: 0.00001 | ValLoss: 0.93272 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05955 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 544 |  TrainLoss: 0.00000 | ValLoss: 0.91964 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.07781 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 545 |  TrainLoss: 0.00001 | ValLoss: 0.91266 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10484 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 546 |  TrainLoss: 0.00000 | ValLoss: 0.92292 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13065 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 547 |  TrainLoss: 0.00001 | ValLoss: 0.92165 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.12948 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 548 |  TrainLoss: 0.00001 | ValLoss: 0.92509 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.08813 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 549 |  TrainLoss: 0.00000 | ValLoss: 0.95380 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05110 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 550 |  TrainLoss: 0.00001 | ValLoss: 0.92850 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09636 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 551 |  TrainLoss: 0.00000 | ValLoss: 0.99475 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22088 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 552 |  TrainLoss: 0.00001 | ValLoss: 1.02395 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.23455 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 553 |  TrainLoss: 0.00001 | ValLoss: 0.92970 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13488 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 554 |  TrainLoss: 0.00001 | ValLoss: 0.93407 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07036 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 555 |  TrainLoss: 0.00001 | ValLoss: 0.97444 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.02369 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 556 |  TrainLoss: 0.00001 | ValLoss: 0.95203 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05334 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 557 |  TrainLoss: 0.00001 | ValLoss: 0.93400 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10785 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 558 |  TrainLoss: 0.00002 | ValLoss: 1.13056 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.26615 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 559 |  TrainLoss: 0.00004 | ValLoss: 1.13550 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.99736 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 560 |  TrainLoss: 0.00008 | ValLoss: 1.06695 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05994 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 561 |  TrainLoss: 0.00001 | ValLoss: 1.10101 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.27651 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 562 |  TrainLoss: 0.00002 | ValLoss: 1.18418 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.31823 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 563 |  TrainLoss: 0.00005 | ValLoss: 1.14231 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07758 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 564 |  TrainLoss: 0.00030 | ValLoss: 1.03535 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.24255 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 565 |  TrainLoss: 0.00042 | ValLoss: 1.26437 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.35325 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 566 |  TrainLoss: 0.00001 | ValLoss: 1.04068 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09653 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 567 |  TrainLoss: 0.00074 | ValLoss: 1.11476 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.02195 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 568 |  TrainLoss: 0.00000 | ValLoss: 1.09285 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.28995 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 569 |  TrainLoss: 0.00030 | ValLoss: 1.37526 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.38359 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 570 |  TrainLoss: 0.00012 | ValLoss: 1.09193 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.31076 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 571 |  TrainLoss: 0.00004 | ValLoss: 0.90380 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19720 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 572 |  TrainLoss: 0.00001 | ValLoss: 0.88424 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.11989 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 573 |  TrainLoss: 0.00000 | ValLoss: 0.91224 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08109 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 574 |  TrainLoss: 0.00001 | ValLoss: 0.95489 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05714 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 575 |  TrainLoss: 0.00001 | ValLoss: 0.97476 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05037 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 576 |  TrainLoss: 0.00001 | ValLoss: 0.98550 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04354 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 577 |  TrainLoss: 0.00000 | ValLoss: 0.99054 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04104 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 578 |  TrainLoss: 0.00001 | ValLoss: 0.99076 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04098 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 579 |  TrainLoss: 0.00000 | ValLoss: 0.98943 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04198 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 580 |  TrainLoss: 0.00001 | ValLoss: 0.98625 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04440 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 581 |  TrainLoss: 0.00003 | ValLoss: 0.96121 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.05412 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 582 |  TrainLoss: 0.00001 | ValLoss: 0.93700 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07106 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 583 |  TrainLoss: 0.00000 | ValLoss: 0.90813 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08237 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 584 |  TrainLoss: 0.00001 | ValLoss: 0.89788 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09175 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 585 |  TrainLoss: 0.00004 | ValLoss: 0.88221 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.10730 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 586 |  TrainLoss: 0.00001 | ValLoss: 0.86261 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13222 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 587 |  TrainLoss: 0.00000 | ValLoss: 0.86309 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14626 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 588 |  TrainLoss: 0.00000 | ValLoss: 0.86578 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15623 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 589 |  TrainLoss: 0.00000 | ValLoss: 0.86800 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16175 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 590 |  TrainLoss: 0.00000 | ValLoss: 0.86926 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16405 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 591 |  TrainLoss: 0.00000 | ValLoss: 0.87009 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16596 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 592 |  TrainLoss: 0.00000 | ValLoss: 0.87056 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16695 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 593 |  TrainLoss: 0.00001 | ValLoss: 0.86949 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16502 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 594 |  TrainLoss: 0.00001 | ValLoss: 0.86821 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16200 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 595 |  TrainLoss: 0.00001 | ValLoss: 0.86704 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.15969 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 596 |  TrainLoss: 0.00001 | ValLoss: 0.86535 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15575 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 597 |  TrainLoss: 0.00001 | ValLoss: 0.86327 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15238 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 598 |  TrainLoss: 0.00000 | ValLoss: 0.86220 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14746 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 599 |  TrainLoss: 0.00000 | ValLoss: 0.86166 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14302 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 600 |  TrainLoss: 0.00000 | ValLoss: 0.87070 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14081 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 601 |  TrainLoss: 0.00001 | ValLoss: 0.87076 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13746 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 602 |  TrainLoss: 0.00001 | ValLoss: 0.87214 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14418 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 603 |  TrainLoss: 0.00000 | ValLoss: 0.86549 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15279 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 604 |  TrainLoss: 0.00000 | ValLoss: 0.86748 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15819 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 605 |  TrainLoss: 0.00000 | ValLoss: 0.86848 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15958 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 606 |  TrainLoss: 0.00000 | ValLoss: 0.86843 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15958 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 607 |  TrainLoss: 0.00000 | ValLoss: 0.86819 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15848 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 608 |  TrainLoss: 0.00000 | ValLoss: 0.86764 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15623 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 609 |  TrainLoss: 0.00001 | ValLoss: 0.86761 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15655 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 610 |  TrainLoss: 0.00001 | ValLoss: 0.87671 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15502 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 611 |  TrainLoss: 0.00000 | ValLoss: 0.87664 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15407 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 612 |  TrainLoss: 0.00000 | ValLoss: 0.87617 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15270 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 613 |  TrainLoss: 0.00000 | ValLoss: 0.87572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15041 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 614 |  TrainLoss: 0.00001 | ValLoss: 0.87530 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15083 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 615 |  TrainLoss: 0.00001 | ValLoss: 0.87505 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14475 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 616 |  TrainLoss: 0.00000 | ValLoss: 0.87511 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13954 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 617 |  TrainLoss: 0.00001 | ValLoss: 0.87520 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13965 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 618 |  TrainLoss: 0.00001 | ValLoss: 0.87557 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14046 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 619 |  TrainLoss: 0.00000 | ValLoss: 0.87668 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14223 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 620 |  TrainLoss: 0.00000 | ValLoss: 0.87772 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14379 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 621 |  TrainLoss: 0.00000 | ValLoss: 0.87816 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14331 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 622 |  TrainLoss: 0.00000 | ValLoss: 0.87818 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14156 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 623 |  TrainLoss: 0.00000 | ValLoss: 0.87813 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13770 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 624 |  TrainLoss: 0.00001 | ValLoss: 0.87811 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13960 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 625 |  TrainLoss: 0.00001 | ValLoss: 0.87849 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13988 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 626 |  TrainLoss: 0.00000 | ValLoss: 0.89224 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13965 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 627 |  TrainLoss: 0.00000 | ValLoss: 0.89298 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13806 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 628 |  TrainLoss: 0.00001 | ValLoss: 0.89402 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14346 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 629 |  TrainLoss: 0.00000 | ValLoss: 0.89552 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14814 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 630 |  TrainLoss: 0.00005 | ValLoss: 0.99332 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.28244 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 631 |  TrainLoss: 0.00019 | ValLoss: 0.94820 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.12962 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 632 |  TrainLoss: 0.00006 | ValLoss: 1.29557 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.02681 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 633 |  TrainLoss: 0.00006 | ValLoss: 1.14706 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.04615 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 634 |  TrainLoss: 0.00003 | ValLoss: 0.97043 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14538 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 635 |  TrainLoss: 0.00000 | ValLoss: 0.93533 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.24764 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 636 |  TrainLoss: 0.00000 | ValLoss: 1.00349 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.30829 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 637 |  TrainLoss: 0.00000 | ValLoss: 1.06100 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.33611 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 638 |  TrainLoss: 0.00002 | ValLoss: 1.06877 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.33753 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 639 |  TrainLoss: 0.00001 | ValLoss: 1.04089 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.32072 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 640 |  TrainLoss: 0.00015 | ValLoss: 1.04909 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08777 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 641 |  TrainLoss: 0.00002 | ValLoss: 1.29038 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.99026 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 642 |  TrainLoss: 0.00011 | ValLoss: 1.25810 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.98156 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 643 |  TrainLoss: 0.00204 | ValLoss: 0.95864 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.21410 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 644 |  TrainLoss: 0.00009 | ValLoss: 1.79296 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 1.57611 | TestAcc: 0.85484 | TestF1: 0.81\n",
            "Epoch: 645 |  TrainLoss: 0.02282 | ValLoss: 1.13801 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.30094 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 646 |  TrainLoss: 0.00000 | ValLoss: 1.20766 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.96791 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 647 |  TrainLoss: 0.00471 | ValLoss: 1.31687 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 1.02211 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 648 |  TrainLoss: 0.00041 | ValLoss: 1.10620 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.01187 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 649 |  TrainLoss: 0.00001 | ValLoss: 0.97997 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11189 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 650 |  TrainLoss: 0.00001 | ValLoss: 1.07267 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20402 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 651 |  TrainLoss: 0.00002 | ValLoss: 1.16322 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.25402 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 652 |  TrainLoss: 0.00002 | ValLoss: 1.22216 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.28199 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 653 |  TrainLoss: 0.00002 | ValLoss: 1.26003 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.29815 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 654 |  TrainLoss: 0.00008 | ValLoss: 1.27449 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.30299 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 655 |  TrainLoss: 0.00002 | ValLoss: 1.27893 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.30375 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 656 |  TrainLoss: 0.00002 | ValLoss: 1.27994 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.30404 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 657 |  TrainLoss: 0.00002 | ValLoss: 1.27651 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.30205 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 658 |  TrainLoss: 0.00003 | ValLoss: 1.27069 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.29877 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 659 |  TrainLoss: 0.00004 | ValLoss: 1.26129 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.29464 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 660 |  TrainLoss: 0.00014 | ValLoss: 1.23691 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 1.28353 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 661 |  TrainLoss: 0.00003 | ValLoss: 1.19639 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.26348 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 662 |  TrainLoss: 0.00001 | ValLoss: 1.17105 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.25009 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 663 |  TrainLoss: 0.00001 | ValLoss: 1.15560 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.24094 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 664 |  TrainLoss: 0.00001 | ValLoss: 1.14456 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.23482 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 665 |  TrainLoss: 0.00001 | ValLoss: 1.13751 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22994 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 666 |  TrainLoss: 0.00001 | ValLoss: 1.13156 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22741 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 667 |  TrainLoss: 0.00001 | ValLoss: 1.12719 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22479 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 668 |  TrainLoss: 0.00001 | ValLoss: 1.12553 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22284 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 669 |  TrainLoss: 0.00001 | ValLoss: 1.12233 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22072 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 670 |  TrainLoss: 0.00003 | ValLoss: 1.11551 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.21758 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 671 |  TrainLoss: 0.00002 | ValLoss: 1.10961 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.21307 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 672 |  TrainLoss: 0.00001 | ValLoss: 1.10369 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20932 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 673 |  TrainLoss: 0.00001 | ValLoss: 1.10064 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20734 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 674 |  TrainLoss: 0.00002 | ValLoss: 1.09623 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20454 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 675 |  TrainLoss: 0.00001 | ValLoss: 1.09143 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20144 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 676 |  TrainLoss: 0.00001 | ValLoss: 1.08764 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19759 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 677 |  TrainLoss: 0.00001 | ValLoss: 1.08282 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19559 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 678 |  TrainLoss: 0.00000 | ValLoss: 1.07887 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19296 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 679 |  TrainLoss: 0.00001 | ValLoss: 1.07726 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19035 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 680 |  TrainLoss: 0.00001 | ValLoss: 1.07313 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.18764 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 681 |  TrainLoss: 0.00001 | ValLoss: 1.06897 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.18592 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 682 |  TrainLoss: 0.00002 | ValLoss: 1.06683 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.18300 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 683 |  TrainLoss: 0.00001 | ValLoss: 1.06185 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.17920 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 684 |  TrainLoss: 0.00001 | ValLoss: 1.05750 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.17605 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 685 |  TrainLoss: 0.00001 | ValLoss: 1.05364 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.17302 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 686 |  TrainLoss: 0.00001 | ValLoss: 1.05204 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.17158 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 687 |  TrainLoss: 0.00001 | ValLoss: 1.04876 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16912 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 688 |  TrainLoss: 0.00001 | ValLoss: 1.04582 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16698 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 689 |  TrainLoss: 0.00001 | ValLoss: 1.04327 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16510 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 690 |  TrainLoss: 0.00000 | ValLoss: 1.04291 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16275 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 691 |  TrainLoss: 0.00001 | ValLoss: 1.04077 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16089 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 692 |  TrainLoss: 0.00000 | ValLoss: 1.03850 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16075 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 693 |  TrainLoss: 0.00001 | ValLoss: 1.03653 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15937 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 694 |  TrainLoss: 0.00000 | ValLoss: 1.03469 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15808 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 695 |  TrainLoss: 0.00001 | ValLoss: 1.03205 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15607 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 696 |  TrainLoss: 0.00001 | ValLoss: 1.03165 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15374 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 697 |  TrainLoss: 0.00001 | ValLoss: 1.02863 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.15119 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 698 |  TrainLoss: 0.00001 | ValLoss: 1.02554 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14873 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 699 |  TrainLoss: 0.00003 | ValLoss: 1.02083 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14668 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 700 |  TrainLoss: 0.00001 | ValLoss: 1.01945 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14273 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 701 |  TrainLoss: 0.00000 | ValLoss: 1.01713 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.14062 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 702 |  TrainLoss: 0.00001 | ValLoss: 1.01492 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13879 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 703 |  TrainLoss: 0.00000 | ValLoss: 1.01309 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.13729 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 704 |  TrainLoss: 0.00002 | ValLoss: 1.01031 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.13324 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 705 |  TrainLoss: 0.00001 | ValLoss: 1.00569 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.12844 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 706 |  TrainLoss: 0.00001 | ValLoss: 1.00225 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.12442 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 707 |  TrainLoss: 0.00001 | ValLoss: 0.99921 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.12088 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 708 |  TrainLoss: 0.00000 | ValLoss: 1.00021 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.11947 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 709 |  TrainLoss: 0.00000 | ValLoss: 0.99826 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.11708 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 710 |  TrainLoss: 0.00000 | ValLoss: 0.99666 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11512 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 711 |  TrainLoss: 0.00001 | ValLoss: 0.99509 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11263 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 712 |  TrainLoss: 0.00000 | ValLoss: 0.99347 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.11049 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 713 |  TrainLoss: 0.00001 | ValLoss: 0.99198 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10867 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 714 |  TrainLoss: 0.00000 | ValLoss: 0.99520 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10687 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 715 |  TrainLoss: 0.00001 | ValLoss: 0.99441 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10616 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 716 |  TrainLoss: 0.00001 | ValLoss: 0.99350 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10681 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 717 |  TrainLoss: 0.00001 | ValLoss: 0.99235 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10448 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 718 |  TrainLoss: 0.00001 | ValLoss: 0.99132 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10207 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 719 |  TrainLoss: 0.00000 | ValLoss: 0.99084 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.10050 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 720 |  TrainLoss: 0.00001 | ValLoss: 0.99062 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09938 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 721 |  TrainLoss: 0.00000 | ValLoss: 0.99530 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09825 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 722 |  TrainLoss: 0.00000 | ValLoss: 0.99514 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09699 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 723 |  TrainLoss: 0.00001 | ValLoss: 0.99491 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09599 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 724 |  TrainLoss: 0.00000 | ValLoss: 0.99465 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09730 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 725 |  TrainLoss: 0.00001 | ValLoss: 0.99431 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09514 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 726 |  TrainLoss: 0.00000 | ValLoss: 0.99427 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.09184 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 727 |  TrainLoss: 0.00001 | ValLoss: 1.00054 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.08833 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 728 |  TrainLoss: 0.00000 | ValLoss: 1.00160 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08287 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 729 |  TrainLoss: 0.00000 | ValLoss: 1.00263 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08009 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 730 |  TrainLoss: 0.00000 | ValLoss: 1.00347 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08168 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 731 |  TrainLoss: 0.00000 | ValLoss: 1.00411 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08119 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 732 |  TrainLoss: 0.00001 | ValLoss: 1.00543 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07911 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 733 |  TrainLoss: 0.00001 | ValLoss: 1.01460 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07746 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 734 |  TrainLoss: 0.00001 | ValLoss: 1.01766 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07447 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 735 |  TrainLoss: 0.00001 | ValLoss: 1.01992 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07292 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 736 |  TrainLoss: 0.00000 | ValLoss: 1.02142 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07133 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 737 |  TrainLoss: 0.00001 | ValLoss: 1.02228 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07105 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 738 |  TrainLoss: 0.00001 | ValLoss: 1.02275 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07323 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 739 |  TrainLoss: 0.00000 | ValLoss: 1.02378 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07856 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 740 |  TrainLoss: 0.00000 | ValLoss: 1.02395 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.07936 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 741 |  TrainLoss: 0.00000 | ValLoss: 1.02316 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08100 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 742 |  TrainLoss: 0.00000 | ValLoss: 1.02141 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08249 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 743 |  TrainLoss: 0.00000 | ValLoss: 1.02058 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08376 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 744 |  TrainLoss: 0.00000 | ValLoss: 1.02014 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08479 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 745 |  TrainLoss: 0.00000 | ValLoss: 1.01977 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08586 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 746 |  TrainLoss: 0.00000 | ValLoss: 1.01733 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08572 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 747 |  TrainLoss: 0.00000 | ValLoss: 1.01641 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08845 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 748 |  TrainLoss: 0.00001 | ValLoss: 1.01748 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09219 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 749 |  TrainLoss: 0.00000 | ValLoss: 1.01928 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09005 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 750 |  TrainLoss: 0.00000 | ValLoss: 1.02079 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08963 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 751 |  TrainLoss: 0.00000 | ValLoss: 1.03178 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08778 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 752 |  TrainLoss: 0.00000 | ValLoss: 1.03270 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08834 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 753 |  TrainLoss: 0.00001 | ValLoss: 1.03277 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08879 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 754 |  TrainLoss: 0.00000 | ValLoss: 1.03240 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08975 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 755 |  TrainLoss: 0.00001 | ValLoss: 1.03279 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08860 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 756 |  TrainLoss: 0.00000 | ValLoss: 1.03178 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09037 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 757 |  TrainLoss: 0.00000 | ValLoss: 1.03180 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08997 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 758 |  TrainLoss: 0.00000 | ValLoss: 1.03177 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.08996 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 759 |  TrainLoss: 0.00000 | ValLoss: 1.02982 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09262 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 760 |  TrainLoss: 0.00001 | ValLoss: 1.02582 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.09737 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 761 |  TrainLoss: 0.00001 | ValLoss: 1.02321 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.10466 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 762 |  TrainLoss: 0.00000 | ValLoss: 1.01255 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.12930 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 763 |  TrainLoss: 0.00000 | ValLoss: 1.01864 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.14622 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 764 |  TrainLoss: 0.00002 | ValLoss: 1.03607 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.17760 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 765 |  TrainLoss: 0.00000 | ValLoss: 1.08783 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.22395 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 766 |  TrainLoss: 0.00000 | ValLoss: 1.12073 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.24592 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 767 |  TrainLoss: 0.00000 | ValLoss: 1.11424 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.24299 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 768 |  TrainLoss: 0.00000 | ValLoss: 1.10102 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.23910 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 769 |  TrainLoss: 0.00000 | ValLoss: 1.09323 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.23022 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 770 |  TrainLoss: 0.00000 | ValLoss: 1.07112 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.21463 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 771 |  TrainLoss: 0.00000 | ValLoss: 1.06240 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.20701 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 772 |  TrainLoss: 0.00000 | ValLoss: 1.05063 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19707 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 773 |  TrainLoss: 0.00001 | ValLoss: 1.04356 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.18039 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 774 |  TrainLoss: 0.00001 | ValLoss: 1.02484 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.16118 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 775 |  TrainLoss: 0.00000 | ValLoss: 1.03522 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14045 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 776 |  TrainLoss: 0.00000 | ValLoss: 1.04134 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.13013 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 777 |  TrainLoss: 0.00000 | ValLoss: 1.04815 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.12419 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 778 |  TrainLoss: 0.00000 | ValLoss: 1.05369 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.13010 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 779 |  TrainLoss: 0.00000 | ValLoss: 1.05846 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.12868 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 780 |  TrainLoss: 0.00000 | ValLoss: 1.05798 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.12846 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 781 |  TrainLoss: 0.00000 | ValLoss: 1.05364 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.13165 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 782 |  TrainLoss: 0.00000 | ValLoss: 1.04986 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.13353 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 783 |  TrainLoss: 0.00000 | ValLoss: 1.04419 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.13986 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 784 |  TrainLoss: 0.00000 | ValLoss: 1.04083 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14459 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 785 |  TrainLoss: 0.00000 | ValLoss: 1.03946 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14717 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 786 |  TrainLoss: 0.00000 | ValLoss: 1.03908 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14841 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 787 |  TrainLoss: 0.00000 | ValLoss: 1.03849 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.14979 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 788 |  TrainLoss: 0.00000 | ValLoss: 1.03817 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.15025 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 789 |  TrainLoss: 0.00000 | ValLoss: 1.03435 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.15595 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 790 |  TrainLoss: 0.00000 | ValLoss: 1.03178 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.16661 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 791 |  TrainLoss: 0.00000 | ValLoss: 1.03261 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.17426 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 792 |  TrainLoss: 0.00000 | ValLoss: 1.03331 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.17798 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 793 |  TrainLoss: 0.00000 | ValLoss: 1.03370 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.18033 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 794 |  TrainLoss: 0.00000 | ValLoss: 1.03260 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.17838 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 795 |  TrainLoss: 0.00000 | ValLoss: 1.03140 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.17528 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 796 |  TrainLoss: 0.00001 | ValLoss: 1.04591 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.20051 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 797 |  TrainLoss: 0.00000 | ValLoss: 1.06754 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.19428 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 798 |  TrainLoss: 0.00000 | ValLoss: 1.06962 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.19095 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 799 |  TrainLoss: 0.00000 | ValLoss: 1.07315 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.18429 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 800 |  TrainLoss: 0.00000 | ValLoss: 1.07613 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.18556 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 801 |  TrainLoss: 0.00000 | ValLoss: 1.07767 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.18774 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 802 |  TrainLoss: 0.00000 | ValLoss: 1.08064 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.18307 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 803 |  TrainLoss: 0.00000 | ValLoss: 1.09136 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.17164 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 804 |  TrainLoss: 0.00000 | ValLoss: 1.09870 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.16488 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 805 |  TrainLoss: 0.00000 | ValLoss: 1.10062 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.16838 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 806 |  TrainLoss: 0.00000 | ValLoss: 1.10232 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.16774 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 807 |  TrainLoss: 0.00000 | ValLoss: 1.09947 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.17104 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 808 |  TrainLoss: 0.00000 | ValLoss: 1.09839 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.17129 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 809 |  TrainLoss: 0.00000 | ValLoss: 1.09584 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 1.17428 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 810 |  TrainLoss: 0.00000 | ValLoss: 1.08912 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.23857 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 811 |  TrainLoss: 0.00001 | ValLoss: 1.17868 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.34626 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 812 |  TrainLoss: 0.00000 | ValLoss: 1.24136 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.38076 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 813 |  TrainLoss: 0.00000 | ValLoss: 1.22052 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.36940 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 814 |  TrainLoss: 0.00000 | ValLoss: 1.12983 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.29937 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 815 |  TrainLoss: 0.00000 | ValLoss: 1.08696 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 1.23946 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 816 |  TrainLoss: 0.00000 | ValLoss: 1.09245 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.56149 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 817 |  TrainLoss: 0.00000 | ValLoss: 3.81701 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.55044 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 818 |  TrainLoss: 0.00001 | ValLoss: 3.90171 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.48440 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 819 |  TrainLoss: 0.00000 | ValLoss: 3.94415 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.44245 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 820 |  TrainLoss: 0.00000 | ValLoss: 3.92149 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.46410 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 821 |  TrainLoss: 0.00000 | ValLoss: 3.85818 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.51942 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 822 |  TrainLoss: 0.00000 | ValLoss: 1.11655 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.64096 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 823 |  TrainLoss: 0.00000 | ValLoss: 1.31024 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.42131 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 824 |  TrainLoss: 0.00000 | ValLoss: 1.38060 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.46232 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 825 |  TrainLoss: 0.00000 | ValLoss: 1.35286 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.45381 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 826 |  TrainLoss: 0.00000 | ValLoss: 1.30856 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.43489 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 827 |  TrainLoss: 0.00000 | ValLoss: 1.26449 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 1.41519 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 828 |  TrainLoss: 0.00000 | ValLoss: 1.19049 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.73036 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 829 |  TrainLoss: 0.00000 | ValLoss: 3.83225 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.66967 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 830 |  TrainLoss: 0.00000 | ValLoss: 3.81358 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.62269 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 831 |  TrainLoss: 0.00000 | ValLoss: 3.82894 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.60101 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 832 |  TrainLoss: 0.00000 | ValLoss: 3.85055 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.58291 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 833 |  TrainLoss: 0.00000 | ValLoss: 3.84327 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.58657 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 834 |  TrainLoss: 0.00000 | ValLoss: 3.82481 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.60816 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 835 |  TrainLoss: 0.00000 | ValLoss: 3.81805 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.61758 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 836 |  TrainLoss: 0.00000 | ValLoss: 3.81363 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.62654 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 837 |  TrainLoss: 0.00000 | ValLoss: 3.81163 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.63275 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 838 |  TrainLoss: 0.00000 | ValLoss: 3.81083 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.63823 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 839 |  TrainLoss: 0.00000 | ValLoss: 3.81098 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.63698 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 840 |  TrainLoss: 0.00000 | ValLoss: 3.81094 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.64641 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 841 |  TrainLoss: 0.00000 | ValLoss: 3.82047 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.66551 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 842 |  TrainLoss: 0.00000 | ValLoss: 3.82645 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.67644 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 843 |  TrainLoss: 0.00000 | ValLoss: 3.81571 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.65041 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 844 |  TrainLoss: 0.00000 | ValLoss: 3.82364 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.63106 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 845 |  TrainLoss: 0.00000 | ValLoss: 3.81768 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.64543 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 846 |  TrainLoss: 0.00000 | ValLoss: 3.81666 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.66145 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 847 |  TrainLoss: 0.00000 | ValLoss: 3.81776 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.66928 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 848 |  TrainLoss: 0.00000 | ValLoss: 3.81960 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.67653 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 849 |  TrainLoss: 0.00000 | ValLoss: 3.84161 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.72262 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 850 |  TrainLoss: 0.00000 | ValLoss: 3.82982 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.69750 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 851 |  TrainLoss: 0.00000 | ValLoss: 3.84093 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.65886 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 852 |  TrainLoss: 0.00000 | ValLoss: 3.86024 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.63919 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 853 |  TrainLoss: 0.00000 | ValLoss: 3.85153 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.65229 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 854 |  TrainLoss: 0.00000 | ValLoss: 3.83699 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.68978 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 855 |  TrainLoss: 0.00000 | ValLoss: 3.83823 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.71396 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 856 |  TrainLoss: 0.00000 | ValLoss: 3.84329 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.66865 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 857 |  TrainLoss: 0.00000 | ValLoss: 3.84409 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.67053 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 858 |  TrainLoss: 0.00000 | ValLoss: 3.83832 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.68447 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 859 |  TrainLoss: 0.00000 | ValLoss: 3.83636 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.69196 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 860 |  TrainLoss: 0.00000 | ValLoss: 3.83651 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.70517 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 861 |  TrainLoss: 0.00000 | ValLoss: 3.86840 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.76347 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 862 |  TrainLoss: 0.00000 | ValLoss: 3.88681 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.78119 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 863 |  TrainLoss: 0.00000 | ValLoss: 3.83400 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 2.70994 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 864 |  TrainLoss: 0.00000 | ValLoss: 3.94776 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 3.94212 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 865 |  TrainLoss: 0.00001 | ValLoss: 3.95029 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.82840 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 866 |  TrainLoss: 0.00000 | ValLoss: 1.79113 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 3.01732 | TestAcc: 0.85484 | TestF1: 0.82\n",
            "Epoch: 867 |  TrainLoss: 0.00018 | ValLoss: 3.90392 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 3.86063 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 868 |  TrainLoss: 0.00001 | ValLoss: 6.93048 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 5.19173 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 869 |  TrainLoss: 0.00209 | ValLoss: 3.81878 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 3.85750 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 870 |  TrainLoss: 0.00006 | ValLoss: 2.17047 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 1.85735 | TestAcc: 0.83871 | TestF1: 0.78\n",
            "Epoch: 871 |  TrainLoss: 0.00237 | ValLoss: 4.12200 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.76142 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 872 |  TrainLoss: 0.00000 | ValLoss: 3.88975 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 3.85826 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 873 |  TrainLoss: 0.00000 | ValLoss: 4.08475 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 3.74060 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 874 |  TrainLoss: 0.00006 | ValLoss: 6.89512 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 5.09704 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 875 |  TrainLoss: 0.00003 | ValLoss: 6.91402 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 5.13086 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 876 |  TrainLoss: 0.00070 | ValLoss: 4.10211 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 5.09836 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 877 |  TrainLoss: 0.00001 | ValLoss: 3.95589 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 3.82852 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 878 |  TrainLoss: 0.00000 | ValLoss: 3.90510 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 3.87960 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 879 |  TrainLoss: 0.00000 | ValLoss: 3.87404 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 2.56520 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 880 |  TrainLoss: 0.00000 | ValLoss: 3.86002 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.58917 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 881 |  TrainLoss: 0.00000 | ValLoss: 3.85472 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60493 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 882 |  TrainLoss: 0.00000 | ValLoss: 3.85323 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61615 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 883 |  TrainLoss: 0.00000 | ValLoss: 3.85315 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62277 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 884 |  TrainLoss: 0.00000 | ValLoss: 3.85328 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62641 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 885 |  TrainLoss: 0.00000 | ValLoss: 3.85357 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62964 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 886 |  TrainLoss: 0.00000 | ValLoss: 3.85368 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63167 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 887 |  TrainLoss: 0.00000 | ValLoss: 3.85387 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63297 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 888 |  TrainLoss: 0.00000 | ValLoss: 3.85397 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63378 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 889 |  TrainLoss: 0.00000 | ValLoss: 3.85400 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63289 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 890 |  TrainLoss: 0.00000 | ValLoss: 3.85398 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63321 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 891 |  TrainLoss: 0.00000 | ValLoss: 3.85404 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63340 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 892 |  TrainLoss: 0.00000 | ValLoss: 3.85408 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63346 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 893 |  TrainLoss: 0.00000 | ValLoss: 3.85397 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63332 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 894 |  TrainLoss: 0.00000 | ValLoss: 3.85406 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63337 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 895 |  TrainLoss: 0.00000 | ValLoss: 3.85414 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63351 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 896 |  TrainLoss: 0.00000 | ValLoss: 3.85415 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63358 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 897 |  TrainLoss: 0.00000 | ValLoss: 3.85415 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63345 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 898 |  TrainLoss: 0.00000 | ValLoss: 3.85414 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63320 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 899 |  TrainLoss: 0.00000 | ValLoss: 3.85412 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63306 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 900 |  TrainLoss: 0.00000 | ValLoss: 3.85406 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63299 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 901 |  TrainLoss: 0.00000 | ValLoss: 3.85402 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63294 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 902 |  TrainLoss: 0.00000 | ValLoss: 3.85407 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63288 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 903 |  TrainLoss: 0.00000 | ValLoss: 3.85401 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63421 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 904 |  TrainLoss: 0.00000 | ValLoss: 3.85404 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63414 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 905 |  TrainLoss: 0.00000 | ValLoss: 3.85403 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63401 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 906 |  TrainLoss: 0.00000 | ValLoss: 3.85400 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63386 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 907 |  TrainLoss: 0.00000 | ValLoss: 3.85395 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63381 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 908 |  TrainLoss: 0.00000 | ValLoss: 3.85391 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63305 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 909 |  TrainLoss: 0.00000 | ValLoss: 3.85374 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63141 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 910 |  TrainLoss: 0.00000 | ValLoss: 3.85369 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63031 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 911 |  TrainLoss: 0.00001 | ValLoss: 3.85351 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62907 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 912 |  TrainLoss: 0.00000 | ValLoss: 3.85340 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62672 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 913 |  TrainLoss: 0.00000 | ValLoss: 3.85335 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62678 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 914 |  TrainLoss: 0.00000 | ValLoss: 3.85331 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62585 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 915 |  TrainLoss: 0.00000 | ValLoss: 3.85330 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62524 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 916 |  TrainLoss: 0.00000 | ValLoss: 3.85328 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62459 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 917 |  TrainLoss: 0.00000 | ValLoss: 3.85332 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62399 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 918 |  TrainLoss: 0.00000 | ValLoss: 3.85327 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62344 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 919 |  TrainLoss: 0.00000 | ValLoss: 3.85333 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62301 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 920 |  TrainLoss: 0.00000 | ValLoss: 3.85336 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62270 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 921 |  TrainLoss: 0.00000 | ValLoss: 3.85342 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62241 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 922 |  TrainLoss: 0.00000 | ValLoss: 3.85340 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62219 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 923 |  TrainLoss: 0.00000 | ValLoss: 3.85345 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62206 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 924 |  TrainLoss: 0.00000 | ValLoss: 3.85339 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62228 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 925 |  TrainLoss: 0.00000 | ValLoss: 3.85349 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62282 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 926 |  TrainLoss: 0.00000 | ValLoss: 3.85351 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62311 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 927 |  TrainLoss: 0.00000 | ValLoss: 3.85356 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62330 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 928 |  TrainLoss: 0.00000 | ValLoss: 3.85355 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62228 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 929 |  TrainLoss: 0.00000 | ValLoss: 3.85362 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62097 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 930 |  TrainLoss: 0.00000 | ValLoss: 3.85368 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62012 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 931 |  TrainLoss: 0.00000 | ValLoss: 3.85368 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61955 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 932 |  TrainLoss: 0.00000 | ValLoss: 3.85367 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62087 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 933 |  TrainLoss: 0.00000 | ValLoss: 3.85369 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62051 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 934 |  TrainLoss: 0.00000 | ValLoss: 3.85377 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61939 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 935 |  TrainLoss: 0.00000 | ValLoss: 3.85399 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61751 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 936 |  TrainLoss: 0.00000 | ValLoss: 3.85424 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61647 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 937 |  TrainLoss: 0.00000 | ValLoss: 3.85419 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61614 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 938 |  TrainLoss: 0.00000 | ValLoss: 3.85434 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61588 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 939 |  TrainLoss: 0.00000 | ValLoss: 3.85429 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61580 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 940 |  TrainLoss: 0.00000 | ValLoss: 3.85441 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61573 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 941 |  TrainLoss: 0.00000 | ValLoss: 3.85443 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61588 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 942 |  TrainLoss: 0.00000 | ValLoss: 3.85445 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61584 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 943 |  TrainLoss: 0.00000 | ValLoss: 3.85447 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61583 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 944 |  TrainLoss: 0.00000 | ValLoss: 3.85426 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61750 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 945 |  TrainLoss: 0.00000 | ValLoss: 3.85404 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62025 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 946 |  TrainLoss: 0.00000 | ValLoss: 3.85393 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62170 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 947 |  TrainLoss: 0.00000 | ValLoss: 3.85396 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62292 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 948 |  TrainLoss: 0.00000 | ValLoss: 3.85392 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62382 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 949 |  TrainLoss: 0.00000 | ValLoss: 3.85404 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62440 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 950 |  TrainLoss: 0.00000 | ValLoss: 3.85404 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62420 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 951 |  TrainLoss: 0.00000 | ValLoss: 3.85380 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62241 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 952 |  TrainLoss: 0.00000 | ValLoss: 3.85379 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61997 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 953 |  TrainLoss: 0.00000 | ValLoss: 3.85368 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61832 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 954 |  TrainLoss: 0.00000 | ValLoss: 3.85383 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61742 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 955 |  TrainLoss: 0.00000 | ValLoss: 3.85390 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61683 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 956 |  TrainLoss: 0.00000 | ValLoss: 3.85401 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61597 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 957 |  TrainLoss: 0.00000 | ValLoss: 3.85410 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61547 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 958 |  TrainLoss: 0.00000 | ValLoss: 3.85409 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61596 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 959 |  TrainLoss: 0.00000 | ValLoss: 3.85410 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61682 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 960 |  TrainLoss: 0.00000 | ValLoss: 3.85420 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61716 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 961 |  TrainLoss: 0.00000 | ValLoss: 3.85428 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61494 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 962 |  TrainLoss: 0.00000 | ValLoss: 3.85447 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61456 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 963 |  TrainLoss: 0.00000 | ValLoss: 3.85460 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61261 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 964 |  TrainLoss: 0.00000 | ValLoss: 3.85491 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61134 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 965 |  TrainLoss: 0.00000 | ValLoss: 3.85580 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60719 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 966 |  TrainLoss: 0.00000 | ValLoss: 3.85662 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60728 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 967 |  TrainLoss: 0.00000 | ValLoss: 3.85691 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60616 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 968 |  TrainLoss: 0.00000 | ValLoss: 3.85737 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60525 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 969 |  TrainLoss: 0.00000 | ValLoss: 3.85751 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60464 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 970 |  TrainLoss: 0.00000 | ValLoss: 3.85779 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60425 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 971 |  TrainLoss: 0.00000 | ValLoss: 3.85771 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60413 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 972 |  TrainLoss: 0.00000 | ValLoss: 3.85799 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60327 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 973 |  TrainLoss: 0.00000 | ValLoss: 3.85866 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.59959 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 974 |  TrainLoss: 0.00000 | ValLoss: 3.85921 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.59983 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 975 |  TrainLoss: 0.00000 | ValLoss: 3.85922 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60084 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 976 |  TrainLoss: 0.00000 | ValLoss: 3.85868 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60258 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 977 |  TrainLoss: 0.00000 | ValLoss: 3.85838 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60517 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 978 |  TrainLoss: 0.00000 | ValLoss: 3.85812 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60649 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 979 |  TrainLoss: 0.00000 | ValLoss: 3.85818 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60769 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 980 |  TrainLoss: 0.00000 | ValLoss: 3.85814 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60833 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 981 |  TrainLoss: 0.00000 | ValLoss: 3.85783 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61000 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 982 |  TrainLoss: 0.00000 | ValLoss: 3.85757 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61130 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 983 |  TrainLoss: 0.00000 | ValLoss: 3.85756 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61265 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 984 |  TrainLoss: 0.00000 | ValLoss: 3.85781 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61300 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 985 |  TrainLoss: 0.00000 | ValLoss: 3.85690 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62046 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 986 |  TrainLoss: 0.00000 | ValLoss: 3.85685 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62767 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 987 |  TrainLoss: 0.00000 | ValLoss: 3.85719 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63137 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 988 |  TrainLoss: 0.00000 | ValLoss: 3.85753 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63432 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 989 |  TrainLoss: 0.00000 | ValLoss: 3.85781 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63591 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 990 |  TrainLoss: 0.00000 | ValLoss: 3.85795 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63691 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 991 |  TrainLoss: 0.00000 | ValLoss: 3.85812 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63723 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 992 |  TrainLoss: 0.00000 | ValLoss: 3.85790 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63602 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 993 |  TrainLoss: 0.00000 | ValLoss: 3.85777 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.63408 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 994 |  TrainLoss: 0.00000 | ValLoss: 3.85751 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62996 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 995 |  TrainLoss: 0.00000 | ValLoss: 3.85738 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62316 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 996 |  TrainLoss: 0.00000 | ValLoss: 3.85720 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.62036 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 997 |  TrainLoss: 0.00000 | ValLoss: 3.85737 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.61633 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 998 |  TrainLoss: 0.00000 | ValLoss: 3.86000 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.60650 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 999 |  TrainLoss: 0.00000 | ValLoss: 3.86570 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 2.59522 | TestAcc: 0.90323 | TestF1: 0.88\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1000):\n",
        "  train_loss = train(epoch)\n",
        "  test_loss, test_acc, test_f1 = test(epoch)\n",
        "  val_loss, val_acc, val_f1 = val(epoch)\n",
        "\n",
        "  print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "          f'ValLoss: {val_loss:.5f} | ValAcc: {val_acc:.5f} | ValF1: {val_f1:.2f} | '\n",
        "          f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}'\n",
        "                    )\n",
        "  # print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.7f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Metrics found after training \n",
        "**Epoch: 114 |  TrainLoss: 0.15898 | ValLoss: 0.31973 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21842 | TestAcc: 0.93548 | TestF1: 0.93**\n",
        "\n",
        "*Note : We choose the best epoch based on lowest Validation Loss.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Effects pf HyperParamter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('best_model_pol.json') as f:\n",
        "    best_param = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'params': {'layer_size1': 512,\n",
              "  'layer_size2': 512,\n",
              "  'layer_size3': 256,\n",
              "  'layer_size4': 64,\n",
              "  'layer_size5': 512,\n",
              "  'layer_size6': 64,\n",
              "  'learning_rate': 1.433675124157315e-05,\n",
              "  'b1': 0.9286759768614723}}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_trial={}\n",
        "best_trial['params'] = best_param\n",
        "best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Studying Effect of learning  rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------Training for Learning rate 1e-05---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69347 | ValLoss: 0.69220 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69397 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69327 | ValLoss: 0.69208 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69395 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69271 | ValLoss: 0.69197 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69395 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69294 | ValLoss: 0.69189 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69394 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69288 | ValLoss: 0.69179 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69395 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69285 | ValLoss: 0.69168 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69399 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69259 | ValLoss: 0.69160 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69400 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69290 | ValLoss: 0.69156 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69399 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69256 | ValLoss: 0.69147 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69396 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69225 | ValLoss: 0.69135 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69398 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69296 | ValLoss: 0.69125 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69398 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69158 | ValLoss: 0.69116 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69398 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69201 | ValLoss: 0.69103 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69402 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69222 | ValLoss: 0.69095 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69402 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69266 | ValLoss: 0.69088 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69402 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.69263 | ValLoss: 0.69078 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69402 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.69162 | ValLoss: 0.69071 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69392 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.69142 | ValLoss: 0.69058 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69392 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.69116 | ValLoss: 0.69046 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69382 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.69207 | ValLoss: 0.69026 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69381 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.69124 | ValLoss: 0.69008 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69379 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.69172 | ValLoss: 0.68997 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69372 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.69086 | ValLoss: 0.68972 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69373 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.69119 | ValLoss: 0.68964 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69347 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.69110 | ValLoss: 0.68939 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69340 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.69119 | ValLoss: 0.68909 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69336 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.69014 | ValLoss: 0.68882 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69326 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.68979 | ValLoss: 0.68849 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69319 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 28 |  TrainLoss: 0.69129 | ValLoss: 0.68826 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69295 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 29 |  TrainLoss: 0.69026 | ValLoss: 0.68804 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69272 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 30 |  TrainLoss: 0.68950 | ValLoss: 0.68784 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69245 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 31 |  TrainLoss: 0.68950 | ValLoss: 0.68745 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69237 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 32 |  TrainLoss: 0.69021 | ValLoss: 0.68700 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69241 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 33 |  TrainLoss: 0.68853 | ValLoss: 0.68678 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69208 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 34 |  TrainLoss: 0.68922 | ValLoss: 0.68636 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69191 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 35 |  TrainLoss: 0.68819 | ValLoss: 0.68607 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69154 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 36 |  TrainLoss: 0.68841 | ValLoss: 0.68578 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69106 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 37 |  TrainLoss: 0.68806 | ValLoss: 0.68523 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69081 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 38 |  TrainLoss: 0.68705 | ValLoss: 0.68473 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69047 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 39 |  TrainLoss: 0.68572 | ValLoss: 0.68386 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69041 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 40 |  TrainLoss: 0.68573 | ValLoss: 0.68330 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68996 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 41 |  TrainLoss: 0.68571 | ValLoss: 0.68301 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68918 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 42 |  TrainLoss: 0.68400 | ValLoss: 0.68229 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68883 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 43 |  TrainLoss: 0.68485 | ValLoss: 0.68190 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68805 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 44 |  TrainLoss: 0.68267 | ValLoss: 0.68120 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68754 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 45 |  TrainLoss: 0.68331 | ValLoss: 0.68029 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68710 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 46 |  TrainLoss: 0.68177 | ValLoss: 0.67955 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68636 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 47 |  TrainLoss: 0.68276 | ValLoss: 0.67868 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68567 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 48 |  TrainLoss: 0.67994 | ValLoss: 0.67818 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68450 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 49 |  TrainLoss: 0.68090 | ValLoss: 0.67730 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68369 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 50 |  TrainLoss: 0.67859 | ValLoss: 0.67635 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68289 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 51 |  TrainLoss: 0.67774 | ValLoss: 0.67505 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68230 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 52 |  TrainLoss: 0.67852 | ValLoss: 0.67410 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68127 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 53 |  TrainLoss: 0.67562 | ValLoss: 0.67291 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.68043 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 54 |  TrainLoss: 0.67473 | ValLoss: 0.67168 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.67941 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 55 |  TrainLoss: 0.67187 | ValLoss: 0.67035 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.67825 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 56 |  TrainLoss: 0.67231 | ValLoss: 0.66905 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.67684 | TestAcc: 0.59677 | TestF1: 0.68\n",
            "Epoch: 57 |  TrainLoss: 0.67113 | ValLoss: 0.66740 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.67579 | TestAcc: 0.59677 | TestF1: 0.68\n",
            "Epoch: 58 |  TrainLoss: 0.67001 | ValLoss: 0.66609 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.67398 | TestAcc: 0.61290 | TestF1: 0.68\n",
            "Epoch: 59 |  TrainLoss: 0.66626 | ValLoss: 0.66477 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67202 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 60 |  TrainLoss: 0.66478 | ValLoss: 0.66311 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67025 | TestAcc: 0.69355 | TestF1: 0.72\n",
            "Epoch: 61 |  TrainLoss: 0.66567 | ValLoss: 0.66117 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66862 | TestAcc: 0.69355 | TestF1: 0.72\n",
            "Epoch: 62 |  TrainLoss: 0.66401 | ValLoss: 0.65944 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66657 | TestAcc: 0.69355 | TestF1: 0.72\n",
            "Epoch: 63 |  TrainLoss: 0.66254 | ValLoss: 0.65783 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.66427 | TestAcc: 0.70968 | TestF1: 0.74\n",
            "Epoch: 64 |  TrainLoss: 0.65959 | ValLoss: 0.65652 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.66136 | TestAcc: 0.72581 | TestF1: 0.74\n",
            "Epoch: 65 |  TrainLoss: 0.65490 | ValLoss: 0.65484 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.65865 | TestAcc: 0.75806 | TestF1: 0.76\n",
            "Epoch: 66 |  TrainLoss: 0.65583 | ValLoss: 0.65157 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.65749 | TestAcc: 0.70968 | TestF1: 0.74\n",
            "Epoch: 67 |  TrainLoss: 0.65216 | ValLoss: 0.64904 | ValAcc: 0.74194 | ValF1: 0.80 | TestLoss: 0.65550 | TestAcc: 0.70968 | TestF1: 0.74\n",
            "Epoch: 68 |  TrainLoss: 0.64906 | ValLoss: 0.64657 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.65273 | TestAcc: 0.74194 | TestF1: 0.76\n",
            "Epoch: 69 |  TrainLoss: 0.64623 | ValLoss: 0.64424 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.64948 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 70 |  TrainLoss: 0.64302 | ValLoss: 0.64234 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.64566 | TestAcc: 0.75806 | TestF1: 0.76\n",
            "Epoch: 71 |  TrainLoss: 0.64004 | ValLoss: 0.63994 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.64213 | TestAcc: 0.79032 | TestF1: 0.79\n",
            "Epoch: 72 |  TrainLoss: 0.63756 | ValLoss: 0.63612 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.63997 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 73 |  TrainLoss: 0.63446 | ValLoss: 0.63269 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.63735 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 74 |  TrainLoss: 0.63210 | ValLoss: 0.62915 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.63489 | TestAcc: 0.75806 | TestF1: 0.77\n",
            "Epoch: 75 |  TrainLoss: 0.62967 | ValLoss: 0.62551 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.63219 | TestAcc: 0.74194 | TestF1: 0.76\n",
            "Epoch: 76 |  TrainLoss: 0.62433 | ValLoss: 0.62390 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.62537 | TestAcc: 0.82258 | TestF1: 0.81\n",
            "Epoch: 77 |  TrainLoss: 0.61589 | ValLoss: 0.62143 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.62014 | TestAcc: 0.82258 | TestF1: 0.81\n",
            "Epoch: 78 |  TrainLoss: 0.61839 | ValLoss: 0.61677 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.61664 | TestAcc: 0.82258 | TestF1: 0.81\n",
            "Epoch: 79 |  TrainLoss: 0.61625 | ValLoss: 0.61192 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.61363 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 80 |  TrainLoss: 0.60746 | ValLoss: 0.60837 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.60849 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 81 |  TrainLoss: 0.60116 | ValLoss: 0.60386 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.60410 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 82 |  TrainLoss: 0.59636 | ValLoss: 0.60008 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.59812 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 83 |  TrainLoss: 0.60051 | ValLoss: 0.59590 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.59256 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 84 |  TrainLoss: 0.59232 | ValLoss: 0.59213 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.58656 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 85 |  TrainLoss: 0.58281 | ValLoss: 0.58712 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.58136 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 86 |  TrainLoss: 0.58097 | ValLoss: 0.58349 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.57441 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 87 |  TrainLoss: 0.57517 | ValLoss: 0.57680 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.57008 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 88 |  TrainLoss: 0.56836 | ValLoss: 0.57262 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.56323 | TestAcc: 0.85484 | TestF1: 0.84\n",
            "Epoch: 89 |  TrainLoss: 0.56031 | ValLoss: 0.56818 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.55628 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 90 |  TrainLoss: 0.55648 | ValLoss: 0.56238 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.55021 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 91 |  TrainLoss: 0.55006 | ValLoss: 0.55612 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.54433 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 92 |  TrainLoss: 0.53194 | ValLoss: 0.55103 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.53677 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 93 |  TrainLoss: 0.54390 | ValLoss: 0.54648 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.52821 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 94 |  TrainLoss: 0.53024 | ValLoss: 0.54190 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.51969 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 95 |  TrainLoss: 0.51783 | ValLoss: 0.53481 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.51284 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 96 |  TrainLoss: 0.51265 | ValLoss: 0.52757 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.50663 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 97 |  TrainLoss: 0.51369 | ValLoss: 0.52049 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.50158 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 98 |  TrainLoss: 0.49599 | ValLoss: 0.51633 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.49095 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 99 |  TrainLoss: 0.50150 | ValLoss: 0.51309 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.48107 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 100 |  TrainLoss: 0.49231 | ValLoss: 0.50927 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.47252 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 101 |  TrainLoss: 0.48199 | ValLoss: 0.49973 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.46738 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 102 |  TrainLoss: 0.49356 | ValLoss: 0.49332 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.46116 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 103 |  TrainLoss: 0.46933 | ValLoss: 0.49158 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.45023 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 104 |  TrainLoss: 0.45289 | ValLoss: 0.48313 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.44517 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 105 |  TrainLoss: 0.45576 | ValLoss: 0.47737 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.43757 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 106 |  TrainLoss: 0.45256 | ValLoss: 0.47629 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.42689 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 107 |  TrainLoss: 0.43892 | ValLoss: 0.46834 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.42085 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 108 |  TrainLoss: 0.44217 | ValLoss: 0.46116 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41602 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 109 |  TrainLoss: 0.43579 | ValLoss: 0.45546 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.40948 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 110 |  TrainLoss: 0.41531 | ValLoss: 0.45602 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.39751 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 111 |  TrainLoss: 0.43104 | ValLoss: 0.45442 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.38994 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 112 |  TrainLoss: 0.39876 | ValLoss: 0.44481 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.38480 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 113 |  TrainLoss: 0.39701 | ValLoss: 0.43874 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37907 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 114 |  TrainLoss: 0.40693 | ValLoss: 0.43333 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37340 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 115 |  TrainLoss: 0.39355 | ValLoss: 0.42840 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36817 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 116 |  TrainLoss: 0.37180 | ValLoss: 0.42434 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36150 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 117 |  TrainLoss: 0.38971 | ValLoss: 0.42169 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35364 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 118 |  TrainLoss: 0.38678 | ValLoss: 0.41617 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35132 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 119 |  TrainLoss: 0.36570 | ValLoss: 0.41204 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34690 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 120 |  TrainLoss: 0.35865 | ValLoss: 0.41047 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33735 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 121 |  TrainLoss: 0.35693 | ValLoss: 0.40809 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33088 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 122 |  TrainLoss: 0.34673 | ValLoss: 0.40662 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32455 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 123 |  TrainLoss: 0.33931 | ValLoss: 0.40164 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31996 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 124 |  TrainLoss: 0.34791 | ValLoss: 0.39634 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31745 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 125 |  TrainLoss: 0.33095 | ValLoss: 0.39323 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31170 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 126 |  TrainLoss: 0.34612 | ValLoss: 0.39119 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30561 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 127 |  TrainLoss: 0.33616 | ValLoss: 0.38885 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30091 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 128 |  TrainLoss: 0.31204 | ValLoss: 0.38540 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29758 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 129 |  TrainLoss: 0.32781 | ValLoss: 0.38259 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29351 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 130 |  TrainLoss: 0.31516 | ValLoss: 0.37984 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28971 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 131 |  TrainLoss: 0.31516 | ValLoss: 0.37761 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28610 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 132 |  TrainLoss: 0.31676 | ValLoss: 0.37643 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28217 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 133 |  TrainLoss: 0.30548 | ValLoss: 0.37303 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28030 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 134 |  TrainLoss: 0.29891 | ValLoss: 0.37045 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27917 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 135 |  TrainLoss: 0.29167 | ValLoss: 0.36869 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27463 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 136 |  TrainLoss: 0.29933 | ValLoss: 0.36682 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27184 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 137 |  TrainLoss: 0.28810 | ValLoss: 0.36495 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27096 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 138 |  TrainLoss: 0.28868 | ValLoss: 0.36474 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26433 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 139 |  TrainLoss: 0.30290 | ValLoss: 0.36510 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.26112 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 140 |  TrainLoss: 0.28475 | ValLoss: 0.35997 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26321 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 141 |  TrainLoss: 0.28513 | ValLoss: 0.35857 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26399 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 142 |  TrainLoss: 0.26426 | ValLoss: 0.35673 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25870 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 143 |  TrainLoss: 0.26006 | ValLoss: 0.35556 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25517 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 144 |  TrainLoss: 0.26419 | ValLoss: 0.35365 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25334 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 145 |  TrainLoss: 0.26834 | ValLoss: 0.35236 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24990 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 146 |  TrainLoss: 0.26437 | ValLoss: 0.35047 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25034 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 147 |  TrainLoss: 0.25298 | ValLoss: 0.35071 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25606 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 148 |  TrainLoss: 0.26007 | ValLoss: 0.34770 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24566 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 149 |  TrainLoss: 0.25218 | ValLoss: 0.35143 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.24210 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 150 |  TrainLoss: 0.24764 | ValLoss: 0.34565 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24417 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 151 |  TrainLoss: 0.25261 | ValLoss: 0.34480 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24376 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 152 |  TrainLoss: 0.22257 | ValLoss: 0.34399 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24353 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 153 |  TrainLoss: 0.23758 | ValLoss: 0.34222 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24010 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 154 |  TrainLoss: 0.24992 | ValLoss: 0.34083 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23737 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 155 |  TrainLoss: 0.23505 | ValLoss: 0.34031 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23351 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.23190 | ValLoss: 0.33882 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23445 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 157 |  TrainLoss: 0.23612 | ValLoss: 0.33816 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23669 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 158 |  TrainLoss: 0.20678 | ValLoss: 0.33751 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23743 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 159 |  TrainLoss: 0.21180 | ValLoss: 0.33583 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23266 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 160 |  TrainLoss: 0.21951 | ValLoss: 0.33523 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22787 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 161 |  TrainLoss: 0.23137 | ValLoss: 0.33488 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23120 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 162 |  TrainLoss: 0.20314 | ValLoss: 0.33382 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22882 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 163 |  TrainLoss: 0.21016 | ValLoss: 0.33278 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22836 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 164 |  TrainLoss: 0.21528 | ValLoss: 0.33154 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22492 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 165 |  TrainLoss: 0.19613 | ValLoss: 0.33089 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22357 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 166 |  TrainLoss: 0.21368 | ValLoss: 0.32949 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22330 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 167 |  TrainLoss: 0.22695 | ValLoss: 0.32844 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22300 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 168 |  TrainLoss: 0.19078 | ValLoss: 0.32792 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22447 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 169 |  TrainLoss: 0.21150 | ValLoss: 0.32915 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22896 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 170 |  TrainLoss: 0.19957 | ValLoss: 0.32635 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22233 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 171 |  TrainLoss: 0.18875 | ValLoss: 0.32623 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22343 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 172 |  TrainLoss: 0.18365 | ValLoss: 0.32656 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22497 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 173 |  TrainLoss: 0.18416 | ValLoss: 0.32602 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22337 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 174 |  TrainLoss: 0.19718 | ValLoss: 0.32505 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21914 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 175 |  TrainLoss: 0.18630 | ValLoss: 0.32551 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21881 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 176 |  TrainLoss: 0.18627 | ValLoss: 0.32492 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22423 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 177 |  TrainLoss: 0.18435 | ValLoss: 0.32521 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22561 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 178 |  TrainLoss: 0.18295 | ValLoss: 0.32463 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22430 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 179 |  TrainLoss: 0.17303 | ValLoss: 0.32159 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21799 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 180 |  TrainLoss: 0.15727 | ValLoss: 0.32093 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21696 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 181 |  TrainLoss: 0.17675 | ValLoss: 0.32062 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21902 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 182 |  TrainLoss: 0.17809 | ValLoss: 0.32417 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22667 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 183 |  TrainLoss: 0.18528 | ValLoss: 0.32040 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21981 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 184 |  TrainLoss: 0.18173 | ValLoss: 0.31934 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21575 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 185 |  TrainLoss: 0.17279 | ValLoss: 0.31888 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21618 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 186 |  TrainLoss: 0.16502 | ValLoss: 0.31917 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21798 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 187 |  TrainLoss: 0.15979 | ValLoss: 0.32166 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22156 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 188 |  TrainLoss: 0.16220 | ValLoss: 0.31864 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21739 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 189 |  TrainLoss: 0.16867 | ValLoss: 0.31772 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21586 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 190 |  TrainLoss: 0.15351 | ValLoss: 0.31744 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21650 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 191 |  TrainLoss: 0.16142 | ValLoss: 0.32051 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22136 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 192 |  TrainLoss: 0.15545 | ValLoss: 0.32253 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22402 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 193 |  TrainLoss: 0.15072 | ValLoss: 0.31938 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.21886 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 194 |  TrainLoss: 0.15577 | ValLoss: 0.32070 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.21982 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 195 |  TrainLoss: 0.15245 | ValLoss: 0.31864 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21688 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 196 |  TrainLoss: 0.14323 | ValLoss: 0.31936 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21747 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 197 |  TrainLoss: 0.13992 | ValLoss: 0.32299 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22235 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 198 |  TrainLoss: 0.14470 | ValLoss: 0.31977 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21772 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 199 |  TrainLoss: 0.14961 | ValLoss: 0.32009 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21793 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 200 |  TrainLoss: 0.14361 | ValLoss: 0.32073 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21858 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 201 |  TrainLoss: 0.13412 | ValLoss: 0.32459 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22334 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 202 |  TrainLoss: 0.13152 | ValLoss: 0.32659 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22557 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 203 |  TrainLoss: 0.12637 | ValLoss: 0.32176 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21869 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 204 |  TrainLoss: 0.12580 | ValLoss: 0.32211 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21939 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 205 |  TrainLoss: 0.12753 | ValLoss: 0.32623 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22458 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 206 |  TrainLoss: 0.12909 | ValLoss: 0.33063 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22922 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 207 |  TrainLoss: 0.12762 | ValLoss: 0.32570 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22217 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 208 |  TrainLoss: 0.12594 | ValLoss: 0.32597 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22161 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 209 |  TrainLoss: 0.13795 | ValLoss: 0.32660 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22240 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 210 |  TrainLoss: 0.12514 | ValLoss: 0.33202 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22835 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 211 |  TrainLoss: 0.11755 | ValLoss: 0.33681 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23409 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 212 |  TrainLoss: 0.14052 | ValLoss: 0.33072 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22614 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 213 |  TrainLoss: 0.11874 | ValLoss: 0.33029 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22546 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 214 |  TrainLoss: 0.11166 | ValLoss: 0.33130 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22629 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 215 |  TrainLoss: 0.11509 | ValLoss: 0.34009 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23449 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 216 |  TrainLoss: 0.12410 | ValLoss: 0.33885 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23300 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 217 |  TrainLoss: 0.11892 | ValLoss: 0.33385 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22926 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 218 |  TrainLoss: 0.11844 | ValLoss: 0.33605 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23088 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 219 |  TrainLoss: 0.12892 | ValLoss: 0.34287 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23688 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 220 |  TrainLoss: 0.10753 | ValLoss: 0.33773 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23242 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 221 |  TrainLoss: 0.11300 | ValLoss: 0.33857 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23339 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 222 |  TrainLoss: 0.10494 | ValLoss: 0.34264 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23653 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 223 |  TrainLoss: 0.10604 | ValLoss: 0.34258 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23600 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 224 |  TrainLoss: 0.09049 | ValLoss: 0.34403 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23630 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 225 |  TrainLoss: 0.10253 | ValLoss: 0.34923 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23931 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 226 |  TrainLoss: 0.09885 | ValLoss: 0.34972 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23899 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 227 |  TrainLoss: 0.09661 | ValLoss: 0.35288 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24012 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 228 |  TrainLoss: 0.10489 | ValLoss: 0.35736 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24223 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 229 |  TrainLoss: 0.09794 | ValLoss: 0.35585 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24093 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 230 |  TrainLoss: 0.09575 | ValLoss: 0.35461 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24092 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 231 |  TrainLoss: 0.10467 | ValLoss: 0.35688 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24199 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 232 |  TrainLoss: 0.09819 | ValLoss: 0.36107 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24406 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 233 |  TrainLoss: 0.09550 | ValLoss: 0.36331 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24542 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 234 |  TrainLoss: 0.09551 | ValLoss: 0.36175 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24605 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 235 |  TrainLoss: 0.09321 | ValLoss: 0.36323 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24815 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 236 |  TrainLoss: 0.09264 | ValLoss: 0.36524 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24893 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 237 |  TrainLoss: 0.08827 | ValLoss: 0.36898 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25054 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 238 |  TrainLoss: 0.07776 | ValLoss: 0.36908 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25178 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 239 |  TrainLoss: 0.08835 | ValLoss: 0.37309 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25372 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 240 |  TrainLoss: 0.09122 | ValLoss: 0.37436 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25550 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 241 |  TrainLoss: 0.08631 | ValLoss: 0.37528 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25851 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 242 |  TrainLoss: 0.08122 | ValLoss: 0.37708 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25914 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 243 |  TrainLoss: 0.07200 | ValLoss: 0.38124 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26000 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 244 |  TrainLoss: 0.08409 | ValLoss: 0.38349 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26175 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 245 |  TrainLoss: 0.08191 | ValLoss: 0.38377 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26251 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 246 |  TrainLoss: 0.08489 | ValLoss: 0.38478 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26424 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 247 |  TrainLoss: 0.08320 | ValLoss: 0.38595 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26568 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 248 |  TrainLoss: 0.07349 | ValLoss: 0.38732 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26700 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 249 |  TrainLoss: 0.07898 | ValLoss: 0.38801 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26923 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "------Training for Learning rate 1e-06---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69289 | ValLoss: 0.69321 | ValAcc: 0.54839 | ValF1: 0.71 | TestLoss: 0.69344 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69307 | ValLoss: 0.69320 | ValAcc: 0.54839 | ValF1: 0.71 | TestLoss: 0.69344 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69349 | ValLoss: 0.69318 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69344 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69354 | ValLoss: 0.69316 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69346 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69346 | ValLoss: 0.69313 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69347 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69346 | ValLoss: 0.69311 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69347 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69345 | ValLoss: 0.69309 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69348 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69360 | ValLoss: 0.69307 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69349 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69397 | ValLoss: 0.69304 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69350 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69292 | ValLoss: 0.69302 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69350 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69401 | ValLoss: 0.69300 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69351 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69290 | ValLoss: 0.69298 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69352 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69413 | ValLoss: 0.69296 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69352 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69282 | ValLoss: 0.69294 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69352 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69393 | ValLoss: 0.69292 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69353 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.69396 | ValLoss: 0.69290 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69353 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.69269 | ValLoss: 0.69288 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69354 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.69317 | ValLoss: 0.69287 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69354 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.69298 | ValLoss: 0.69286 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69355 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.69390 | ValLoss: 0.69284 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69355 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.69264 | ValLoss: 0.69283 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69356 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.69277 | ValLoss: 0.69281 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69356 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.69325 | ValLoss: 0.69278 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69358 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.69301 | ValLoss: 0.69277 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69358 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.69256 | ValLoss: 0.69275 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69358 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.69277 | ValLoss: 0.69273 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69359 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.69276 | ValLoss: 0.69272 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69359 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.69355 | ValLoss: 0.69270 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69359 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 28 |  TrainLoss: 0.69296 | ValLoss: 0.69269 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 29 |  TrainLoss: 0.69344 | ValLoss: 0.69268 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 30 |  TrainLoss: 0.69283 | ValLoss: 0.69266 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 31 |  TrainLoss: 0.69331 | ValLoss: 0.69265 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 32 |  TrainLoss: 0.69284 | ValLoss: 0.69264 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 33 |  TrainLoss: 0.69278 | ValLoss: 0.69263 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69359 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 34 |  TrainLoss: 0.69330 | ValLoss: 0.69262 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 35 |  TrainLoss: 0.69391 | ValLoss: 0.69260 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 36 |  TrainLoss: 0.69310 | ValLoss: 0.69258 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 37 |  TrainLoss: 0.69288 | ValLoss: 0.69257 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 38 |  TrainLoss: 0.69257 | ValLoss: 0.69256 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 39 |  TrainLoss: 0.69298 | ValLoss: 0.69254 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 40 |  TrainLoss: 0.69298 | ValLoss: 0.69253 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 41 |  TrainLoss: 0.69253 | ValLoss: 0.69252 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 42 |  TrainLoss: 0.69232 | ValLoss: 0.69251 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 43 |  TrainLoss: 0.69308 | ValLoss: 0.69250 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 44 |  TrainLoss: 0.69321 | ValLoss: 0.69248 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 45 |  TrainLoss: 0.69395 | ValLoss: 0.69247 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 46 |  TrainLoss: 0.69253 | ValLoss: 0.69245 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 47 |  TrainLoss: 0.69228 | ValLoss: 0.69243 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 48 |  TrainLoss: 0.69348 | ValLoss: 0.69241 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 49 |  TrainLoss: 0.69283 | ValLoss: 0.69239 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 50 |  TrainLoss: 0.69267 | ValLoss: 0.69238 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 51 |  TrainLoss: 0.69335 | ValLoss: 0.69237 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 52 |  TrainLoss: 0.69258 | ValLoss: 0.69236 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 53 |  TrainLoss: 0.69264 | ValLoss: 0.69234 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 54 |  TrainLoss: 0.69218 | ValLoss: 0.69232 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 55 |  TrainLoss: 0.69295 | ValLoss: 0.69231 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 56 |  TrainLoss: 0.69243 | ValLoss: 0.69229 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 57 |  TrainLoss: 0.69245 | ValLoss: 0.69227 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 58 |  TrainLoss: 0.69326 | ValLoss: 0.69225 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 59 |  TrainLoss: 0.69273 | ValLoss: 0.69223 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69363 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 60 |  TrainLoss: 0.69236 | ValLoss: 0.69221 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69363 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 61 |  TrainLoss: 0.69213 | ValLoss: 0.69219 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69364 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 62 |  TrainLoss: 0.69300 | ValLoss: 0.69217 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69364 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 63 |  TrainLoss: 0.69251 | ValLoss: 0.69214 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 64 |  TrainLoss: 0.69313 | ValLoss: 0.69213 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 65 |  TrainLoss: 0.69327 | ValLoss: 0.69210 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 66 |  TrainLoss: 0.69291 | ValLoss: 0.69209 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 67 |  TrainLoss: 0.69203 | ValLoss: 0.69206 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 68 |  TrainLoss: 0.69199 | ValLoss: 0.69204 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 69 |  TrainLoss: 0.69287 | ValLoss: 0.69203 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 70 |  TrainLoss: 0.69202 | ValLoss: 0.69201 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 71 |  TrainLoss: 0.69207 | ValLoss: 0.69200 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 72 |  TrainLoss: 0.69311 | ValLoss: 0.69199 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 73 |  TrainLoss: 0.69298 | ValLoss: 0.69196 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 74 |  TrainLoss: 0.69218 | ValLoss: 0.69194 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 75 |  TrainLoss: 0.69217 | ValLoss: 0.69192 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 76 |  TrainLoss: 0.69219 | ValLoss: 0.69190 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 77 |  TrainLoss: 0.69267 | ValLoss: 0.69188 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 78 |  TrainLoss: 0.69294 | ValLoss: 0.69185 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 79 |  TrainLoss: 0.69187 | ValLoss: 0.69183 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 80 |  TrainLoss: 0.69205 | ValLoss: 0.69181 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 81 |  TrainLoss: 0.69286 | ValLoss: 0.69179 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 82 |  TrainLoss: 0.69174 | ValLoss: 0.69176 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 83 |  TrainLoss: 0.69193 | ValLoss: 0.69173 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 84 |  TrainLoss: 0.69241 | ValLoss: 0.69171 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 85 |  TrainLoss: 0.69203 | ValLoss: 0.69168 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 86 |  TrainLoss: 0.69177 | ValLoss: 0.69166 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 87 |  TrainLoss: 0.69301 | ValLoss: 0.69162 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 88 |  TrainLoss: 0.69207 | ValLoss: 0.69159 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 89 |  TrainLoss: 0.69214 | ValLoss: 0.69157 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 90 |  TrainLoss: 0.69280 | ValLoss: 0.69155 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 91 |  TrainLoss: 0.69234 | ValLoss: 0.69152 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 92 |  TrainLoss: 0.69302 | ValLoss: 0.69149 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 93 |  TrainLoss: 0.69177 | ValLoss: 0.69147 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 94 |  TrainLoss: 0.69183 | ValLoss: 0.69144 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 95 |  TrainLoss: 0.69271 | ValLoss: 0.69141 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 96 |  TrainLoss: 0.69222 | ValLoss: 0.69139 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 97 |  TrainLoss: 0.69183 | ValLoss: 0.69135 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 98 |  TrainLoss: 0.69122 | ValLoss: 0.69131 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 99 |  TrainLoss: 0.69306 | ValLoss: 0.69128 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 100 |  TrainLoss: 0.69127 | ValLoss: 0.69126 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69367 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 101 |  TrainLoss: 0.69243 | ValLoss: 0.69124 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69366 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 102 |  TrainLoss: 0.69163 | ValLoss: 0.69122 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 103 |  TrainLoss: 0.69156 | ValLoss: 0.69120 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 104 |  TrainLoss: 0.69133 | ValLoss: 0.69118 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 105 |  TrainLoss: 0.69227 | ValLoss: 0.69115 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69364 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 106 |  TrainLoss: 0.69196 | ValLoss: 0.69113 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69364 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 107 |  TrainLoss: 0.69134 | ValLoss: 0.69110 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69363 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 108 |  TrainLoss: 0.69208 | ValLoss: 0.69108 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 109 |  TrainLoss: 0.69217 | ValLoss: 0.69105 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 110 |  TrainLoss: 0.69157 | ValLoss: 0.69101 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69363 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 111 |  TrainLoss: 0.69208 | ValLoss: 0.69098 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69363 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 112 |  TrainLoss: 0.69212 | ValLoss: 0.69096 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69362 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 113 |  TrainLoss: 0.69181 | ValLoss: 0.69094 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 114 |  TrainLoss: 0.69058 | ValLoss: 0.69091 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69361 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 115 |  TrainLoss: 0.69121 | ValLoss: 0.69087 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 116 |  TrainLoss: 0.69234 | ValLoss: 0.69085 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69360 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 117 |  TrainLoss: 0.69192 | ValLoss: 0.69083 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69359 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 118 |  TrainLoss: 0.69148 | ValLoss: 0.69082 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69357 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 119 |  TrainLoss: 0.69118 | ValLoss: 0.69079 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69357 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 120 |  TrainLoss: 0.69144 | ValLoss: 0.69076 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69358 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 121 |  TrainLoss: 0.69206 | ValLoss: 0.69073 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69357 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 122 |  TrainLoss: 0.69054 | ValLoss: 0.69069 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69358 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 123 |  TrainLoss: 0.69283 | ValLoss: 0.69065 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69357 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 124 |  TrainLoss: 0.69233 | ValLoss: 0.69063 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69356 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 125 |  TrainLoss: 0.69238 | ValLoss: 0.69062 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69354 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 126 |  TrainLoss: 0.69073 | ValLoss: 0.69060 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69353 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 127 |  TrainLoss: 0.69251 | ValLoss: 0.69057 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69353 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 128 |  TrainLoss: 0.69172 | ValLoss: 0.69054 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69352 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 129 |  TrainLoss: 0.69090 | ValLoss: 0.69052 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69351 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 130 |  TrainLoss: 0.69102 | ValLoss: 0.69049 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69350 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 131 |  TrainLoss: 0.69215 | ValLoss: 0.69047 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69349 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 132 |  TrainLoss: 0.69128 | ValLoss: 0.69045 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69348 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 133 |  TrainLoss: 0.69164 | ValLoss: 0.69044 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69347 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 134 |  TrainLoss: 0.69229 | ValLoss: 0.69042 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69345 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 135 |  TrainLoss: 0.69182 | ValLoss: 0.69040 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69344 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 136 |  TrainLoss: 0.69203 | ValLoss: 0.69036 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69343 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 137 |  TrainLoss: 0.69086 | ValLoss: 0.69034 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69342 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 138 |  TrainLoss: 0.69159 | ValLoss: 0.69031 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69340 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 139 |  TrainLoss: 0.69175 | ValLoss: 0.69027 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69340 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 140 |  TrainLoss: 0.69141 | ValLoss: 0.69023 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69339 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 141 |  TrainLoss: 0.69041 | ValLoss: 0.69020 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69338 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 142 |  TrainLoss: 0.69135 | ValLoss: 0.69017 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69337 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 143 |  TrainLoss: 0.69127 | ValLoss: 0.69014 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69336 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 144 |  TrainLoss: 0.69077 | ValLoss: 0.69010 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69336 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 145 |  TrainLoss: 0.69195 | ValLoss: 0.69007 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69335 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 146 |  TrainLoss: 0.69198 | ValLoss: 0.69005 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69333 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 147 |  TrainLoss: 0.69073 | ValLoss: 0.69003 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69331 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 148 |  TrainLoss: 0.69167 | ValLoss: 0.69001 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69330 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 149 |  TrainLoss: 0.69189 | ValLoss: 0.68998 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69329 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 150 |  TrainLoss: 0.69070 | ValLoss: 0.68995 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69328 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 151 |  TrainLoss: 0.69123 | ValLoss: 0.68992 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69327 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 152 |  TrainLoss: 0.69170 | ValLoss: 0.68989 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69326 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 153 |  TrainLoss: 0.69119 | ValLoss: 0.68987 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69322 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 154 |  TrainLoss: 0.69096 | ValLoss: 0.68983 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69322 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 155 |  TrainLoss: 0.69100 | ValLoss: 0.68980 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69321 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 156 |  TrainLoss: 0.69112 | ValLoss: 0.68977 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69321 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 157 |  TrainLoss: 0.69093 | ValLoss: 0.68973 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69320 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 158 |  TrainLoss: 0.69144 | ValLoss: 0.68970 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69320 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 159 |  TrainLoss: 0.69136 | ValLoss: 0.68966 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69320 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 160 |  TrainLoss: 0.69103 | ValLoss: 0.68964 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69318 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 161 |  TrainLoss: 0.69133 | ValLoss: 0.68962 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69316 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 162 |  TrainLoss: 0.69107 | ValLoss: 0.68959 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69315 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 163 |  TrainLoss: 0.69071 | ValLoss: 0.68956 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69313 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 164 |  TrainLoss: 0.69162 | ValLoss: 0.68953 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69312 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 165 |  TrainLoss: 0.69046 | ValLoss: 0.68950 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69310 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 166 |  TrainLoss: 0.69053 | ValLoss: 0.68946 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69310 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 167 |  TrainLoss: 0.69062 | ValLoss: 0.68942 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69309 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 168 |  TrainLoss: 0.69079 | ValLoss: 0.68941 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69306 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 169 |  TrainLoss: 0.69119 | ValLoss: 0.68939 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69303 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 170 |  TrainLoss: 0.69011 | ValLoss: 0.68935 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69302 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 171 |  TrainLoss: 0.69080 | ValLoss: 0.68932 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69300 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 172 |  TrainLoss: 0.69077 | ValLoss: 0.68930 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69298 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 173 |  TrainLoss: 0.69030 | ValLoss: 0.68927 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69296 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 174 |  TrainLoss: 0.69103 | ValLoss: 0.68924 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69294 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 175 |  TrainLoss: 0.69058 | ValLoss: 0.68922 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69292 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 176 |  TrainLoss: 0.69039 | ValLoss: 0.68918 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69292 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 177 |  TrainLoss: 0.69046 | ValLoss: 0.68915 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69291 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 178 |  TrainLoss: 0.69000 | ValLoss: 0.68910 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69291 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 179 |  TrainLoss: 0.69001 | ValLoss: 0.68907 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69290 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 180 |  TrainLoss: 0.68981 | ValLoss: 0.68904 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69288 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 181 |  TrainLoss: 0.68900 | ValLoss: 0.68902 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69286 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 182 |  TrainLoss: 0.69043 | ValLoss: 0.68897 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69286 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 183 |  TrainLoss: 0.69056 | ValLoss: 0.68893 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69285 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 184 |  TrainLoss: 0.68991 | ValLoss: 0.68892 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69282 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 185 |  TrainLoss: 0.69010 | ValLoss: 0.68889 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69280 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 186 |  TrainLoss: 0.69040 | ValLoss: 0.68885 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69280 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 187 |  TrainLoss: 0.69025 | ValLoss: 0.68881 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69279 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 188 |  TrainLoss: 0.69051 | ValLoss: 0.68879 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69276 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 189 |  TrainLoss: 0.69003 | ValLoss: 0.68876 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69274 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 190 |  TrainLoss: 0.68941 | ValLoss: 0.68873 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69272 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 191 |  TrainLoss: 0.69016 | ValLoss: 0.68868 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69271 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 192 |  TrainLoss: 0.68999 | ValLoss: 0.68866 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69269 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 193 |  TrainLoss: 0.68967 | ValLoss: 0.68863 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69267 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 194 |  TrainLoss: 0.68951 | ValLoss: 0.68860 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69265 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 195 |  TrainLoss: 0.69001 | ValLoss: 0.68857 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69263 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 196 |  TrainLoss: 0.69001 | ValLoss: 0.68854 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69262 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 197 |  TrainLoss: 0.69033 | ValLoss: 0.68852 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69259 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 198 |  TrainLoss: 0.68982 | ValLoss: 0.68850 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69256 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 199 |  TrainLoss: 0.68926 | ValLoss: 0.68849 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69252 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 200 |  TrainLoss: 0.68913 | ValLoss: 0.68845 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69251 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 201 |  TrainLoss: 0.69033 | ValLoss: 0.68842 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69250 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 202 |  TrainLoss: 0.69091 | ValLoss: 0.68840 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69247 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 203 |  TrainLoss: 0.69053 | ValLoss: 0.68839 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69243 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 204 |  TrainLoss: 0.68987 | ValLoss: 0.68837 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69240 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 205 |  TrainLoss: 0.69118 | ValLoss: 0.68835 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69238 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 206 |  TrainLoss: 0.69080 | ValLoss: 0.68833 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69236 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 207 |  TrainLoss: 0.68919 | ValLoss: 0.68831 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69234 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 208 |  TrainLoss: 0.68876 | ValLoss: 0.68827 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69233 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 209 |  TrainLoss: 0.68938 | ValLoss: 0.68822 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69233 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 210 |  TrainLoss: 0.69030 | ValLoss: 0.68819 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69231 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 211 |  TrainLoss: 0.68923 | ValLoss: 0.68816 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69229 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 212 |  TrainLoss: 0.69047 | ValLoss: 0.68814 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69226 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 213 |  TrainLoss: 0.68854 | ValLoss: 0.68813 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69222 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 214 |  TrainLoss: 0.68870 | ValLoss: 0.68809 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69221 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 215 |  TrainLoss: 0.68938 | ValLoss: 0.68804 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69220 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 216 |  TrainLoss: 0.68978 | ValLoss: 0.68801 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69218 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 217 |  TrainLoss: 0.68891 | ValLoss: 0.68797 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69217 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 218 |  TrainLoss: 0.69010 | ValLoss: 0.68794 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69215 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 219 |  TrainLoss: 0.68997 | ValLoss: 0.68791 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69212 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 220 |  TrainLoss: 0.68966 | ValLoss: 0.68789 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69209 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 221 |  TrainLoss: 0.69031 | ValLoss: 0.68785 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69208 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 222 |  TrainLoss: 0.69003 | ValLoss: 0.68783 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69206 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 223 |  TrainLoss: 0.68887 | ValLoss: 0.68781 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69203 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 224 |  TrainLoss: 0.68878 | ValLoss: 0.68777 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69201 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 225 |  TrainLoss: 0.68898 | ValLoss: 0.68775 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69198 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 226 |  TrainLoss: 0.68888 | ValLoss: 0.68773 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69195 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 227 |  TrainLoss: 0.68843 | ValLoss: 0.68771 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69192 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 228 |  TrainLoss: 0.68993 | ValLoss: 0.68766 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69192 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 229 |  TrainLoss: 0.68942 | ValLoss: 0.68764 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69189 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 230 |  TrainLoss: 0.68908 | ValLoss: 0.68760 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69187 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 231 |  TrainLoss: 0.68865 | ValLoss: 0.68757 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69185 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 232 |  TrainLoss: 0.68888 | ValLoss: 0.68753 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69184 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 233 |  TrainLoss: 0.68905 | ValLoss: 0.68750 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69182 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 234 |  TrainLoss: 0.68869 | ValLoss: 0.68748 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69179 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 235 |  TrainLoss: 0.68865 | ValLoss: 0.68745 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69176 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 236 |  TrainLoss: 0.68903 | ValLoss: 0.68742 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69174 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 237 |  TrainLoss: 0.68917 | ValLoss: 0.68738 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69172 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 238 |  TrainLoss: 0.68833 | ValLoss: 0.68736 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69168 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 239 |  TrainLoss: 0.68898 | ValLoss: 0.68734 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69164 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 240 |  TrainLoss: 0.68902 | ValLoss: 0.68730 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69163 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 241 |  TrainLoss: 0.68807 | ValLoss: 0.68726 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69161 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 242 |  TrainLoss: 0.68936 | ValLoss: 0.68721 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69160 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 243 |  TrainLoss: 0.68828 | ValLoss: 0.68716 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69158 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 244 |  TrainLoss: 0.68878 | ValLoss: 0.68714 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69155 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 245 |  TrainLoss: 0.68741 | ValLoss: 0.68713 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69150 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 246 |  TrainLoss: 0.69012 | ValLoss: 0.68708 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69149 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 247 |  TrainLoss: 0.68878 | ValLoss: 0.68705 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69146 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 248 |  TrainLoss: 0.68801 | ValLoss: 0.68703 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69142 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 249 |  TrainLoss: 0.68831 | ValLoss: 0.68698 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69141 | TestAcc: 0.51613 | TestF1: 0.63\n"
          ]
        }
      ],
      "source": [
        "lrs = [1e-3,1e-4,1e-5,1e-6]\n",
        "lrs_data={}\n",
        "# lrs = [1e-5,1e-6]\n",
        "\n",
        "for i in lrs:\n",
        "    print(\"------Training for Learning rate {}---------\".format(i))\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net(test_data_pol.num_features,[best_trial['params']['layer_size1'],best_trial['params']['layer_size2'],best_trial['params']['layer_size3'],\n",
        "                                            best_trial['params']['layer_size4'],best_trial['params']['layer_size5'],best_trial['params']['layer_size6']],1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=i,betas=(best_trial['params']['b1'],0.99))\n",
        "    lossff = torch.nn.BCELoss()\n",
        "    print(device)\n",
        "\n",
        "    print(\"Number of Parameters of the model :\",sum([param.nelement() for param in model.parameters()]))\n",
        "    print(model)\n",
        "    lrs_data[i] = {\"val_loss\":[],\"test_loss\":[],\"train_loss\":[]}\n",
        "    for epoch in range(250):\n",
        "        train_loss = train(epoch)\n",
        "        test_loss, test_acc, test_f1 = test(epoch)\n",
        "        val_loss, val_acc, val_f1 = val(epoch)\n",
        "        lrs_data[i]['val_loss'].append(val_loss)\n",
        "        lrs_data[i]['test_loss'].append(test_loss)\n",
        "        lrs_data[i]['train_loss'].append(train_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "                f'ValLoss: {val_loss:.5f} | ValAcc: {val_acc:.5f} | ValF1: {val_f1:.2f} | '\n",
        "                f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}'\n",
        "                            )\n",
        "        # print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.7f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('learning_rate.json','w') as f:\n",
        "    json.dump(lrs_data,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# summarize effect of lr vs loss\n",
        "#f,ax = plt.subplots(1,2,figsize=(16,12))\n",
        "import matplotlib.pyplot as plt\n",
        "for i in lrs_data.values():\n",
        "    plt.plot(i['val_loss'])\n",
        "\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(list(lrs_data.keys()), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "lrs= list(lrs_data.keys())\n",
        "lrs.remove(str(0.001))\n",
        "for i in lrs:\n",
        "    plt.plot(lrs_data[i]['val_loss'])\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(lrs, loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Studying Effects of beta/momentum of Adam Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------Training for beta 0.9494745743057798---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69593 | ValLoss: 0.70271 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68588 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 01 |  TrainLoss: 0.69547 | ValLoss: 0.70215 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68611 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 02 |  TrainLoss: 0.69518 | ValLoss: 0.70161 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68630 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 03 |  TrainLoss: 0.69546 | ValLoss: 0.70115 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68647 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 04 |  TrainLoss: 0.69427 | ValLoss: 0.70071 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68664 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 05 |  TrainLoss: 0.69499 | ValLoss: 0.70032 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68677 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 06 |  TrainLoss: 0.69472 | ValLoss: 0.69991 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68692 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 07 |  TrainLoss: 0.69467 | ValLoss: 0.69954 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68700 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 08 |  TrainLoss: 0.69405 | ValLoss: 0.69915 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68709 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 09 |  TrainLoss: 0.69473 | ValLoss: 0.69884 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68711 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 10 |  TrainLoss: 0.69390 | ValLoss: 0.69852 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68715 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 11 |  TrainLoss: 0.69309 | ValLoss: 0.69829 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68718 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 12 |  TrainLoss: 0.69410 | ValLoss: 0.69800 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68723 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 13 |  TrainLoss: 0.69405 | ValLoss: 0.69776 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68717 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 14 |  TrainLoss: 0.69331 | ValLoss: 0.69757 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68705 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 15 |  TrainLoss: 0.69323 | ValLoss: 0.69724 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68697 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 16 |  TrainLoss: 0.69269 | ValLoss: 0.69696 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68683 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 17 |  TrainLoss: 0.69315 | ValLoss: 0.69667 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68671 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 18 |  TrainLoss: 0.69284 | ValLoss: 0.69634 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68662 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 19 |  TrainLoss: 0.69137 | ValLoss: 0.69597 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68659 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 20 |  TrainLoss: 0.69199 | ValLoss: 0.69572 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68636 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 21 |  TrainLoss: 0.69190 | ValLoss: 0.69549 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68610 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 22 |  TrainLoss: 0.69064 | ValLoss: 0.69513 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68582 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 23 |  TrainLoss: 0.69088 | ValLoss: 0.69455 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68562 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 24 |  TrainLoss: 0.68973 | ValLoss: 0.69388 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68544 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 25 |  TrainLoss: 0.68969 | ValLoss: 0.69320 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68518 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 26 |  TrainLoss: 0.68850 | ValLoss: 0.69241 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68494 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 27 |  TrainLoss: 0.68860 | ValLoss: 0.69180 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68445 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 28 |  TrainLoss: 0.68667 | ValLoss: 0.69079 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68427 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 29 |  TrainLoss: 0.68735 | ValLoss: 0.68983 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.68388 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 30 |  TrainLoss: 0.68621 | ValLoss: 0.68885 | ValAcc: 0.45161 | ValF1: 0.11 | TestLoss: 0.68329 | TestAcc: 0.62903 | TestF1: 0.21\n",
            "Epoch: 31 |  TrainLoss: 0.68538 | ValLoss: 0.68795 | ValAcc: 0.48387 | ValF1: 0.20 | TestLoss: 0.68243 | TestAcc: 0.64516 | TestF1: 0.27\n",
            "Epoch: 32 |  TrainLoss: 0.68395 | ValLoss: 0.68702 | ValAcc: 0.54839 | ValF1: 0.36 | TestLoss: 0.68149 | TestAcc: 0.64516 | TestF1: 0.27\n",
            "Epoch: 33 |  TrainLoss: 0.68400 | ValLoss: 0.68653 | ValAcc: 0.54839 | ValF1: 0.36 | TestLoss: 0.68018 | TestAcc: 0.64516 | TestF1: 0.27\n",
            "Epoch: 34 |  TrainLoss: 0.68380 | ValLoss: 0.68489 | ValAcc: 0.64516 | ValF1: 0.59 | TestLoss: 0.67964 | TestAcc: 0.77419 | TestF1: 0.63\n",
            "Epoch: 35 |  TrainLoss: 0.68296 | ValLoss: 0.68357 | ValAcc: 0.70968 | ValF1: 0.69 | TestLoss: 0.67875 | TestAcc: 0.85484 | TestF1: 0.79\n",
            "Epoch: 36 |  TrainLoss: 0.68032 | ValLoss: 0.68218 | ValAcc: 0.77419 | ValF1: 0.77 | TestLoss: 0.67761 | TestAcc: 0.87097 | TestF1: 0.82\n",
            "Epoch: 37 |  TrainLoss: 0.68187 | ValLoss: 0.68057 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.67649 | TestAcc: 0.88710 | TestF1: 0.84\n",
            "Epoch: 38 |  TrainLoss: 0.67961 | ValLoss: 0.67928 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.67473 | TestAcc: 0.88710 | TestF1: 0.84\n",
            "Epoch: 39 |  TrainLoss: 0.67561 | ValLoss: 0.67773 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.67291 | TestAcc: 0.88710 | TestF1: 0.84\n",
            "Epoch: 40 |  TrainLoss: 0.67469 | ValLoss: 0.67586 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.67125 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 41 |  TrainLoss: 0.67175 | ValLoss: 0.67454 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.66840 | TestAcc: 0.88710 | TestF1: 0.84\n",
            "Epoch: 42 |  TrainLoss: 0.67089 | ValLoss: 0.67220 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.66609 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 43 |  TrainLoss: 0.66794 | ValLoss: 0.66986 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.66327 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 44 |  TrainLoss: 0.66564 | ValLoss: 0.66661 | ValAcc: 0.74194 | ValF1: 0.76 | TestLoss: 0.66086 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 45 |  TrainLoss: 0.66106 | ValLoss: 0.66360 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.65762 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 46 |  TrainLoss: 0.65849 | ValLoss: 0.66051 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.65406 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 47 |  TrainLoss: 0.65356 | ValLoss: 0.65691 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.65081 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 48 |  TrainLoss: 0.65187 | ValLoss: 0.65328 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.64696 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 49 |  TrainLoss: 0.64637 | ValLoss: 0.64889 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.64341 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 50 |  TrainLoss: 0.64551 | ValLoss: 0.64616 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.63755 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 51 |  TrainLoss: 0.63730 | ValLoss: 0.64181 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.63272 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 52 |  TrainLoss: 0.63615 | ValLoss: 0.63680 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.62805 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 53 |  TrainLoss: 0.62783 | ValLoss: 0.63181 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.62262 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 54 |  TrainLoss: 0.62873 | ValLoss: 0.62702 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.61651 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 55 |  TrainLoss: 0.61911 | ValLoss: 0.62250 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.60962 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 56 |  TrainLoss: 0.61252 | ValLoss: 0.61645 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.60363 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 57 |  TrainLoss: 0.60969 | ValLoss: 0.61006 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.59764 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 58 |  TrainLoss: 0.60144 | ValLoss: 0.60404 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.59031 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 59 |  TrainLoss: 0.58944 | ValLoss: 0.59756 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.58259 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 60 |  TrainLoss: 0.59005 | ValLoss: 0.59095 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.57458 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 61 |  TrainLoss: 0.57936 | ValLoss: 0.58491 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.56540 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 62 |  TrainLoss: 0.56650 | ValLoss: 0.57847 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.55626 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 63 |  TrainLoss: 0.56716 | ValLoss: 0.56862 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.55007 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 64 |  TrainLoss: 0.55618 | ValLoss: 0.56186 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.54011 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 65 |  TrainLoss: 0.55222 | ValLoss: 0.55752 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.52747 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 66 |  TrainLoss: 0.54002 | ValLoss: 0.54895 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.51821 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 67 |  TrainLoss: 0.52173 | ValLoss: 0.54088 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.50799 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 68 |  TrainLoss: 0.52690 | ValLoss: 0.53031 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.50012 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 69 |  TrainLoss: 0.51145 | ValLoss: 0.52047 | ValAcc: 0.77419 | ValF1: 0.81 | TestLoss: 0.49220 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 70 |  TrainLoss: 0.50593 | ValLoss: 0.51639 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.47652 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 71 |  TrainLoss: 0.48747 | ValLoss: 0.51007 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.46426 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 72 |  TrainLoss: 0.48309 | ValLoss: 0.49650 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.45805 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 73 |  TrainLoss: 0.47177 | ValLoss: 0.48772 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.44827 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 74 |  TrainLoss: 0.47164 | ValLoss: 0.47902 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.43900 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 75 |  TrainLoss: 0.45407 | ValLoss: 0.48250 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.41943 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 76 |  TrainLoss: 0.44571 | ValLoss: 0.46689 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41421 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 77 |  TrainLoss: 0.44945 | ValLoss: 0.45673 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41011 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 78 |  TrainLoss: 0.42205 | ValLoss: 0.45069 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.39824 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 79 |  TrainLoss: 0.41813 | ValLoss: 0.44984 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.38185 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 80 |  TrainLoss: 0.41738 | ValLoss: 0.44161 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.37347 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 81 |  TrainLoss: 0.41238 | ValLoss: 0.43142 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36907 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 82 |  TrainLoss: 0.38811 | ValLoss: 0.42424 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36427 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 83 |  TrainLoss: 0.38532 | ValLoss: 0.41847 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35596 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 84 |  TrainLoss: 0.36551 | ValLoss: 0.41353 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34399 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 85 |  TrainLoss: 0.37964 | ValLoss: 0.41414 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.32947 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 86 |  TrainLoss: 0.35120 | ValLoss: 0.40859 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.32176 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 87 |  TrainLoss: 0.35367 | ValLoss: 0.39871 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32102 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 88 |  TrainLoss: 0.34824 | ValLoss: 0.39370 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32064 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 89 |  TrainLoss: 0.34381 | ValLoss: 0.38977 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30584 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 90 |  TrainLoss: 0.33585 | ValLoss: 0.38890 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.29417 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 91 |  TrainLoss: 0.30742 | ValLoss: 0.38538 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.28733 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 92 |  TrainLoss: 0.31201 | ValLoss: 0.37734 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28580 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 93 |  TrainLoss: 0.29633 | ValLoss: 0.37289 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28305 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 94 |  TrainLoss: 0.29895 | ValLoss: 0.37088 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27272 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 95 |  TrainLoss: 0.31345 | ValLoss: 0.37455 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.26384 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 96 |  TrainLoss: 0.27945 | ValLoss: 0.36415 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26243 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 97 |  TrainLoss: 0.29573 | ValLoss: 0.36012 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26591 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 98 |  TrainLoss: 0.28056 | ValLoss: 0.35730 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26077 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 99 |  TrainLoss: 0.29258 | ValLoss: 0.35490 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25133 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 100 |  TrainLoss: 0.29544 | ValLoss: 0.35253 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24747 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 101 |  TrainLoss: 0.26348 | ValLoss: 0.34987 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24755 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 102 |  TrainLoss: 0.25774 | ValLoss: 0.34974 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23888 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 103 |  TrainLoss: 0.25683 | ValLoss: 0.34693 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23710 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 104 |  TrainLoss: 0.26152 | ValLoss: 0.34530 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23452 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 105 |  TrainLoss: 0.24710 | ValLoss: 0.34272 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23631 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 106 |  TrainLoss: 0.23760 | ValLoss: 0.34135 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23552 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 107 |  TrainLoss: 0.23554 | ValLoss: 0.33969 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23161 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 108 |  TrainLoss: 0.22220 | ValLoss: 0.33764 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22649 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 109 |  TrainLoss: 0.23792 | ValLoss: 0.33936 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21946 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 110 |  TrainLoss: 0.20897 | ValLoss: 0.33355 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22083 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 111 |  TrainLoss: 0.22419 | ValLoss: 0.33400 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23297 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 112 |  TrainLoss: 0.22535 | ValLoss: 0.32943 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21847 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 113 |  TrainLoss: 0.20756 | ValLoss: 0.32841 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21407 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 114 |  TrainLoss: 0.21755 | ValLoss: 0.32614 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21495 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 115 |  TrainLoss: 0.22677 | ValLoss: 0.32451 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21602 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 116 |  TrainLoss: 0.19808 | ValLoss: 0.32298 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21372 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 117 |  TrainLoss: 0.20876 | ValLoss: 0.32501 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20797 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 118 |  TrainLoss: 0.20175 | ValLoss: 0.32087 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21466 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 119 |  TrainLoss: 0.19967 | ValLoss: 0.31932 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21203 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 120 |  TrainLoss: 0.19311 | ValLoss: 0.31839 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21062 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 121 |  TrainLoss: 0.18196 | ValLoss: 0.31806 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20801 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 122 |  TrainLoss: 0.18323 | ValLoss: 0.31703 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20745 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 123 |  TrainLoss: 0.17554 | ValLoss: 0.31572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21151 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 124 |  TrainLoss: 0.17394 | ValLoss: 0.31409 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20812 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 125 |  TrainLoss: 0.19469 | ValLoss: 0.31263 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21077 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 126 |  TrainLoss: 0.18635 | ValLoss: 0.31191 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20693 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 127 |  TrainLoss: 0.18221 | ValLoss: 0.31205 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20650 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 128 |  TrainLoss: 0.15881 | ValLoss: 0.31172 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20942 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 129 |  TrainLoss: 0.16295 | ValLoss: 0.31186 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20801 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 130 |  TrainLoss: 0.15065 | ValLoss: 0.31265 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20449 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 131 |  TrainLoss: 0.16567 | ValLoss: 0.31258 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20425 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 132 |  TrainLoss: 0.16813 | ValLoss: 0.31220 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.20423 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 133 |  TrainLoss: 0.15276 | ValLoss: 0.31059 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20715 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 134 |  TrainLoss: 0.14234 | ValLoss: 0.31103 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21223 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 135 |  TrainLoss: 0.14649 | ValLoss: 0.31091 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20936 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 136 |  TrainLoss: 0.13702 | ValLoss: 0.31127 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20460 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 137 |  TrainLoss: 0.14990 | ValLoss: 0.31225 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20394 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 138 |  TrainLoss: 0.12670 | ValLoss: 0.31239 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20807 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 139 |  TrainLoss: 0.13281 | ValLoss: 0.31373 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20922 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 140 |  TrainLoss: 0.14084 | ValLoss: 0.31528 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21318 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 141 |  TrainLoss: 0.13438 | ValLoss: 0.31581 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20964 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 142 |  TrainLoss: 0.12011 | ValLoss: 0.31650 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20916 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 143 |  TrainLoss: 0.11408 | ValLoss: 0.31714 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20797 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 144 |  TrainLoss: 0.14593 | ValLoss: 0.31780 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20981 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 145 |  TrainLoss: 0.11695 | ValLoss: 0.31907 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21011 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 146 |  TrainLoss: 0.12282 | ValLoss: 0.32056 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20988 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 147 |  TrainLoss: 0.11757 | ValLoss: 0.32333 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21349 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 148 |  TrainLoss: 0.12303 | ValLoss: 0.32504 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20822 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 149 |  TrainLoss: 0.11636 | ValLoss: 0.32712 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.20902 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 150 |  TrainLoss: 0.11344 | ValLoss: 0.32892 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21243 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 151 |  TrainLoss: 0.11271 | ValLoss: 0.33238 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21637 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 152 |  TrainLoss: 0.09775 | ValLoss: 0.33414 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21278 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 153 |  TrainLoss: 0.10966 | ValLoss: 0.33673 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21204 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 154 |  TrainLoss: 0.09355 | ValLoss: 0.33890 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21337 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 155 |  TrainLoss: 0.10457 | ValLoss: 0.34091 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21475 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.09184 | ValLoss: 0.34269 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21711 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 157 |  TrainLoss: 0.07584 | ValLoss: 0.34466 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21866 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 158 |  TrainLoss: 0.08844 | ValLoss: 0.34677 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22189 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 159 |  TrainLoss: 0.08738 | ValLoss: 0.34885 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22046 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 160 |  TrainLoss: 0.09694 | ValLoss: 0.35239 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22709 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 161 |  TrainLoss: 0.10095 | ValLoss: 0.35362 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22320 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 162 |  TrainLoss: 0.08587 | ValLoss: 0.35592 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22422 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 163 |  TrainLoss: 0.08665 | ValLoss: 0.35822 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22502 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 164 |  TrainLoss: 0.09808 | ValLoss: 0.35991 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22594 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 165 |  TrainLoss: 0.09192 | ValLoss: 0.35682 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22914 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 166 |  TrainLoss: 0.07459 | ValLoss: 0.35944 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22814 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 167 |  TrainLoss: 0.07295 | ValLoss: 0.36324 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22865 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 168 |  TrainLoss: 0.07853 | ValLoss: 0.36018 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23027 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 169 |  TrainLoss: 0.09455 | ValLoss: 0.35873 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23254 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 170 |  TrainLoss: 0.06432 | ValLoss: 0.37722 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23697 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 171 |  TrainLoss: 0.07682 | ValLoss: 0.36493 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23609 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 172 |  TrainLoss: 0.07202 | ValLoss: 0.35386 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24877 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 173 |  TrainLoss: 0.07838 | ValLoss: 0.35691 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24440 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 174 |  TrainLoss: 0.07800 | ValLoss: 0.38536 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24773 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 175 |  TrainLoss: 0.06357 | ValLoss: 0.39253 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25373 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 176 |  TrainLoss: 0.07095 | ValLoss: 0.36887 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24809 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 177 |  TrainLoss: 0.07437 | ValLoss: 0.35382 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25694 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 178 |  TrainLoss: 0.06141 | ValLoss: 0.35631 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25355 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 179 |  TrainLoss: 0.05440 | ValLoss: 0.36930 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25305 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 180 |  TrainLoss: 0.06025 | ValLoss: 0.37259 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25510 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 181 |  TrainLoss: 0.05842 | ValLoss: 0.37182 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25645 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 182 |  TrainLoss: 0.06561 | ValLoss: 0.37103 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25780 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 183 |  TrainLoss: 0.05446 | ValLoss: 0.36867 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25931 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 184 |  TrainLoss: 0.04913 | ValLoss: 0.36156 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26165 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 185 |  TrainLoss: 0.05568 | ValLoss: 0.36856 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26314 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 186 |  TrainLoss: 0.05589 | ValLoss: 0.38068 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26774 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 187 |  TrainLoss: 0.05762 | ValLoss: 0.37279 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26740 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 188 |  TrainLoss: 0.05667 | ValLoss: 0.37163 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26945 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 189 |  TrainLoss: 0.05745 | ValLoss: 0.36913 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27151 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 190 |  TrainLoss: 0.04737 | ValLoss: 0.36430 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27352 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 191 |  TrainLoss: 0.05091 | ValLoss: 0.36418 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27598 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 192 |  TrainLoss: 0.05066 | ValLoss: 0.37030 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27944 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 193 |  TrainLoss: 0.04227 | ValLoss: 0.36044 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28161 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 194 |  TrainLoss: 0.04919 | ValLoss: 0.35891 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28344 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 195 |  TrainLoss: 0.04022 | ValLoss: 0.37022 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28710 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 196 |  TrainLoss: 0.03642 | ValLoss: 0.37416 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29034 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 197 |  TrainLoss: 0.04464 | ValLoss: 0.36393 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29025 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 198 |  TrainLoss: 0.04422 | ValLoss: 0.36313 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29242 | TestAcc: 0.87097 | TestF1: 0.85\n",
            "Epoch: 199 |  TrainLoss: 0.04072 | ValLoss: 0.35639 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29373 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "------Training for beta 0.9476557611418708---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69385 | ValLoss: 0.68858 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69867 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69314 | ValLoss: 0.68837 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69867 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69277 | ValLoss: 0.68833 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69856 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69285 | ValLoss: 0.68820 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69857 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69284 | ValLoss: 0.68805 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69858 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69292 | ValLoss: 0.68794 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69858 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69250 | ValLoss: 0.68786 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69848 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69242 | ValLoss: 0.68774 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69840 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69248 | ValLoss: 0.68759 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69837 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69206 | ValLoss: 0.68742 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69835 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69252 | ValLoss: 0.68734 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69820 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69177 | ValLoss: 0.68731 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69800 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69183 | ValLoss: 0.68713 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69797 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69152 | ValLoss: 0.68700 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69779 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69192 | ValLoss: 0.68688 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69752 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.69140 | ValLoss: 0.68657 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69738 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.69030 | ValLoss: 0.68631 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69716 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.69055 | ValLoss: 0.68600 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69694 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.69037 | ValLoss: 0.68576 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69653 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.68953 | ValLoss: 0.68532 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69626 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.68986 | ValLoss: 0.68494 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69587 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.68929 | ValLoss: 0.68459 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69545 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.68846 | ValLoss: 0.68410 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69512 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.68887 | ValLoss: 0.68364 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69467 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.68822 | ValLoss: 0.68300 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69431 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.68678 | ValLoss: 0.68217 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69380 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.68710 | ValLoss: 0.68111 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69342 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.68508 | ValLoss: 0.68011 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69267 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 28 |  TrainLoss: 0.68415 | ValLoss: 0.67916 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69153 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 29 |  TrainLoss: 0.68311 | ValLoss: 0.67826 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69016 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 30 |  TrainLoss: 0.68122 | ValLoss: 0.67695 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68900 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 31 |  TrainLoss: 0.68028 | ValLoss: 0.67519 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68812 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 32 |  TrainLoss: 0.67861 | ValLoss: 0.67320 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68774 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 33 |  TrainLoss: 0.67824 | ValLoss: 0.67202 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68568 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 34 |  TrainLoss: 0.67485 | ValLoss: 0.67055 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68389 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 35 |  TrainLoss: 0.67376 | ValLoss: 0.66893 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68189 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 36 |  TrainLoss: 0.67304 | ValLoss: 0.66692 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68005 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 37 |  TrainLoss: 0.67023 | ValLoss: 0.66468 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.67798 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 38 |  TrainLoss: 0.66847 | ValLoss: 0.66301 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67472 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 39 |  TrainLoss: 0.66328 | ValLoss: 0.66099 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67180 | TestAcc: 0.54839 | TestF1: 0.65\n",
            "Epoch: 40 |  TrainLoss: 0.66350 | ValLoss: 0.65763 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.66985 | TestAcc: 0.54839 | TestF1: 0.65\n",
            "Epoch: 41 |  TrainLoss: 0.66226 | ValLoss: 0.65534 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.66590 | TestAcc: 0.62903 | TestF1: 0.69\n",
            "Epoch: 42 |  TrainLoss: 0.65639 | ValLoss: 0.65280 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.66192 | TestAcc: 0.66129 | TestF1: 0.71\n",
            "Epoch: 43 |  TrainLoss: 0.65489 | ValLoss: 0.64900 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.65911 | TestAcc: 0.66129 | TestF1: 0.71\n",
            "Epoch: 44 |  TrainLoss: 0.64819 | ValLoss: 0.64565 | ValAcc: 0.77419 | ValF1: 0.84 | TestLoss: 0.65531 | TestAcc: 0.67742 | TestF1: 0.72\n",
            "Epoch: 45 |  TrainLoss: 0.64509 | ValLoss: 0.64131 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.65264 | TestAcc: 0.66129 | TestF1: 0.71\n",
            "Epoch: 46 |  TrainLoss: 0.64188 | ValLoss: 0.63760 | ValAcc: 0.74194 | ValF1: 0.81 | TestLoss: 0.64805 | TestAcc: 0.67742 | TestF1: 0.72\n",
            "Epoch: 47 |  TrainLoss: 0.63919 | ValLoss: 0.63498 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.64102 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 48 |  TrainLoss: 0.63217 | ValLoss: 0.63057 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.63626 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 49 |  TrainLoss: 0.62644 | ValLoss: 0.62533 | ValAcc: 0.77419 | ValF1: 0.83 | TestLoss: 0.63197 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 50 |  TrainLoss: 0.61796 | ValLoss: 0.61999 | ValAcc: 0.77419 | ValF1: 0.83 | TestLoss: 0.62728 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 51 |  TrainLoss: 0.61513 | ValLoss: 0.61503 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.62048 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 52 |  TrainLoss: 0.61119 | ValLoss: 0.61064 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.61182 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 53 |  TrainLoss: 0.60135 | ValLoss: 0.60447 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.60458 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 54 |  TrainLoss: 0.59689 | ValLoss: 0.59697 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.59822 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 55 |  TrainLoss: 0.59074 | ValLoss: 0.59168 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.58858 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 56 |  TrainLoss: 0.58143 | ValLoss: 0.58395 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.58154 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 57 |  TrainLoss: 0.57797 | ValLoss: 0.57681 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.57317 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 58 |  TrainLoss: 0.56513 | ValLoss: 0.57222 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.56012 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 59 |  TrainLoss: 0.56096 | ValLoss: 0.56491 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.55066 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 60 |  TrainLoss: 0.54513 | ValLoss: 0.55510 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.54389 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 61 |  TrainLoss: 0.54640 | ValLoss: 0.54581 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.53944 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 62 |  TrainLoss: 0.53594 | ValLoss: 0.54027 | ValAcc: 0.74194 | ValF1: 0.79 | TestLoss: 0.52228 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 63 |  TrainLoss: 0.52423 | ValLoss: 0.53423 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.50885 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 64 |  TrainLoss: 0.51311 | ValLoss: 0.52374 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.50116 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 65 |  TrainLoss: 0.50490 | ValLoss: 0.51547 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.49077 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 66 |  TrainLoss: 0.49426 | ValLoss: 0.50713 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.48058 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 67 |  TrainLoss: 0.48888 | ValLoss: 0.50161 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.46555 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 68 |  TrainLoss: 0.47410 | ValLoss: 0.49256 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.45578 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 69 |  TrainLoss: 0.46430 | ValLoss: 0.48343 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.44680 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 70 |  TrainLoss: 0.45039 | ValLoss: 0.47864 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.43138 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 71 |  TrainLoss: 0.45622 | ValLoss: 0.46864 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.42264 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 72 |  TrainLoss: 0.43264 | ValLoss: 0.46089 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41152 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 73 |  TrainLoss: 0.41882 | ValLoss: 0.45250 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.40247 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 74 |  TrainLoss: 0.41739 | ValLoss: 0.44474 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.39401 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 75 |  TrainLoss: 0.41073 | ValLoss: 0.44223 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37725 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 76 |  TrainLoss: 0.38880 | ValLoss: 0.43180 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37158 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 77 |  TrainLoss: 0.38066 | ValLoss: 0.42557 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36156 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 78 |  TrainLoss: 0.36871 | ValLoss: 0.42198 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34909 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 79 |  TrainLoss: 0.37275 | ValLoss: 0.41507 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34112 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 80 |  TrainLoss: 0.36587 | ValLoss: 0.40958 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33311 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 81 |  TrainLoss: 0.34697 | ValLoss: 0.40124 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33093 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 82 |  TrainLoss: 0.34679 | ValLoss: 0.39590 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32775 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 83 |  TrainLoss: 0.35173 | ValLoss: 0.39197 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31425 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 84 |  TrainLoss: 0.33150 | ValLoss: 0.39043 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30334 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 85 |  TrainLoss: 0.32294 | ValLoss: 0.38670 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29690 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 86 |  TrainLoss: 0.31960 | ValLoss: 0.37974 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29763 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 87 |  TrainLoss: 0.31616 | ValLoss: 0.37634 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29547 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 88 |  TrainLoss: 0.30462 | ValLoss: 0.37300 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28664 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 89 |  TrainLoss: 0.29572 | ValLoss: 0.37009 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28082 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 90 |  TrainLoss: 0.29436 | ValLoss: 0.36703 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27497 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 91 |  TrainLoss: 0.29160 | ValLoss: 0.36794 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26573 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 92 |  TrainLoss: 0.28546 | ValLoss: 0.36210 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26558 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 93 |  TrainLoss: 0.28315 | ValLoss: 0.35949 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26708 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 94 |  TrainLoss: 0.26076 | ValLoss: 0.35669 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25963 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 95 |  TrainLoss: 0.25038 | ValLoss: 0.35452 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25396 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 96 |  TrainLoss: 0.25865 | ValLoss: 0.35176 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25249 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 97 |  TrainLoss: 0.27087 | ValLoss: 0.34925 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25066 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 98 |  TrainLoss: 0.26095 | ValLoss: 0.34751 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25211 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 99 |  TrainLoss: 0.25112 | ValLoss: 0.34561 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24296 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 100 |  TrainLoss: 0.23775 | ValLoss: 0.34422 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23982 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 101 |  TrainLoss: 0.22895 | ValLoss: 0.34280 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24644 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 102 |  TrainLoss: 0.22355 | ValLoss: 0.34054 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24091 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 103 |  TrainLoss: 0.22970 | ValLoss: 0.33880 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23477 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 104 |  TrainLoss: 0.22998 | ValLoss: 0.33672 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23594 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 105 |  TrainLoss: 0.24433 | ValLoss: 0.33638 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23860 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 106 |  TrainLoss: 0.20646 | ValLoss: 0.33368 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23338 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 107 |  TrainLoss: 0.21602 | ValLoss: 0.33191 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22989 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 108 |  TrainLoss: 0.21906 | ValLoss: 0.33091 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22739 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 109 |  TrainLoss: 0.21288 | ValLoss: 0.32925 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22907 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 110 |  TrainLoss: 0.20169 | ValLoss: 0.32956 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23316 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 111 |  TrainLoss: 0.19082 | ValLoss: 0.32661 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22572 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 112 |  TrainLoss: 0.18116 | ValLoss: 0.32619 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22310 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 113 |  TrainLoss: 0.19407 | ValLoss: 0.32616 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22666 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 114 |  TrainLoss: 0.17893 | ValLoss: 0.32776 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23007 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 115 |  TrainLoss: 0.19656 | ValLoss: 0.32495 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22264 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 116 |  TrainLoss: 0.18832 | ValLoss: 0.32543 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22679 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 117 |  TrainLoss: 0.17845 | ValLoss: 0.32726 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23096 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 118 |  TrainLoss: 0.18042 | ValLoss: 0.32334 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22375 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 119 |  TrainLoss: 0.16737 | ValLoss: 0.32528 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22888 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 120 |  TrainLoss: 0.16248 | ValLoss: 0.32296 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22364 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 121 |  TrainLoss: 0.17074 | ValLoss: 0.32247 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21785 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 122 |  TrainLoss: 0.16439 | ValLoss: 0.32208 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.21856 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 123 |  TrainLoss: 0.14480 | ValLoss: 0.32371 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22621 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 124 |  TrainLoss: 0.15510 | ValLoss: 0.32585 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22958 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 125 |  TrainLoss: 0.16370 | ValLoss: 0.32273 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.22033 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 126 |  TrainLoss: 0.14611 | ValLoss: 0.32487 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.21754 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 127 |  TrainLoss: 0.14776 | ValLoss: 0.32315 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.22260 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 128 |  TrainLoss: 0.13867 | ValLoss: 0.32540 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22957 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 129 |  TrainLoss: 0.14587 | ValLoss: 0.32572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23083 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 130 |  TrainLoss: 0.14339 | ValLoss: 0.32350 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.22527 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 131 |  TrainLoss: 0.13951 | ValLoss: 0.32341 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.22869 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 132 |  TrainLoss: 0.13354 | ValLoss: 0.32306 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.22926 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 133 |  TrainLoss: 0.12962 | ValLoss: 0.32410 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23059 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 134 |  TrainLoss: 0.13942 | ValLoss: 0.32598 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23267 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 135 |  TrainLoss: 0.12055 | ValLoss: 0.32697 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23382 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 136 |  TrainLoss: 0.12404 | ValLoss: 0.32661 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23376 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 137 |  TrainLoss: 0.12085 | ValLoss: 0.32738 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23496 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 138 |  TrainLoss: 0.12496 | ValLoss: 0.32895 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24002 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 139 |  TrainLoss: 0.11652 | ValLoss: 0.32894 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.23726 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 140 |  TrainLoss: 0.10138 | ValLoss: 0.32992 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24179 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 141 |  TrainLoss: 0.11437 | ValLoss: 0.33121 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24460 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 142 |  TrainLoss: 0.10552 | ValLoss: 0.33490 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25018 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 143 |  TrainLoss: 0.10826 | ValLoss: 0.33371 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24614 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 144 |  TrainLoss: 0.09748 | ValLoss: 0.33535 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24622 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 145 |  TrainLoss: 0.10796 | ValLoss: 0.33706 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24955 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 146 |  TrainLoss: 0.09661 | ValLoss: 0.34332 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26045 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 147 |  TrainLoss: 0.09737 | ValLoss: 0.33973 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25261 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 148 |  TrainLoss: 0.08841 | ValLoss: 0.34375 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25308 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 149 |  TrainLoss: 0.09186 | ValLoss: 0.34442 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25720 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 150 |  TrainLoss: 0.08739 | ValLoss: 0.35453 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27042 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 151 |  TrainLoss: 0.08912 | ValLoss: 0.35048 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26150 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 152 |  TrainLoss: 0.08516 | ValLoss: 0.35387 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26215 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 153 |  TrainLoss: 0.08606 | ValLoss: 0.35509 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26718 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 154 |  TrainLoss: 0.08512 | ValLoss: 0.36102 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27522 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 155 |  TrainLoss: 0.08920 | ValLoss: 0.35791 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26876 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 156 |  TrainLoss: 0.07446 | ValLoss: 0.36015 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27113 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 157 |  TrainLoss: 0.08649 | ValLoss: 0.36299 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27414 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 158 |  TrainLoss: 0.07988 | ValLoss: 0.36553 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27408 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 159 |  TrainLoss: 0.08246 | ValLoss: 0.36703 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27668 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 160 |  TrainLoss: 0.07186 | ValLoss: 0.37053 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28226 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 161 |  TrainLoss: 0.08070 | ValLoss: 0.37028 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28197 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 162 |  TrainLoss: 0.06390 | ValLoss: 0.37146 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28353 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 163 |  TrainLoss: 0.07864 | ValLoss: 0.37346 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28957 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 164 |  TrainLoss: 0.06714 | ValLoss: 0.37342 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29017 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 165 |  TrainLoss: 0.06736 | ValLoss: 0.37536 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29204 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 166 |  TrainLoss: 0.06502 | ValLoss: 0.37574 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29443 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 167 |  TrainLoss: 0.07036 | ValLoss: 0.37560 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29987 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 168 |  TrainLoss: 0.06281 | ValLoss: 0.38099 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29845 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 169 |  TrainLoss: 0.06549 | ValLoss: 0.38481 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30170 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 170 |  TrainLoss: 0.05182 | ValLoss: 0.37768 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30298 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 171 |  TrainLoss: 0.06455 | ValLoss: 0.37734 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30633 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 172 |  TrainLoss: 0.05056 | ValLoss: 0.37835 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30780 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 173 |  TrainLoss: 0.05425 | ValLoss: 0.38275 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31125 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 174 |  TrainLoss: 0.05766 | ValLoss: 0.37854 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31237 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 175 |  TrainLoss: 0.05412 | ValLoss: 0.37935 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31517 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 176 |  TrainLoss: 0.05267 | ValLoss: 0.38192 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31856 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 177 |  TrainLoss: 0.05053 | ValLoss: 0.38297 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32128 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 178 |  TrainLoss: 0.04738 | ValLoss: 0.38117 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32334 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 179 |  TrainLoss: 0.04740 | ValLoss: 0.37891 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32513 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 180 |  TrainLoss: 0.03757 | ValLoss: 0.38175 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32806 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 181 |  TrainLoss: 0.04704 | ValLoss: 0.38674 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33203 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 182 |  TrainLoss: 0.04583 | ValLoss: 0.38431 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33265 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 183 |  TrainLoss: 0.03362 | ValLoss: 0.38669 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33618 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 184 |  TrainLoss: 0.04964 | ValLoss: 0.38729 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33910 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 185 |  TrainLoss: 0.04587 | ValLoss: 0.38459 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33950 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 186 |  TrainLoss: 0.04605 | ValLoss: 0.38486 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34165 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 187 |  TrainLoss: 0.04370 | ValLoss: 0.39098 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34799 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 188 |  TrainLoss: 0.04257 | ValLoss: 0.39252 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35231 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 189 |  TrainLoss: 0.04429 | ValLoss: 0.38643 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34936 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 190 |  TrainLoss: 0.03829 | ValLoss: 0.38978 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35493 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 191 |  TrainLoss: 0.04143 | ValLoss: 0.39010 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35785 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 192 |  TrainLoss: 0.03175 | ValLoss: 0.38917 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.35854 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 193 |  TrainLoss: 0.04101 | ValLoss: 0.39075 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36264 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 194 |  TrainLoss: 0.03768 | ValLoss: 0.39386 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36845 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 195 |  TrainLoss: 0.04135 | ValLoss: 0.39171 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.36888 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 196 |  TrainLoss: 0.03589 | ValLoss: 0.39191 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37101 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 197 |  TrainLoss: 0.02664 | ValLoss: 0.39414 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37834 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 198 |  TrainLoss: 0.03061 | ValLoss: 0.39442 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37996 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 199 |  TrainLoss: 0.02943 | ValLoss: 0.39517 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.38290 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "------Training for beta 0.9450459991269611---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69239 | ValLoss: 0.68650 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70140 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69256 | ValLoss: 0.68647 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70116 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69204 | ValLoss: 0.68660 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70075 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69328 | ValLoss: 0.68674 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70027 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69265 | ValLoss: 0.68664 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.70007 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69189 | ValLoss: 0.68663 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69972 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69255 | ValLoss: 0.68638 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69960 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69178 | ValLoss: 0.68609 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69951 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69115 | ValLoss: 0.68595 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69919 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69142 | ValLoss: 0.68586 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69875 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69123 | ValLoss: 0.68567 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69839 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69083 | ValLoss: 0.68554 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69798 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69028 | ValLoss: 0.68556 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69742 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69040 | ValLoss: 0.68544 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69696 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69020 | ValLoss: 0.68495 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69679 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.68949 | ValLoss: 0.68448 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69652 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.68917 | ValLoss: 0.68398 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69617 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.68861 | ValLoss: 0.68374 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69546 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.68786 | ValLoss: 0.68340 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69479 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.68812 | ValLoss: 0.68289 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69428 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.68695 | ValLoss: 0.68263 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69334 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.68516 | ValLoss: 0.68212 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69251 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.68582 | ValLoss: 0.68165 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69145 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.68474 | ValLoss: 0.68061 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69064 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.68317 | ValLoss: 0.68003 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68927 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.68287 | ValLoss: 0.67937 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68792 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.68156 | ValLoss: 0.67825 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68672 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 27 |  TrainLoss: 0.67984 | ValLoss: 0.67773 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68480 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 28 |  TrainLoss: 0.68033 | ValLoss: 0.67730 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68284 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 29 |  TrainLoss: 0.67706 | ValLoss: 0.67525 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68182 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 30 |  TrainLoss: 0.67623 | ValLoss: 0.67388 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68011 | TestAcc: 0.58065 | TestF1: 0.67\n",
            "Epoch: 31 |  TrainLoss: 0.67527 | ValLoss: 0.67285 | ValAcc: 0.77419 | ValF1: 0.84 | TestLoss: 0.67755 | TestAcc: 0.72581 | TestF1: 0.75\n",
            "Epoch: 32 |  TrainLoss: 0.67264 | ValLoss: 0.67114 | ValAcc: 0.77419 | ValF1: 0.83 | TestLoss: 0.67548 | TestAcc: 0.74194 | TestF1: 0.76\n",
            "Epoch: 33 |  TrainLoss: 0.67042 | ValLoss: 0.66956 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.67305 | TestAcc: 0.77419 | TestF1: 0.79\n",
            "Epoch: 34 |  TrainLoss: 0.66873 | ValLoss: 0.66748 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.67101 | TestAcc: 0.77419 | TestF1: 0.79\n",
            "Epoch: 35 |  TrainLoss: 0.66544 | ValLoss: 0.66637 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.66732 | TestAcc: 0.82258 | TestF1: 0.81\n",
            "Epoch: 36 |  TrainLoss: 0.66589 | ValLoss: 0.66445 | ValAcc: 0.74194 | ValF1: 0.78 | TestLoss: 0.66418 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 37 |  TrainLoss: 0.66189 | ValLoss: 0.66327 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.66026 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 38 |  TrainLoss: 0.65963 | ValLoss: 0.66028 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.65700 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 39 |  TrainLoss: 0.65594 | ValLoss: 0.65762 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.65333 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 40 |  TrainLoss: 0.65445 | ValLoss: 0.65378 | ValAcc: 0.77419 | ValF1: 0.80 | TestLoss: 0.65031 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 41 |  TrainLoss: 0.65139 | ValLoss: 0.65167 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.64559 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 42 |  TrainLoss: 0.64842 | ValLoss: 0.64842 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.64190 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 43 |  TrainLoss: 0.64483 | ValLoss: 0.64694 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.63656 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 44 |  TrainLoss: 0.63900 | ValLoss: 0.64684 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.63100 | TestAcc: 0.85484 | TestF1: 0.81\n",
            "Epoch: 45 |  TrainLoss: 0.63473 | ValLoss: 0.64274 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.62613 | TestAcc: 0.87097 | TestF1: 0.83\n",
            "Epoch: 46 |  TrainLoss: 0.63415 | ValLoss: 0.63695 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.62187 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 47 |  TrainLoss: 0.62558 | ValLoss: 0.63090 | ValAcc: 0.77419 | ValF1: 0.79 | TestLoss: 0.61944 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 48 |  TrainLoss: 0.62365 | ValLoss: 0.62932 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.61073 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 49 |  TrainLoss: 0.61840 | ValLoss: 0.62518 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.60473 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 50 |  TrainLoss: 0.61193 | ValLoss: 0.62018 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.59859 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 51 |  TrainLoss: 0.60830 | ValLoss: 0.61429 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.59278 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 52 |  TrainLoss: 0.59899 | ValLoss: 0.60910 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.58552 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 53 |  TrainLoss: 0.59424 | ValLoss: 0.60504 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.57617 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 54 |  TrainLoss: 0.58900 | ValLoss: 0.59931 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.56794 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 55 |  TrainLoss: 0.58125 | ValLoss: 0.59322 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.55932 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 56 |  TrainLoss: 0.57271 | ValLoss: 0.58708 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.55007 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 57 |  TrainLoss: 0.56642 | ValLoss: 0.57899 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.54273 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 58 |  TrainLoss: 0.55141 | ValLoss: 0.57248 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.53259 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 59 |  TrainLoss: 0.54211 | ValLoss: 0.56638 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.52168 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 60 |  TrainLoss: 0.53948 | ValLoss: 0.55962 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.51131 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 61 |  TrainLoss: 0.53209 | ValLoss: 0.54886 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.50659 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 62 |  TrainLoss: 0.51987 | ValLoss: 0.54172 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.49534 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 63 |  TrainLoss: 0.51909 | ValLoss: 0.53531 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.48261 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 64 |  TrainLoss: 0.49949 | ValLoss: 0.52790 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.47145 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 65 |  TrainLoss: 0.48998 | ValLoss: 0.51907 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.46142 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 66 |  TrainLoss: 0.47819 | ValLoss: 0.51199 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.44928 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 67 |  TrainLoss: 0.47976 | ValLoss: 0.50434 | ValAcc: 0.80645 | ValF1: 0.81 | TestLoss: 0.43827 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 68 |  TrainLoss: 0.46687 | ValLoss: 0.49317 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.43354 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 69 |  TrainLoss: 0.45677 | ValLoss: 0.48474 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.42841 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 70 |  TrainLoss: 0.44207 | ValLoss: 0.47798 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.41119 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 71 |  TrainLoss: 0.43060 | ValLoss: 0.47105 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.39938 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 72 |  TrainLoss: 0.42243 | ValLoss: 0.46685 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.38643 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 73 |  TrainLoss: 0.41193 | ValLoss: 0.45328 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.38473 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 74 |  TrainLoss: 0.40996 | ValLoss: 0.44641 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.37391 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 75 |  TrainLoss: 0.38711 | ValLoss: 0.44064 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.36130 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 76 |  TrainLoss: 0.39685 | ValLoss: 0.43373 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.35275 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 77 |  TrainLoss: 0.38671 | ValLoss: 0.42592 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.35862 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 78 |  TrainLoss: 0.37467 | ValLoss: 0.41985 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.34607 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 79 |  TrainLoss: 0.35269 | ValLoss: 0.41935 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.32449 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 80 |  TrainLoss: 0.36217 | ValLoss: 0.41086 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.31773 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 81 |  TrainLoss: 0.34861 | ValLoss: 0.40211 | ValAcc: 0.80645 | ValF1: 0.82 | TestLoss: 0.31388 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 82 |  TrainLoss: 0.32834 | ValLoss: 0.39594 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30837 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 83 |  TrainLoss: 0.31853 | ValLoss: 0.39191 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.29827 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 84 |  TrainLoss: 0.32547 | ValLoss: 0.38414 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29927 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 85 |  TrainLoss: 0.31956 | ValLoss: 0.37945 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29478 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 86 |  TrainLoss: 0.29618 | ValLoss: 0.37963 | ValAcc: 0.83871 | ValF1: 0.85 | TestLoss: 0.27786 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 87 |  TrainLoss: 0.31134 | ValLoss: 0.37318 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.27414 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 88 |  TrainLoss: 0.28692 | ValLoss: 0.36930 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28934 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 89 |  TrainLoss: 0.28681 | ValLoss: 0.36321 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26841 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 90 |  TrainLoss: 0.27712 | ValLoss: 0.36173 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.25941 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 91 |  TrainLoss: 0.26265 | ValLoss: 0.35576 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26109 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 92 |  TrainLoss: 0.27501 | ValLoss: 0.35293 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25933 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 93 |  TrainLoss: 0.25069 | ValLoss: 0.35043 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25178 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 94 |  TrainLoss: 0.23336 | ValLoss: 0.35048 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.24379 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 95 |  TrainLoss: 0.26437 | ValLoss: 0.34397 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24787 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 96 |  TrainLoss: 0.23263 | ValLoss: 0.34116 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24597 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 97 |  TrainLoss: 0.24317 | ValLoss: 0.34022 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25013 | TestAcc: 0.95161 | TestF1: 0.95\n",
            "Epoch: 98 |  TrainLoss: 0.25533 | ValLoss: 0.33601 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23605 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 99 |  TrainLoss: 0.24413 | ValLoss: 0.33452 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23253 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 100 |  TrainLoss: 0.24517 | ValLoss: 0.33635 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22801 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 101 |  TrainLoss: 0.23247 | ValLoss: 0.33071 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23019 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 102 |  TrainLoss: 0.22395 | ValLoss: 0.32976 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23554 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 103 |  TrainLoss: 0.22869 | ValLoss: 0.32793 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23165 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 104 |  TrainLoss: 0.21133 | ValLoss: 0.33145 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22257 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 105 |  TrainLoss: 0.21616 | ValLoss: 0.32700 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22264 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 106 |  TrainLoss: 0.19065 | ValLoss: 0.32554 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23126 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 107 |  TrainLoss: 0.19475 | ValLoss: 0.32400 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22825 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 108 |  TrainLoss: 0.19540 | ValLoss: 0.32290 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21894 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 109 |  TrainLoss: 0.18672 | ValLoss: 0.32015 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21968 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 110 |  TrainLoss: 0.19355 | ValLoss: 0.32490 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23515 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 111 |  TrainLoss: 0.19016 | ValLoss: 0.31786 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21838 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 112 |  TrainLoss: 0.17551 | ValLoss: 0.31936 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21533 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 113 |  TrainLoss: 0.18824 | ValLoss: 0.31576 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21630 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 114 |  TrainLoss: 0.18772 | ValLoss: 0.31912 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22990 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 115 |  TrainLoss: 0.14895 | ValLoss: 0.31497 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21766 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 116 |  TrainLoss: 0.17153 | ValLoss: 0.31753 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21258 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 117 |  TrainLoss: 0.15107 | ValLoss: 0.31486 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21524 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 118 |  TrainLoss: 0.16580 | ValLoss: 0.31573 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21776 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 119 |  TrainLoss: 0.15321 | ValLoss: 0.31546 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21568 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 120 |  TrainLoss: 0.15096 | ValLoss: 0.31545 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21494 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 121 |  TrainLoss: 0.14526 | ValLoss: 0.31501 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21547 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 122 |  TrainLoss: 0.15279 | ValLoss: 0.31536 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21719 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 123 |  TrainLoss: 0.15238 | ValLoss: 0.31529 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21770 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 124 |  TrainLoss: 0.13826 | ValLoss: 0.31602 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21922 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 125 |  TrainLoss: 0.13160 | ValLoss: 0.31609 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21773 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 126 |  TrainLoss: 0.14398 | ValLoss: 0.31637 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21881 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 127 |  TrainLoss: 0.13749 | ValLoss: 0.31751 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22106 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 128 |  TrainLoss: 0.12377 | ValLoss: 0.31954 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22393 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 129 |  TrainLoss: 0.13810 | ValLoss: 0.31974 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22005 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 130 |  TrainLoss: 0.11451 | ValLoss: 0.32174 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21971 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 131 |  TrainLoss: 0.12019 | ValLoss: 0.32197 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22140 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 132 |  TrainLoss: 0.11645 | ValLoss: 0.32371 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22274 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 133 |  TrainLoss: 0.11581 | ValLoss: 0.32477 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22330 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 134 |  TrainLoss: 0.12238 | ValLoss: 0.32580 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22468 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 135 |  TrainLoss: 0.10728 | ValLoss: 0.32697 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22618 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 136 |  TrainLoss: 0.10972 | ValLoss: 0.32816 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22779 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 137 |  TrainLoss: 0.10695 | ValLoss: 0.33085 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23090 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 138 |  TrainLoss: 0.10785 | ValLoss: 0.33203 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23066 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 139 |  TrainLoss: 0.11513 | ValLoss: 0.33482 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23188 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 140 |  TrainLoss: 0.10409 | ValLoss: 0.33761 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23355 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 141 |  TrainLoss: 0.10590 | ValLoss: 0.34008 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23452 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 142 |  TrainLoss: 0.09245 | ValLoss: 0.34388 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23603 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 143 |  TrainLoss: 0.09558 | ValLoss: 0.34402 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23763 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 144 |  TrainLoss: 0.10042 | ValLoss: 0.34280 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24195 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 145 |  TrainLoss: 0.09871 | ValLoss: 0.34498 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24263 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 146 |  TrainLoss: 0.10054 | ValLoss: 0.35019 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24379 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 147 |  TrainLoss: 0.08788 | ValLoss: 0.35160 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24555 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 148 |  TrainLoss: 0.09221 | ValLoss: 0.35142 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24729 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 149 |  TrainLoss: 0.08687 | ValLoss: 0.34879 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24907 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 150 |  TrainLoss: 0.08198 | ValLoss: 0.35162 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25237 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 151 |  TrainLoss: 0.09569 | ValLoss: 0.35262 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25482 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 152 |  TrainLoss: 0.08732 | ValLoss: 0.35502 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25829 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 153 |  TrainLoss: 0.08024 | ValLoss: 0.34875 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25753 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 154 |  TrainLoss: 0.07737 | ValLoss: 0.35300 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26086 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 155 |  TrainLoss: 0.07777 | ValLoss: 0.35395 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26246 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.07367 | ValLoss: 0.35532 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26406 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 157 |  TrainLoss: 0.08060 | ValLoss: 0.36077 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26877 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 158 |  TrainLoss: 0.07292 | ValLoss: 0.35275 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26519 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 159 |  TrainLoss: 0.06456 | ValLoss: 0.34993 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26412 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 160 |  TrainLoss: 0.06889 | ValLoss: 0.35769 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26951 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 161 |  TrainLoss: 0.07779 | ValLoss: 0.37078 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28003 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 162 |  TrainLoss: 0.07244 | ValLoss: 0.36572 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27861 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 163 |  TrainLoss: 0.07660 | ValLoss: 0.35176 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26857 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 164 |  TrainLoss: 0.05658 | ValLoss: 0.35794 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27409 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 165 |  TrainLoss: 0.07084 | ValLoss: 0.37440 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28952 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 166 |  TrainLoss: 0.06622 | ValLoss: 0.36621 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28351 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 167 |  TrainLoss: 0.06403 | ValLoss: 0.35148 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27114 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 168 |  TrainLoss: 0.06600 | ValLoss: 0.35771 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27748 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 169 |  TrainLoss: 0.06390 | ValLoss: 0.39199 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30597 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 170 |  TrainLoss: 0.07346 | ValLoss: 0.37694 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30027 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 171 |  TrainLoss: 0.06668 | ValLoss: 0.35865 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28815 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 172 |  TrainLoss: 0.06267 | ValLoss: 0.34745 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28004 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 173 |  TrainLoss: 0.06128 | ValLoss: 0.35872 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29400 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 174 |  TrainLoss: 0.06408 | ValLoss: 0.38363 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32074 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 175 |  TrainLoss: 0.05711 | ValLoss: 0.36415 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30659 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 176 |  TrainLoss: 0.06650 | ValLoss: 0.34970 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28893 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 177 |  TrainLoss: 0.05575 | ValLoss: 0.36102 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30732 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 178 |  TrainLoss: 0.05076 | ValLoss: 0.37726 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32621 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 179 |  TrainLoss: 0.06109 | ValLoss: 0.37188 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32371 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 180 |  TrainLoss: 0.04865 | ValLoss: 0.35257 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.30386 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 181 |  TrainLoss: 0.05588 | ValLoss: 0.34933 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.30269 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 182 |  TrainLoss: 0.05522 | ValLoss: 0.37589 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33378 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 183 |  TrainLoss: 0.05656 | ValLoss: 0.38198 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34195 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 184 |  TrainLoss: 0.05318 | ValLoss: 0.34619 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.30937 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 185 |  TrainLoss: 0.05380 | ValLoss: 0.34650 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.31410 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 186 |  TrainLoss: 0.05238 | ValLoss: 0.37512 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.34867 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 187 |  TrainLoss: 0.04612 | ValLoss: 0.37238 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.35017 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 188 |  TrainLoss: 0.04320 | ValLoss: 0.34963 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.33295 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 189 |  TrainLoss: 0.05352 | ValLoss: 0.33902 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.32059 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 190 |  TrainLoss: 0.04325 | ValLoss: 0.35939 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.35177 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 191 |  TrainLoss: 0.04428 | ValLoss: 0.38290 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37477 | TestAcc: 0.87097 | TestF1: 0.84\n",
            "Epoch: 192 |  TrainLoss: 0.04218 | ValLoss: 0.34576 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.34847 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 193 |  TrainLoss: 0.03914 | ValLoss: 0.32868 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.32990 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 194 |  TrainLoss: 0.04354 | ValLoss: 0.33341 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.34433 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 195 |  TrainLoss: 0.03587 | ValLoss: 0.36172 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37676 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 196 |  TrainLoss: 0.04648 | ValLoss: 0.35414 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.37441 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 197 |  TrainLoss: 0.04330 | ValLoss: 0.33441 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.35828 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 198 |  TrainLoss: 0.04217 | ValLoss: 0.31903 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.33759 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 199 |  TrainLoss: 0.04110 | ValLoss: 0.32694 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.35795 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "------Training for beta 0.9604665392766211---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69279 | ValLoss: 0.68956 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69721 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 01 |  TrainLoss: 0.69232 | ValLoss: 0.68955 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69699 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 02 |  TrainLoss: 0.69271 | ValLoss: 0.68949 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69689 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 03 |  TrainLoss: 0.69331 | ValLoss: 0.68941 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69681 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 04 |  TrainLoss: 0.69176 | ValLoss: 0.68934 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69670 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 05 |  TrainLoss: 0.69198 | ValLoss: 0.68926 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69660 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 06 |  TrainLoss: 0.69187 | ValLoss: 0.68910 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69655 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 07 |  TrainLoss: 0.69247 | ValLoss: 0.68885 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69654 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 08 |  TrainLoss: 0.69180 | ValLoss: 0.68862 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69649 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 09 |  TrainLoss: 0.69163 | ValLoss: 0.68845 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69639 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 10 |  TrainLoss: 0.69172 | ValLoss: 0.68815 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69639 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 11 |  TrainLoss: 0.69167 | ValLoss: 0.68784 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69636 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 12 |  TrainLoss: 0.69064 | ValLoss: 0.68753 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69628 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 13 |  TrainLoss: 0.69064 | ValLoss: 0.68723 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69618 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 14 |  TrainLoss: 0.69033 | ValLoss: 0.68698 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69598 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 15 |  TrainLoss: 0.68958 | ValLoss: 0.68644 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69587 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 16 |  TrainLoss: 0.68932 | ValLoss: 0.68585 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69569 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 17 |  TrainLoss: 0.68933 | ValLoss: 0.68518 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69561 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 18 |  TrainLoss: 0.68944 | ValLoss: 0.68489 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69514 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 19 |  TrainLoss: 0.68918 | ValLoss: 0.68448 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69478 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 20 |  TrainLoss: 0.68789 | ValLoss: 0.68414 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69432 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 21 |  TrainLoss: 0.68775 | ValLoss: 0.68356 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69399 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 22 |  TrainLoss: 0.68713 | ValLoss: 0.68287 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69365 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 23 |  TrainLoss: 0.68617 | ValLoss: 0.68221 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69318 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 24 |  TrainLoss: 0.68503 | ValLoss: 0.68149 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69272 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 25 |  TrainLoss: 0.68519 | ValLoss: 0.68068 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69217 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 26 |  TrainLoss: 0.68432 | ValLoss: 0.68020 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69120 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 27 |  TrainLoss: 0.68245 | ValLoss: 0.67926 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69052 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 28 |  TrainLoss: 0.68264 | ValLoss: 0.67780 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.69017 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 29 |  TrainLoss: 0.68031 | ValLoss: 0.67636 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68971 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 30 |  TrainLoss: 0.67986 | ValLoss: 0.67505 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68875 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 31 |  TrainLoss: 0.67866 | ValLoss: 0.67358 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68797 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 32 |  TrainLoss: 0.67768 | ValLoss: 0.67287 | ValAcc: 0.58065 | ValF1: 0.73 | TestLoss: 0.68605 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 33 |  TrainLoss: 0.67638 | ValLoss: 0.67181 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68441 | TestAcc: 0.41935 | TestF1: 0.59\n",
            "Epoch: 34 |  TrainLoss: 0.67618 | ValLoss: 0.67065 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68275 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 35 |  TrainLoss: 0.67344 | ValLoss: 0.66918 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68115 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 36 |  TrainLoss: 0.67303 | ValLoss: 0.66675 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.68015 | TestAcc: 0.43548 | TestF1: 0.60\n",
            "Epoch: 37 |  TrainLoss: 0.67077 | ValLoss: 0.66443 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.67910 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 38 |  TrainLoss: 0.66693 | ValLoss: 0.66247 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.67719 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 39 |  TrainLoss: 0.66568 | ValLoss: 0.66040 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.67512 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 40 |  TrainLoss: 0.66274 | ValLoss: 0.65837 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67259 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 41 |  TrainLoss: 0.66352 | ValLoss: 0.65538 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.67091 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 42 |  TrainLoss: 0.65878 | ValLoss: 0.65276 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.66822 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 43 |  TrainLoss: 0.65437 | ValLoss: 0.65040 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.66487 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 44 |  TrainLoss: 0.65312 | ValLoss: 0.64806 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.66152 | TestAcc: 0.59677 | TestF1: 0.68\n",
            "Epoch: 45 |  TrainLoss: 0.65025 | ValLoss: 0.64386 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.65980 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 46 |  TrainLoss: 0.64550 | ValLoss: 0.63988 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.65781 | TestAcc: 0.56452 | TestF1: 0.66\n",
            "Epoch: 47 |  TrainLoss: 0.64249 | ValLoss: 0.63677 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.65358 | TestAcc: 0.59677 | TestF1: 0.68\n",
            "Epoch: 48 |  TrainLoss: 0.63749 | ValLoss: 0.63337 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.64947 | TestAcc: 0.64516 | TestF1: 0.70\n",
            "Epoch: 49 |  TrainLoss: 0.63654 | ValLoss: 0.62937 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.64572 | TestAcc: 0.64516 | TestF1: 0.70\n",
            "Epoch: 50 |  TrainLoss: 0.63029 | ValLoss: 0.62558 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.64109 | TestAcc: 0.66129 | TestF1: 0.71\n",
            "Epoch: 51 |  TrainLoss: 0.62832 | ValLoss: 0.62082 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.63690 | TestAcc: 0.67742 | TestF1: 0.72\n",
            "Epoch: 52 |  TrainLoss: 0.61809 | ValLoss: 0.61573 | ValAcc: 0.74194 | ValF1: 0.82 | TestLoss: 0.63260 | TestAcc: 0.67742 | TestF1: 0.72\n",
            "Epoch: 53 |  TrainLoss: 0.61512 | ValLoss: 0.61084 | ValAcc: 0.77419 | ValF1: 0.84 | TestLoss: 0.62735 | TestAcc: 0.70968 | TestF1: 0.74\n",
            "Epoch: 54 |  TrainLoss: 0.61305 | ValLoss: 0.60573 | ValAcc: 0.80645 | ValF1: 0.86 | TestLoss: 0.62153 | TestAcc: 0.72581 | TestF1: 0.75\n",
            "Epoch: 55 |  TrainLoss: 0.60163 | ValLoss: 0.59984 | ValAcc: 0.80645 | ValF1: 0.86 | TestLoss: 0.61608 | TestAcc: 0.72581 | TestF1: 0.75\n",
            "Epoch: 56 |  TrainLoss: 0.59893 | ValLoss: 0.59535 | ValAcc: 0.83871 | ValF1: 0.88 | TestLoss: 0.60830 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 57 |  TrainLoss: 0.59159 | ValLoss: 0.58798 | ValAcc: 0.80645 | ValF1: 0.86 | TestLoss: 0.60340 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 58 |  TrainLoss: 0.58136 | ValLoss: 0.58130 | ValAcc: 0.80645 | ValF1: 0.86 | TestLoss: 0.59718 | TestAcc: 0.79032 | TestF1: 0.80\n",
            "Epoch: 59 |  TrainLoss: 0.57756 | ValLoss: 0.57549 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.58933 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 60 |  TrainLoss: 0.57393 | ValLoss: 0.56825 | ValAcc: 0.77419 | ValF1: 0.83 | TestLoss: 0.58360 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 61 |  TrainLoss: 0.56222 | ValLoss: 0.56155 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.57616 | TestAcc: 0.82258 | TestF1: 0.83\n",
            "Epoch: 62 |  TrainLoss: 0.55495 | ValLoss: 0.55502 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.56772 | TestAcc: 0.82258 | TestF1: 0.83\n",
            "Epoch: 63 |  TrainLoss: 0.56078 | ValLoss: 0.54946 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.55814 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 64 |  TrainLoss: 0.54518 | ValLoss: 0.54696 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.54812 | TestAcc: 0.82258 | TestF1: 0.82\n",
            "Epoch: 65 |  TrainLoss: 0.54000 | ValLoss: 0.53657 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.54204 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 66 |  TrainLoss: 0.53045 | ValLoss: 0.52796 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.53770 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 67 |  TrainLoss: 0.52190 | ValLoss: 0.52128 | ValAcc: 0.80645 | ValF1: 0.85 | TestLoss: 0.52826 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 68 |  TrainLoss: 0.51629 | ValLoss: 0.51552 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.51760 | TestAcc: 0.83871 | TestF1: 0.84\n",
            "Epoch: 69 |  TrainLoss: 0.48959 | ValLoss: 0.50928 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.50765 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 70 |  TrainLoss: 0.49938 | ValLoss: 0.50296 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.49767 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 71 |  TrainLoss: 0.48268 | ValLoss: 0.49363 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.48947 | TestAcc: 0.87097 | TestF1: 0.87\n",
            "Epoch: 72 |  TrainLoss: 0.47722 | ValLoss: 0.48515 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.48146 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 73 |  TrainLoss: 0.47500 | ValLoss: 0.47751 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.47254 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 74 |  TrainLoss: 0.46183 | ValLoss: 0.47204 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.45994 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 75 |  TrainLoss: 0.45595 | ValLoss: 0.46444 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.45053 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 76 |  TrainLoss: 0.44922 | ValLoss: 0.45706 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.44141 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 77 |  TrainLoss: 0.43952 | ValLoss: 0.45148 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.43029 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 78 |  TrainLoss: 0.42501 | ValLoss: 0.44427 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.42018 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 79 |  TrainLoss: 0.41323 | ValLoss: 0.43486 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.41234 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 80 |  TrainLoss: 0.40726 | ValLoss: 0.42720 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.40298 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 81 |  TrainLoss: 0.39207 | ValLoss: 0.42049 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.39236 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 82 |  TrainLoss: 0.39387 | ValLoss: 0.41597 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.38061 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 83 |  TrainLoss: 0.38256 | ValLoss: 0.40875 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37134 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 84 |  TrainLoss: 0.37081 | ValLoss: 0.40146 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.36245 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 85 |  TrainLoss: 0.37826 | ValLoss: 0.39416 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35443 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 86 |  TrainLoss: 0.35843 | ValLoss: 0.38740 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34625 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 87 |  TrainLoss: 0.35195 | ValLoss: 0.38059 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33889 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 88 |  TrainLoss: 0.32058 | ValLoss: 0.37500 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.33011 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 89 |  TrainLoss: 0.33517 | ValLoss: 0.36940 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.32237 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 90 |  TrainLoss: 0.31734 | ValLoss: 0.36345 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.31517 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 91 |  TrainLoss: 0.30214 | ValLoss: 0.35860 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.30687 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 92 |  TrainLoss: 0.30451 | ValLoss: 0.35336 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29981 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 93 |  TrainLoss: 0.31248 | ValLoss: 0.34809 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29315 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 94 |  TrainLoss: 0.30383 | ValLoss: 0.34587 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.28590 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 95 |  TrainLoss: 0.27797 | ValLoss: 0.33837 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28545 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 96 |  TrainLoss: 0.27884 | ValLoss: 0.33397 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27827 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 97 |  TrainLoss: 0.29563 | ValLoss: 0.32968 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27258 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 98 |  TrainLoss: 0.27827 | ValLoss: 0.32675 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26490 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 99 |  TrainLoss: 0.27874 | ValLoss: 0.32232 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26113 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 100 |  TrainLoss: 0.25478 | ValLoss: 0.31836 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25748 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 101 |  TrainLoss: 0.24907 | ValLoss: 0.31456 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25623 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 102 |  TrainLoss: 0.25490 | ValLoss: 0.31099 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25355 | TestAcc: 0.95161 | TestF1: 0.94\n",
            "Epoch: 103 |  TrainLoss: 0.23053 | ValLoss: 0.30707 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24706 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 104 |  TrainLoss: 0.23605 | ValLoss: 0.30386 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24362 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 105 |  TrainLoss: 0.22473 | ValLoss: 0.29976 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24357 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 106 |  TrainLoss: 0.24409 | ValLoss: 0.29882 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.24546 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 107 |  TrainLoss: 0.22533 | ValLoss: 0.29366 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23774 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 108 |  TrainLoss: 0.20733 | ValLoss: 0.29215 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.23450 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 109 |  TrainLoss: 0.20938 | ValLoss: 0.28806 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23260 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 110 |  TrainLoss: 0.22763 | ValLoss: 0.28491 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23176 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 111 |  TrainLoss: 0.22172 | ValLoss: 0.28221 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23045 | TestAcc: 0.93548 | TestF1: 0.92\n",
            "Epoch: 112 |  TrainLoss: 0.20266 | ValLoss: 0.27943 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22740 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 113 |  TrainLoss: 0.18720 | ValLoss: 0.27718 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22491 | TestAcc: 0.91935 | TestF1: 0.90\n",
            "Epoch: 114 |  TrainLoss: 0.18312 | ValLoss: 0.27514 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22299 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 115 |  TrainLoss: 0.20323 | ValLoss: 0.27336 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22176 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 116 |  TrainLoss: 0.19784 | ValLoss: 0.26970 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22143 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 117 |  TrainLoss: 0.17851 | ValLoss: 0.26772 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22139 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 118 |  TrainLoss: 0.19656 | ValLoss: 0.26560 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21883 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 119 |  TrainLoss: 0.18280 | ValLoss: 0.26329 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21764 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 120 |  TrainLoss: 0.17757 | ValLoss: 0.26081 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21717 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 121 |  TrainLoss: 0.18537 | ValLoss: 0.25922 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21715 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 122 |  TrainLoss: 0.18359 | ValLoss: 0.25699 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21636 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 123 |  TrainLoss: 0.17867 | ValLoss: 0.25524 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21409 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 124 |  TrainLoss: 0.16799 | ValLoss: 0.25364 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21379 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 125 |  TrainLoss: 0.15745 | ValLoss: 0.25133 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21440 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 126 |  TrainLoss: 0.16254 | ValLoss: 0.24975 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21464 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 127 |  TrainLoss: 0.18426 | ValLoss: 0.24894 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21620 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 128 |  TrainLoss: 0.15784 | ValLoss: 0.24799 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21227 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 129 |  TrainLoss: 0.16011 | ValLoss: 0.24873 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21281 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 130 |  TrainLoss: 0.14553 | ValLoss: 0.24346 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21252 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 131 |  TrainLoss: 0.15019 | ValLoss: 0.24339 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.21710 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 132 |  TrainLoss: 0.16499 | ValLoss: 0.24060 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21376 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 133 |  TrainLoss: 0.14486 | ValLoss: 0.23933 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21299 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 134 |  TrainLoss: 0.15226 | ValLoss: 0.23970 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21349 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 135 |  TrainLoss: 0.12389 | ValLoss: 0.23634 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.21545 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 136 |  TrainLoss: 0.14694 | ValLoss: 0.24268 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22649 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 137 |  TrainLoss: 0.13432 | ValLoss: 0.23836 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22252 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 138 |  TrainLoss: 0.11770 | ValLoss: 0.23425 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21683 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 139 |  TrainLoss: 0.14394 | ValLoss: 0.23820 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21978 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 140 |  TrainLoss: 0.13313 | ValLoss: 0.23255 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.21862 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 141 |  TrainLoss: 0.12065 | ValLoss: 0.23553 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22535 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 142 |  TrainLoss: 0.13117 | ValLoss: 0.23342 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22513 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 143 |  TrainLoss: 0.12404 | ValLoss: 0.23059 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22210 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 144 |  TrainLoss: 0.11841 | ValLoss: 0.23143 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22315 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 145 |  TrainLoss: 0.10399 | ValLoss: 0.22901 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22420 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 146 |  TrainLoss: 0.11705 | ValLoss: 0.22874 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22726 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 147 |  TrainLoss: 0.09969 | ValLoss: 0.23027 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23162 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 148 |  TrainLoss: 0.11898 | ValLoss: 0.22734 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22819 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 149 |  TrainLoss: 0.10073 | ValLoss: 0.22911 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22931 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 150 |  TrainLoss: 0.12290 | ValLoss: 0.22665 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22998 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 151 |  TrainLoss: 0.10925 | ValLoss: 0.23436 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.24006 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 152 |  TrainLoss: 0.10805 | ValLoss: 0.23015 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23648 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 153 |  TrainLoss: 0.10316 | ValLoss: 0.22790 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23361 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 154 |  TrainLoss: 0.09896 | ValLoss: 0.22753 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23448 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 155 |  TrainLoss: 0.10455 | ValLoss: 0.22857 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23776 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.09851 | ValLoss: 0.22672 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23737 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 157 |  TrainLoss: 0.10720 | ValLoss: 0.22811 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23965 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 158 |  TrainLoss: 0.09428 | ValLoss: 0.22756 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24102 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 159 |  TrainLoss: 0.10553 | ValLoss: 0.22887 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.24527 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 160 |  TrainLoss: 0.08753 | ValLoss: 0.22932 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.24778 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 161 |  TrainLoss: 0.08989 | ValLoss: 0.22571 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24540 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 162 |  TrainLoss: 0.09456 | ValLoss: 0.23095 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25065 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 163 |  TrainLoss: 0.08615 | ValLoss: 0.22588 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24909 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 164 |  TrainLoss: 0.09332 | ValLoss: 0.23133 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.25758 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 165 |  TrainLoss: 0.07419 | ValLoss: 0.23065 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.25792 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 166 |  TrainLoss: 0.08057 | ValLoss: 0.22486 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25296 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 167 |  TrainLoss: 0.07990 | ValLoss: 0.22756 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25582 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 168 |  TrainLoss: 0.08013 | ValLoss: 0.22946 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25883 | TestAcc: 0.88710 | TestF1: 0.86\n",
            "Epoch: 169 |  TrainLoss: 0.08274 | ValLoss: 0.22595 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25768 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 170 |  TrainLoss: 0.06772 | ValLoss: 0.22593 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25921 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 171 |  TrainLoss: 0.07739 | ValLoss: 0.22617 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26157 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 172 |  TrainLoss: 0.07874 | ValLoss: 0.22643 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26432 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 173 |  TrainLoss: 0.07562 | ValLoss: 0.22665 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26659 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 174 |  TrainLoss: 0.07313 | ValLoss: 0.22561 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26779 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 175 |  TrainLoss: 0.07375 | ValLoss: 0.22594 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27018 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 176 |  TrainLoss: 0.07940 | ValLoss: 0.22626 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27219 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 177 |  TrainLoss: 0.07533 | ValLoss: 0.22672 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27464 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 178 |  TrainLoss: 0.07685 | ValLoss: 0.23029 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.27748 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 179 |  TrainLoss: 0.07402 | ValLoss: 0.23617 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.28202 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 180 |  TrainLoss: 0.06471 | ValLoss: 0.23175 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28035 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 181 |  TrainLoss: 0.06715 | ValLoss: 0.23160 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28219 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 182 |  TrainLoss: 0.05944 | ValLoss: 0.23240 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28433 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 183 |  TrainLoss: 0.06247 | ValLoss: 0.23581 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.28658 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 184 |  TrainLoss: 0.06581 | ValLoss: 0.23407 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28757 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 185 |  TrainLoss: 0.05521 | ValLoss: 0.23346 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.28910 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 186 |  TrainLoss: 0.05673 | ValLoss: 0.23390 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29114 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 187 |  TrainLoss: 0.06651 | ValLoss: 0.23456 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29280 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 188 |  TrainLoss: 0.04860 | ValLoss: 0.23492 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29470 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 189 |  TrainLoss: 0.06500 | ValLoss: 0.23496 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29623 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 190 |  TrainLoss: 0.05972 | ValLoss: 0.23549 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.29728 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 191 |  TrainLoss: 0.06332 | ValLoss: 0.23583 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29920 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 192 |  TrainLoss: 0.06162 | ValLoss: 0.23657 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30049 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 193 |  TrainLoss: 0.05543 | ValLoss: 0.23928 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30193 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 194 |  TrainLoss: 0.06173 | ValLoss: 0.23865 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30456 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 195 |  TrainLoss: 0.04924 | ValLoss: 0.23986 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30709 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 196 |  TrainLoss: 0.05384 | ValLoss: 0.24208 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.30934 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 197 |  TrainLoss: 0.06033 | ValLoss: 0.24423 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.31232 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 198 |  TrainLoss: 0.04898 | ValLoss: 0.24294 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.31704 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 199 |  TrainLoss: 0.05402 | ValLoss: 0.24651 | ValAcc: 0.90323 | ValF1: 0.92 | TestLoss: 0.31727 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "------Training for beta 0.9810572444331493---------\n",
            "cpu\n",
            "Number of Parameters of the model : 1656769\n",
            "Net(\n",
            "  (conv1): SAGEConv(768, 512, aggr=mean)\n",
            "  (conv2): SAGEConv(512, 512, aggr=mean)\n",
            "  (conv3): SAGEConv(512, 256, aggr=mean)\n",
            "  (full1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (full2): Linear(in_features=64, out_features=512, bias=True)\n",
            "  (full3): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (full4): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (dp1): Dropout(p=0.2, inplace=False)\n",
            "  (dp2): Dropout(p=0.2, inplace=False)\n",
            "  (dp3): Dropout(p=0.2, inplace=False)\n",
            ")\n",
            "Epoch: 00 |  TrainLoss: 0.69341 | ValLoss: 0.69418 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.69190 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 01 |  TrainLoss: 0.69300 | ValLoss: 0.69362 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.69214 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 02 |  TrainLoss: 0.69280 | ValLoss: 0.69322 | ValAcc: 0.41935 | ValF1: 0.00 | TestLoss: 0.69220 | TestAcc: 0.58065 | TestF1: 0.00\n",
            "Epoch: 03 |  TrainLoss: 0.69324 | ValLoss: 0.69260 | ValAcc: 0.61290 | ValF1: 0.60 | TestLoss: 0.69244 | TestAcc: 0.72581 | TestF1: 0.62\n",
            "Epoch: 04 |  TrainLoss: 0.69224 | ValLoss: 0.69220 | ValAcc: 0.64516 | ValF1: 0.72 | TestLoss: 0.69252 | TestAcc: 0.54839 | TestF1: 0.53\n",
            "Epoch: 05 |  TrainLoss: 0.69215 | ValLoss: 0.69176 | ValAcc: 0.67742 | ValF1: 0.75 | TestLoss: 0.69259 | TestAcc: 0.51613 | TestF1: 0.58\n",
            "Epoch: 06 |  TrainLoss: 0.69190 | ValLoss: 0.69127 | ValAcc: 0.70968 | ValF1: 0.78 | TestLoss: 0.69264 | TestAcc: 0.46774 | TestF1: 0.57\n",
            "Epoch: 07 |  TrainLoss: 0.69212 | ValLoss: 0.69071 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.69272 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 08 |  TrainLoss: 0.69051 | ValLoss: 0.69024 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.69275 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 09 |  TrainLoss: 0.69222 | ValLoss: 0.68970 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69286 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 10 |  TrainLoss: 0.69091 | ValLoss: 0.68925 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69286 | TestAcc: 0.45161 | TestF1: 0.60\n",
            "Epoch: 11 |  TrainLoss: 0.69006 | ValLoss: 0.68889 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69274 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 12 |  TrainLoss: 0.68972 | ValLoss: 0.68838 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69273 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 13 |  TrainLoss: 0.68837 | ValLoss: 0.68796 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69259 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 14 |  TrainLoss: 0.68753 | ValLoss: 0.68744 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69248 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 15 |  TrainLoss: 0.68812 | ValLoss: 0.68674 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69245 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 16 |  TrainLoss: 0.68735 | ValLoss: 0.68602 | ValAcc: 0.61290 | ValF1: 0.75 | TestLoss: 0.69237 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 17 |  TrainLoss: 0.68808 | ValLoss: 0.68536 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69219 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 18 |  TrainLoss: 0.68663 | ValLoss: 0.68467 | ValAcc: 0.64516 | ValF1: 0.77 | TestLoss: 0.69199 | TestAcc: 0.46774 | TestF1: 0.61\n",
            "Epoch: 19 |  TrainLoss: 0.68670 | ValLoss: 0.68401 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69173 | TestAcc: 0.48387 | TestF1: 0.62\n",
            "Epoch: 20 |  TrainLoss: 0.68488 | ValLoss: 0.68336 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69142 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 21 |  TrainLoss: 0.68436 | ValLoss: 0.68266 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69111 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 22 |  TrainLoss: 0.68315 | ValLoss: 0.68190 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69078 | TestAcc: 0.50000 | TestF1: 0.63\n",
            "Epoch: 23 |  TrainLoss: 0.68440 | ValLoss: 0.68105 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.69041 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 24 |  TrainLoss: 0.68446 | ValLoss: 0.68019 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68993 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 25 |  TrainLoss: 0.68310 | ValLoss: 0.67914 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68957 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 26 |  TrainLoss: 0.68186 | ValLoss: 0.67826 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68892 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 27 |  TrainLoss: 0.68005 | ValLoss: 0.67716 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68840 | TestAcc: 0.51613 | TestF1: 0.63\n",
            "Epoch: 28 |  TrainLoss: 0.67901 | ValLoss: 0.67613 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68767 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 29 |  TrainLoss: 0.68211 | ValLoss: 0.67523 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68661 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 30 |  TrainLoss: 0.67831 | ValLoss: 0.67391 | ValAcc: 0.67742 | ValF1: 0.78 | TestLoss: 0.68581 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 31 |  TrainLoss: 0.67721 | ValLoss: 0.67262 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68473 | TestAcc: 0.53226 | TestF1: 0.64\n",
            "Epoch: 32 |  TrainLoss: 0.67722 | ValLoss: 0.67120 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68372 | TestAcc: 0.54839 | TestF1: 0.65\n",
            "Epoch: 33 |  TrainLoss: 0.67352 | ValLoss: 0.66971 | ValAcc: 0.70968 | ValF1: 0.80 | TestLoss: 0.68283 | TestAcc: 0.54839 | TestF1: 0.65\n",
            "Epoch: 34 |  TrainLoss: 0.67417 | ValLoss: 0.66853 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.68141 | TestAcc: 0.53226 | TestF1: 0.63\n",
            "Epoch: 35 |  TrainLoss: 0.67258 | ValLoss: 0.66761 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67959 | TestAcc: 0.53226 | TestF1: 0.63\n",
            "Epoch: 36 |  TrainLoss: 0.66808 | ValLoss: 0.66631 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67800 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 37 |  TrainLoss: 0.66962 | ValLoss: 0.66477 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67641 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 38 |  TrainLoss: 0.66672 | ValLoss: 0.66324 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67465 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 39 |  TrainLoss: 0.66532 | ValLoss: 0.66101 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67336 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 40 |  TrainLoss: 0.66632 | ValLoss: 0.65913 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67156 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 41 |  TrainLoss: 0.66252 | ValLoss: 0.65677 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.67005 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 42 |  TrainLoss: 0.65756 | ValLoss: 0.65453 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66811 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 43 |  TrainLoss: 0.65609 | ValLoss: 0.65240 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66581 | TestAcc: 0.56452 | TestF1: 0.65\n",
            "Epoch: 44 |  TrainLoss: 0.65723 | ValLoss: 0.65056 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66282 | TestAcc: 0.62903 | TestF1: 0.68\n",
            "Epoch: 45 |  TrainLoss: 0.65095 | ValLoss: 0.64830 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.66009 | TestAcc: 0.64516 | TestF1: 0.69\n",
            "Epoch: 46 |  TrainLoss: 0.64932 | ValLoss: 0.64609 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.65702 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 47 |  TrainLoss: 0.64834 | ValLoss: 0.64338 | ValAcc: 0.74194 | ValF1: 0.81 | TestLoss: 0.65422 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 48 |  TrainLoss: 0.64140 | ValLoss: 0.64047 | ValAcc: 0.70968 | ValF1: 0.78 | TestLoss: 0.65138 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 49 |  TrainLoss: 0.63854 | ValLoss: 0.63701 | ValAcc: 0.74194 | ValF1: 0.81 | TestLoss: 0.64885 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 50 |  TrainLoss: 0.64163 | ValLoss: 0.63320 | ValAcc: 0.70968 | ValF1: 0.79 | TestLoss: 0.64650 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 51 |  TrainLoss: 0.63419 | ValLoss: 0.63018 | ValAcc: 0.70968 | ValF1: 0.78 | TestLoss: 0.64277 | TestAcc: 0.67742 | TestF1: 0.71\n",
            "Epoch: 52 |  TrainLoss: 0.63305 | ValLoss: 0.62703 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.63880 | TestAcc: 0.69355 | TestF1: 0.72\n",
            "Epoch: 53 |  TrainLoss: 0.62682 | ValLoss: 0.62420 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.63392 | TestAcc: 0.70968 | TestF1: 0.74\n",
            "Epoch: 54 |  TrainLoss: 0.62399 | ValLoss: 0.62114 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.62903 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 55 |  TrainLoss: 0.61622 | ValLoss: 0.61696 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.62510 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 56 |  TrainLoss: 0.61256 | ValLoss: 0.61272 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.62076 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 57 |  TrainLoss: 0.61125 | ValLoss: 0.60812 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.61654 | TestAcc: 0.77419 | TestF1: 0.78\n",
            "Epoch: 58 |  TrainLoss: 0.60527 | ValLoss: 0.60364 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.61182 | TestAcc: 0.79032 | TestF1: 0.79\n",
            "Epoch: 59 |  TrainLoss: 0.59562 | ValLoss: 0.59884 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.60721 | TestAcc: 0.79032 | TestF1: 0.79\n",
            "Epoch: 60 |  TrainLoss: 0.59653 | ValLoss: 0.59475 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.60118 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 61 |  TrainLoss: 0.58944 | ValLoss: 0.58981 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.59627 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 62 |  TrainLoss: 0.58001 | ValLoss: 0.58488 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.59071 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 63 |  TrainLoss: 0.57992 | ValLoss: 0.57951 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.58528 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 64 |  TrainLoss: 0.57065 | ValLoss: 0.57440 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.57910 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 65 |  TrainLoss: 0.57058 | ValLoss: 0.56911 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.57256 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 66 |  TrainLoss: 0.56356 | ValLoss: 0.56410 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.56505 | TestAcc: 0.80645 | TestF1: 0.81\n",
            "Epoch: 67 |  TrainLoss: 0.55941 | ValLoss: 0.55932 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.55711 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 68 |  TrainLoss: 0.55062 | ValLoss: 0.55359 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.55006 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 69 |  TrainLoss: 0.54126 | ValLoss: 0.54704 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.54367 | TestAcc: 0.83871 | TestF1: 0.83\n",
            "Epoch: 70 |  TrainLoss: 0.53568 | ValLoss: 0.54095 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.53656 | TestAcc: 0.85484 | TestF1: 0.85\n",
            "Epoch: 71 |  TrainLoss: 0.53232 | ValLoss: 0.53480 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.52924 | TestAcc: 0.87097 | TestF1: 0.86\n",
            "Epoch: 72 |  TrainLoss: 0.52033 | ValLoss: 0.52920 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.52049 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 73 |  TrainLoss: 0.51330 | ValLoss: 0.52318 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.51189 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 74 |  TrainLoss: 0.50926 | ValLoss: 0.51813 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.50237 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 75 |  TrainLoss: 0.50584 | ValLoss: 0.51077 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.49527 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 76 |  TrainLoss: 0.50009 | ValLoss: 0.50379 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.48796 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 77 |  TrainLoss: 0.48466 | ValLoss: 0.49708 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.48040 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 78 |  TrainLoss: 0.48897 | ValLoss: 0.49021 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.47291 | TestAcc: 0.88710 | TestF1: 0.88\n",
            "Epoch: 79 |  TrainLoss: 0.47452 | ValLoss: 0.48427 | ValAcc: 0.77419 | ValF1: 0.82 | TestLoss: 0.46311 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 80 |  TrainLoss: 0.45965 | ValLoss: 0.47789 | ValAcc: 0.80645 | ValF1: 0.84 | TestLoss: 0.45437 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 81 |  TrainLoss: 0.45527 | ValLoss: 0.47207 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.44437 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 82 |  TrainLoss: 0.44733 | ValLoss: 0.46553 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.43559 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 83 |  TrainLoss: 0.43481 | ValLoss: 0.45901 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.42696 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 84 |  TrainLoss: 0.43223 | ValLoss: 0.45242 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.41845 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 85 |  TrainLoss: 0.43118 | ValLoss: 0.44660 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.40919 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 86 |  TrainLoss: 0.41904 | ValLoss: 0.44056 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.40072 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 87 |  TrainLoss: 0.41104 | ValLoss: 0.43533 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.39193 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 88 |  TrainLoss: 0.39424 | ValLoss: 0.42878 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.38459 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 89 |  TrainLoss: 0.41379 | ValLoss: 0.42345 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.37610 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 90 |  TrainLoss: 0.39539 | ValLoss: 0.41763 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36907 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 91 |  TrainLoss: 0.37860 | ValLoss: 0.41243 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.36133 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 92 |  TrainLoss: 0.36940 | ValLoss: 0.40747 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.35339 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 93 |  TrainLoss: 0.37487 | ValLoss: 0.40329 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.34511 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 94 |  TrainLoss: 0.35613 | ValLoss: 0.39722 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33895 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 95 |  TrainLoss: 0.35283 | ValLoss: 0.39139 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.33349 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 96 |  TrainLoss: 0.32621 | ValLoss: 0.38662 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.32641 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 97 |  TrainLoss: 0.33521 | ValLoss: 0.38292 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31794 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 98 |  TrainLoss: 0.31906 | ValLoss: 0.37837 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.31162 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 99 |  TrainLoss: 0.33068 | ValLoss: 0.37387 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.30588 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 100 |  TrainLoss: 0.32236 | ValLoss: 0.37027 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29912 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 101 |  TrainLoss: 0.31639 | ValLoss: 0.36677 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.29297 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 102 |  TrainLoss: 0.29534 | ValLoss: 0.36222 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28812 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 103 |  TrainLoss: 0.28983 | ValLoss: 0.35845 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.28280 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 104 |  TrainLoss: 0.29408 | ValLoss: 0.35420 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.27905 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 105 |  TrainLoss: 0.28234 | ValLoss: 0.35055 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27541 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 106 |  TrainLoss: 0.29281 | ValLoss: 0.34758 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.26938 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 107 |  TrainLoss: 0.29283 | ValLoss: 0.34694 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26329 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 108 |  TrainLoss: 0.27419 | ValLoss: 0.34301 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26009 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 109 |  TrainLoss: 0.26311 | ValLoss: 0.33941 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.25733 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 110 |  TrainLoss: 0.24392 | ValLoss: 0.33585 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25794 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 111 |  TrainLoss: 0.25918 | ValLoss: 0.33428 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25880 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 112 |  TrainLoss: 0.25119 | ValLoss: 0.33061 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25135 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 113 |  TrainLoss: 0.25442 | ValLoss: 0.32865 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24612 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 114 |  TrainLoss: 0.23759 | ValLoss: 0.32704 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.24284 | TestAcc: 0.88710 | TestF1: 0.87\n",
            "Epoch: 115 |  TrainLoss: 0.23440 | ValLoss: 0.32367 | ValAcc: 0.80645 | ValF1: 0.83 | TestLoss: 0.24177 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 116 |  TrainLoss: 0.22356 | ValLoss: 0.32100 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24158 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 117 |  TrainLoss: 0.24119 | ValLoss: 0.31905 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24196 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 118 |  TrainLoss: 0.21764 | ValLoss: 0.31728 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24144 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 119 |  TrainLoss: 0.22948 | ValLoss: 0.31451 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23715 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 120 |  TrainLoss: 0.21242 | ValLoss: 0.31270 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23260 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 121 |  TrainLoss: 0.20066 | ValLoss: 0.31168 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.23007 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 122 |  TrainLoss: 0.21002 | ValLoss: 0.30978 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22884 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 123 |  TrainLoss: 0.19224 | ValLoss: 0.30780 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22878 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 124 |  TrainLoss: 0.19944 | ValLoss: 0.30674 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23112 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 125 |  TrainLoss: 0.19068 | ValLoss: 0.30497 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22984 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 126 |  TrainLoss: 0.21256 | ValLoss: 0.30271 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22572 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 127 |  TrainLoss: 0.18634 | ValLoss: 0.30134 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22419 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 128 |  TrainLoss: 0.18864 | ValLoss: 0.29985 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22353 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 129 |  TrainLoss: 0.19360 | ValLoss: 0.29814 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.22387 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 130 |  TrainLoss: 0.16779 | ValLoss: 0.29652 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22390 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 131 |  TrainLoss: 0.17316 | ValLoss: 0.29576 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22604 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 132 |  TrainLoss: 0.19855 | ValLoss: 0.29450 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22647 | TestAcc: 0.90323 | TestF1: 0.89\n",
            "Epoch: 133 |  TrainLoss: 0.18972 | ValLoss: 0.29246 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22436 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 134 |  TrainLoss: 0.17993 | ValLoss: 0.29073 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.22094 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 135 |  TrainLoss: 0.17788 | ValLoss: 0.29109 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21874 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 136 |  TrainLoss: 0.18232 | ValLoss: 0.28858 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21943 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 137 |  TrainLoss: 0.16467 | ValLoss: 0.28697 | ValAcc: 0.90323 | ValF1: 0.91 | TestLoss: 0.22271 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 138 |  TrainLoss: 0.16753 | ValLoss: 0.28733 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22643 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 139 |  TrainLoss: 0.15899 | ValLoss: 0.28676 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22746 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 140 |  TrainLoss: 0.16983 | ValLoss: 0.28377 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22256 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 141 |  TrainLoss: 0.16242 | ValLoss: 0.28315 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21972 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 142 |  TrainLoss: 0.16408 | ValLoss: 0.28431 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21846 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 143 |  TrainLoss: 0.15240 | ValLoss: 0.28228 | ValAcc: 0.87097 | ValF1: 0.88 | TestLoss: 0.21913 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 144 |  TrainLoss: 0.14634 | ValLoss: 0.27959 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22454 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 145 |  TrainLoss: 0.14733 | ValLoss: 0.28111 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23178 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 146 |  TrainLoss: 0.13479 | ValLoss: 0.27906 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.22968 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 147 |  TrainLoss: 0.15380 | ValLoss: 0.27716 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22620 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 148 |  TrainLoss: 0.14100 | ValLoss: 0.27846 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22123 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 149 |  TrainLoss: 0.12626 | ValLoss: 0.27975 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22115 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 150 |  TrainLoss: 0.13754 | ValLoss: 0.27708 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22321 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 151 |  TrainLoss: 0.13608 | ValLoss: 0.27696 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23053 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 152 |  TrainLoss: 0.12069 | ValLoss: 0.27934 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23672 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 153 |  TrainLoss: 0.13508 | ValLoss: 0.28012 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23868 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 154 |  TrainLoss: 0.11605 | ValLoss: 0.27794 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.23420 | TestAcc: 0.93548 | TestF1: 0.93\n",
            "Epoch: 155 |  TrainLoss: 0.12466 | ValLoss: 0.27685 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22745 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 156 |  TrainLoss: 0.12043 | ValLoss: 0.28440 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22580 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 157 |  TrainLoss: 0.14527 | ValLoss: 0.28727 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22721 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 158 |  TrainLoss: 0.11211 | ValLoss: 0.28151 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.22769 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 159 |  TrainLoss: 0.12100 | ValLoss: 0.27794 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23237 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 160 |  TrainLoss: 0.11208 | ValLoss: 0.27885 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23828 | TestAcc: 0.91935 | TestF1: 0.91\n",
            "Epoch: 161 |  TrainLoss: 0.10176 | ValLoss: 0.27871 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23708 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 162 |  TrainLoss: 0.11374 | ValLoss: 0.27985 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23446 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 163 |  TrainLoss: 0.10348 | ValLoss: 0.28306 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23429 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 164 |  TrainLoss: 0.12542 | ValLoss: 0.28660 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23564 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 165 |  TrainLoss: 0.10971 | ValLoss: 0.28405 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.23747 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 166 |  TrainLoss: 0.10731 | ValLoss: 0.28260 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24431 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 167 |  TrainLoss: 0.10090 | ValLoss: 0.28555 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.25133 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 168 |  TrainLoss: 0.10137 | ValLoss: 0.28604 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25122 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 169 |  TrainLoss: 0.10070 | ValLoss: 0.28603 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24600 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 170 |  TrainLoss: 0.09423 | ValLoss: 0.28999 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24546 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 171 |  TrainLoss: 0.10363 | ValLoss: 0.29702 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24782 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 172 |  TrainLoss: 0.10202 | ValLoss: 0.29434 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.24886 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 173 |  TrainLoss: 0.09640 | ValLoss: 0.29168 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25423 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 174 |  TrainLoss: 0.09953 | ValLoss: 0.29560 | ValAcc: 0.87097 | ValF1: 0.89 | TestLoss: 0.26131 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 175 |  TrainLoss: 0.08282 | ValLoss: 0.29494 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25745 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 176 |  TrainLoss: 0.08814 | ValLoss: 0.29916 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25572 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 177 |  TrainLoss: 0.09823 | ValLoss: 0.30851 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.25948 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 178 |  TrainLoss: 0.08484 | ValLoss: 0.30857 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26073 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 179 |  TrainLoss: 0.07322 | ValLoss: 0.30341 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26105 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 180 |  TrainLoss: 0.08159 | ValLoss: 0.30282 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26573 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 181 |  TrainLoss: 0.08946 | ValLoss: 0.30497 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26982 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 182 |  TrainLoss: 0.08639 | ValLoss: 0.30665 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26738 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 183 |  TrainLoss: 0.07939 | ValLoss: 0.31182 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26795 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 184 |  TrainLoss: 0.07918 | ValLoss: 0.31550 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.26980 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 185 |  TrainLoss: 0.08006 | ValLoss: 0.31779 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27147 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 186 |  TrainLoss: 0.07184 | ValLoss: 0.31797 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27316 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 187 |  TrainLoss: 0.06281 | ValLoss: 0.32028 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27481 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 188 |  TrainLoss: 0.07961 | ValLoss: 0.31977 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27784 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 189 |  TrainLoss: 0.06020 | ValLoss: 0.32188 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27964 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 190 |  TrainLoss: 0.07038 | ValLoss: 0.32689 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.27966 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 191 |  TrainLoss: 0.06787 | ValLoss: 0.33513 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28090 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 192 |  TrainLoss: 0.06597 | ValLoss: 0.34055 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28281 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 193 |  TrainLoss: 0.07364 | ValLoss: 0.33580 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28416 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 194 |  TrainLoss: 0.06502 | ValLoss: 0.33420 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28732 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 195 |  TrainLoss: 0.06511 | ValLoss: 0.33732 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28815 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 196 |  TrainLoss: 0.06428 | ValLoss: 0.34567 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28772 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 197 |  TrainLoss: 0.05613 | ValLoss: 0.35486 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.28917 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 198 |  TrainLoss: 0.05145 | ValLoss: 0.36268 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29142 | TestAcc: 0.90323 | TestF1: 0.88\n",
            "Epoch: 199 |  TrainLoss: 0.05350 | ValLoss: 0.36674 | ValAcc: 0.83871 | ValF1: 0.86 | TestLoss: 0.29345 | TestAcc: 0.90323 | TestF1: 0.88\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from scipy.stats import loguniform\n",
        "# sampling betas\n",
        "a = 1-1e-1\n",
        "b= 1-1e-3\n",
        "betas = loguniform.rvs(a, b, size=5)\n",
        "betas_data={}\n",
        "for i in betas:\n",
        "    print(\"------Training for beta {}---------\".format(i))\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net(test_data_pol.num_features,[best_trial['params']['layer_size1'],best_trial['params']['layer_size2'],best_trial['params']['layer_size3'],\n",
        "                                            best_trial['params']['layer_size4'],best_trial['params']['layer_size5'],best_trial['params']['layer_size6']],1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_trial['params']['learning_rate'],betas=(i,0.99))\n",
        "    lossff = torch.nn.BCELoss()\n",
        "    print(device)\n",
        "\n",
        "    print(\"Number of Parameters of the model :\",sum([param.nelement() for param in model.parameters()]))\n",
        "    print(model)\n",
        "    betas_data[i] = {\"val_loss\":[],\"test_loss\":[],'train_loss':[]}\n",
        "    for epoch in range(200):\n",
        "        train_loss = train(epoch)\n",
        "        test_loss, test_acc, test_f1 = test(epoch)\n",
        "        val_loss, val_acc, val_f1 = val(epoch)\n",
        "        betas_data[i]['val_loss'].append(val_loss)\n",
        "        betas_data[i]['test_loss'].append(test_loss)\n",
        "        betas_data[i]['train_loss'].append(train_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.5f} | '\n",
        "                f'ValLoss: {val_loss:.5f} | ValAcc: {val_acc:.5f} | ValF1: {val_f1:.2f} | '\n",
        "                f'TestLoss: {test_loss:.5f} | TestAcc: {test_acc:.5f} | TestF1: {test_f1:.2f}'\n",
        "                            )\n",
        "        # print(f'Epoch: {epoch:02d} |  TrainLoss: {train_loss:.7f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('beta.json') as f:\n",
        "    betas_data = json.dump(betas_data,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeVxUVf/A8c/MMMCw7zvijrgikgruueea5p5LmemT/dyylLJHM5c0U1vUci3TxCU1U9PctdwRN1DcwAVBRGVfBmbO7w9iHqcBRFNRO+/X6750zv3ec8+5wzBf7j33HoUQQiBJkiRJkvSCUJZ1AyRJkiRJkh4nmdxIkiRJkvRCkcmNJEmSJEkvFJncSJIkSZL0QpHJjSRJkiRJLxSZ3EiSJEmS9EKRyY0kSZIkSS8UmdxIkiRJkvRCkcmNJEmSJEkvFJncSJL0zIqLi0OhUPD9998/9LZ79+5FoVCwd+/eUsWtW7fu0RopSdIzRyY3kiRJkiS9UGRyI0mSJEnSC0UmN5IkFWvSpEkoFApOnz5Njx49sLe3x8nJiTFjxpCfn09MTAzt2rXD1taW8uXLM3PmTJM6rl27xuuvv46bmxsWFhYEBATwxRdfoNfrjeJu3rxJz549sbW1xd7enl69epGYmFhku44fP07nzp1xcnLC0tKSunXrsmbNmidyDAqdPXuWLl264OjoiKWlJYGBgfzwww9GMXq9nilTpuDv749Go8HBwYHatWvz5ZdfGmJu377N22+/ja+vLxYWFri6utKoUSN27tz5RNsvSf8mZmXdAEmSnn09e/bk9ddfZ+jQoezYsYOZM2eSl5fHzp07eeeddxg7diw//fQT48aNo3LlynTr1g0o+CIPDQ1Fq9Xy6aefUr58eTZv3szYsWO5fPky8+fPByA7O5tWrVpx8+ZNpk+fTtWqVdmyZQu9evUyacuePXto164dDRo04Ntvv8Xe3p7w8HB69epFVlYWgwYNeuz9j4mJITQ0FDc3N7766iucnZ1ZsWIFgwYN4tatW3zwwQcAzJw5k0mTJjFhwgSaNm1KXl4e58+fJyUlxVBX//79OXHiBFOnTqVq1aqkpKRw4sQJ7ty589jbLUn/WkKSJKkYEydOFID44osvjMoDAwMFINavX28oy8vLE66urqJbt26GsvHjxwtAHDlyxGj7//znP0KhUIiYmBghhBALFiwQgPjll1+M4oYMGSIAsWzZMkNZtWrVRN26dUVeXp5RbMeOHYWnp6fQ6XRCCCH27NkjALFnz54S+1gYt3bt2mJjevfuLSwsLMS1a9eMytu3by+srKxESkqKoQ2BgYEl7s/GxkaMGjWqxBhJkv4ZeVlKkqQH6tixo9HrgIAAFAoF7du3N5SZmZlRuXJlrl69aijbvXs31atXp379+kbbDxo0CCEEu3fvBgrOxtja2tK5c2ejuL59+xq9vnTpEufPn6dfv34A5OfnG5ZXXnmFhIQEYmJi/nmH/2b37t20bNkSX19fk35kZWVx6NAhAOrXr8+pU6d455132L59O2lpaSZ11a9fn++//54pU6Zw+PBh8vLyHnt7JenfTiY3kiQ9kJOTk9Frc3NzrKyssLS0NCnPyckxvL5z5w6enp4m9Xl5eRnWF/7r7u5uEufh4WH0+tatWwCMHTsWtVpttLzzzjsAJCcnP2z3Hqi0/QgLC2PWrFkcPnyY9u3b4+zsTMuWLTl+/Lhhm9WrVzNw4EAWL15MSEgITk5ODBgwoNjxRZIkPTyZ3EiS9MQ4OzuTkJBgUn7z5k0AXFxcDHGFicv9/v6FXxgfFhbGsWPHilwCAwMfcy9K3w8zMzPGjBnDiRMnuHv3LqtWreL69eu0bduWrKwsQ+zcuXOJi4vj6tWrTJ8+nfXr1z+RsUKS9G8lkxtJkp6Yli1bEh0dzYkTJ4zKly9fjkKhoEWLFgC0aNGC9PR0Nm3aZBT3008/Gb329/enSpUqnDp1iuDg4CIXW1vbJ9KP3bt3G5KZ+/thZWVFw4YNTbZxcHDgtddeY/jw4dy9e5e4uDiTmHLlyvHuu+/SunVrk2MkSdKjk3dLSZL0xIwePZrly5fToUMHJk+ejJ+fH1u2bGH+/Pn85z//oWrVqgAMGDCAOXPmMGDAAKZOnUqVKlXYunUr27dvN6nzu+++o3379rRt25ZBgwbh7e3N3bt3OXfuHCdOnGDt2rWP1NbDhw8XWd6sWTMmTpzI5s2badGiBf/9739xcnJi5cqVbNmyhZkzZ2Jvbw9Ap06dqFmzJsHBwbi6unL16lXmzp2Ln58fVapUITU1lRYtWtC3b1+qVauGra0tx44dY9u2bYY7zCRJegzKekSzJEnPrsK7pW7fvm1UPnDgQGFtbW0S36xZM1GjRg2jsqtXr4q+ffsKZ2dnoVarhb+/v/j8888NdzUVunHjhujevbuwsbERtra2onv37uLgwYMmd0sJIcSpU6dEz549hZubm1Cr1cLDw0O8/PLL4ttvvzXEPOzdUsUthdufOXNGdOrUSdjb2wtzc3NRp04dk3Z98cUXIjQ0VLi4uAhzc3NRrlw5MXjwYBEXFyeEECInJ0cMGzZM1K5dW9jZ2QmNRiP8/f3FxIkTRWZmZontlCSp9BRCCFFWiZUkSZIkSdLjJsfcSJIkSZL0QpHJjSRJkiRJLxSZ3EiSJEmS9EKRyY0kSZIkSS8UmdxIkiRJkvRCkcmNJEmSJEkvlH/dQ/z0ej03b97E1tYWhUJR1s2RJEmSJKkUhBCkp6fj5eWFUlnyuZl/XXJz8+ZNk5l9JUmSJEl6Ply/fh0fH58SY/51yU3hvDPXr1/Hzs6ujFsjSZIkSVJppKWl4evrW6r54/51yU3hpSg7OzuZ3EiSJEnSc6Y0Q0rkgGJJkiRJkl4oMrmRJEmSJOmFIpMbSZIkSZJeKP+6MTeSJEllQafTkZeXV9bNkKRnmrm5+QNv8y4NmdxIkiQ9QUIIEhMTSUlJKeumSNIzT6lUUqFCBczNzf9RPTK5kSRJeoIKExs3NzesrKzkw0MlqRiFD9lNSEigXLly/+izIpMbSZKkJ0Sn0xkSG2dn57JujiQ981xdXbl58yb5+fmo1epHrkcOKJYkSXpCCsfYWFlZlXFLJOn5UHg5SqfT/aN6ZHIjSZL0hMlLUZJUOo/rsyKTG0mSJEmSXigyuZEkSZIk6YUikxtJkiSpWPPnz6dChQpYWlpSr149Dhw4UGL8vHnzCAgIQKPR4O/vz/Lly4uNDQ8PR6FQ0LVrV6Py9PR0Ro0ahZ+fHxqNhtDQUI4dO1ZsPUOHDkWhUDB37lxDWVxcHAqFoshl7dq1JnXk5uYSGBiIQqHg5MmTRuuKquPbb781rJ80aVKRMdbW1kW2988//8TMzIzAwECj8vXr1xMcHIyDgwPW1tYEBgby448/mmz/oPdk0KBBJm1p2LDhQx2bvXv3Fhtz/3uxa9cuQkNDsbW1xdPTk3HjxpGfn2/Unu3bt9OwYUNsbW1xdXWle/fuxMbGFnlsHhtRxubNmyfKly8vLCwsRFBQkNi/f3+xsQMHDhSAyVK9evVS7y81NVUAIjU19XE038iZGykiPSfvsdcrSdLzKTs7W0RHR4vs7OyybsojCQ8PF2q1WixatEhER0eLkSNHCmtra3H16tUi4+fPny9sbW1FeHi4uHz5sli1apWwsbERmzZtMomNi4sT3t7eokmTJqJLly5G63r27CmqV68u9u3bJy5evCgmTpwo7OzsxI0bN0zq2bBhg6hTp47w8vISc+bMMZTn5+eLhIQEo+WTTz4R1tbWIj093aSeESNGiPbt2wtAREZGGq0DxLJly4zqysrKMqxPT0832Vf16tXFwIEDTfaTkpIiKlasKNq0aSPq1KljtG7Pnj1i/fr1Ijo6Wly6dEnMnTtXqFQqsW3bNkNMad6TgQMHinbt2hm1586dOw91bHJzc01i3nrrLVG+fHmh1+uFEEKcOnVKmJubi08++URcvHhR7N27V1SrVk289957hn1dvnxZWFhYiLCwMHHp0iUREREhmjZtKgIDA02OjRAlf2Ye5vu7TJObh/3gpKSkGB3o69evCycnJzFx4sRS7/NJJTcZOXnCb9xm4Tdus2g4bad4ffFhMWnTWbHicJw4cuWOuJOR+1j3J0nSs+95T27q168vhg0bZlRWrVo1MX78+CLjQ0JCxNixY43KRo4cKRo1amRUlp+fLxo1aiQWL14sBg4caJTcZGVlCZVKJTZv3my0TZ06dcRHH31kVHbjxg3h7e0tzp49K/z8/IySm6IEBgaKN99806R869atolq1aiIqKqrY5GbDhg0l1n2/kydPCqDIP9Z79eolJkyYICZOnGiS3BSlbt26YsKECYbXpXlP/n5MS6O4Y1NIq9UKNzc3MXnyZENZWFiYCA4ONorbsGGDsLS0FGlpaUIIIdauXSvMzMyETqczxGzatEkoFAqh1WpN9vO4kpsyfc7N7NmzGTx4MG+99RYAc+fOZfv27SxYsIDp06ebxNvb22Nvb294vXHjRu7du8cbb7zx1NpcnMS0HFxsLEjOyCUhNYeE1BwOXEw2irG1NMPH0QofRw3eDhp8HDWG176OVthpzORdFZL0ghNCkJ33z25zfVQatarUv2O0Wi0RERGMHz/eqLxNmzYcPHiwyG1yc3OxtLQ03qdGw9GjR8nLyzM8t2Ty5Mm4uroyePBgk0sq+fn56HS6Iuv5448/DK/1ej39+/fn/fffp0aNGg/sT0REBCdPnmTevHlG5bdu3WLIkCFs3LixxFv23333Xd566y0qVKjA4MGDefvtt4udJmDx4sVUrVqVJk2aGJUvW7aMy5cvs2LFCqZMmVJie4UQ7N69m5iYGGbMmAE83Huyd+9e3NzccHBwoFmzZkydOhU3N7ci91Xcsbnfpk2bSE5OZtCgQYay4t7vnJwcIiIiaN68OcHBwahUKpYtW8agQYPIyMjgxx9/pE2bNv/oOTYPUmbJzaN8cP5uyZIltGrVCj8/v2JjcnNzyc3NNbxOS0t7tAY/QCVXG45PaEVqVh6Xbqdz8VYGl5IyuHQ7g4u3MohPySY9J59zCWmcSyi6DbYWZnjYW+Jhb4m7nSUedpa42xf862FnibONOU7W5liqVU+kD5IkPXnZeTqq/3d7mew7enJbrMxL92s/OTkZnU6Hu7u7Ubm7uzuJiYlFbtO2bVsWL15M165dCQoKIiIigqVLl5KXl0dycjKenp78+eefLFmyxGRcSyFbW1tCQkL49NNPCQgIwN3dnVWrVnHkyBGqVKliiJsxYwZmZmaMGDGiVP1ZsmQJAQEBhIaGGsqEEAwaNIhhw4YRHBxMXFxckdt++umntGzZEo1Gw65du3jvvfdITk5mwoQJJrG5ubmsXLnS5Lvt4sWLjB8/ngMHDmBmVvx7kJqaire3N7m5uahUKubPn0/r1q2B0r8n7du3p0ePHvj5+REbG8vHH3/Myy+/TEREBBYWFqU6NkXFtG3bFl9fX0NZ27ZtmTt3LqtWraJnz54kJiYakraEhAQAypcvz++//06PHj0YOnQoOp2OkJAQtm7dWuy+HocyS24e5YNzv4SEBH777Td++umnEuOmT5/OJ5988o/a+jDsrdTU83Oinp+TUXm2Vkd8ShbX72Vz4142N+5l/fVvNvH3skjO0JKem096UgYXkzJK3IeVuQpHK3OcbcwL/rU2x9G6IPFxsi4oc7U1x9XGEhdb81L/MpMkSfq7v5/pEUIUe/bn448/JjExkYYNGyKEwN3dnUGDBjFz5kxUKhXp6em8/vrrLFq0CBcXl2L3+eOPP/Lmm2/i7e2NSqUiKCiIvn37cuLECaDgTMOXX37JiRMnSnUmKjs7m59++omPP/7YqPzrr78mLS2NsLCwEre/P4kpHAQ8efLkIpOb9evXk56ezoABAwxlOp2Ovn378sknn1C1atUS92Vra8vJkyfJyMhg165djBkzhooVK9K8eXNDzIPek169ehn+X7NmTYKDg/Hz82PLli1069bNaNvijs39bty4wfbt21mzZo1ReZs2bfj8888ZNmwY/fv3x8LCgo8//pg//vgDlargj/DExETeeustBg4cSJ8+fUhPT+e///0vr732Gjt27HhiVyvK/FvvYT449/v+++9xcHAwGWX/d2FhYYwZM8bwOi0tzSjzfFo05ioqu9lS2c22yPWFyU9Cag630nK5lZZDYmoOiWk53PpruZupJU8nyNLqyNJmE5+SXap9W5urcLW1+N9iY/G31wVJkLO1BeZm8gY6SXqSNGoV0ZPbltm+S8vFxQWVSmXyx2ZSUpLJH6WG+jUali5dynfffcetW7fw9PRk4cKF2Nra4uLiwunTp4mLi6NTp06GbfR6PQBmZmbExMRQqVIlKlWqxL59+8jMzCQtLQ1PT0969epFhQoVADhw4ABJSUmUK1fOUI9Op+O9995j7ty5Jmdg1q1bR1ZWllHCAbB7924OHz5scjYjODiYfv368cMPPxTZz4YNG5KWlsatW7dMjsXixYvp2LEjHh4ehrL09HSOHz9OZGQk7777rqHfQgjMzMz4/fffefnll4GCiSMrV64MFCRS586dY/r06TRv3vyR3hMAT09P/Pz8uHjxosm64o7N/ZYtW4azszOdO3c2WTdmzBhGjx5NQkICjo6OxMXFERYWZniv5s2bh52dHTNnzjRss2LFCnx9fTly5IjRXVyPU5klN4/6JkFBArR06VL69+//wJlDLSwsijwN9yQkXU3Dwc0Kc83DH9YHJT9Q0O+M3HzuZmpNlywtdzO03MvScidTy50MLUnpOeTk6cnU6si8k0XcnawHtsPRSo2rrQUVXKyp6WVPTW97anjb4WZr+cBtJUl6MIVC8VycTTU3N6devXrs2LGDV1991VC+Y8cOunTpUuK2arUaHx8foOB2744dO6JUKqlWrRpnzpwxip0wYQLp6el8+eWXJn94WltbY21tzb1799i+fbvhC7J///60atXKKLZt27b079+/yDGYS5YsoXPnzri6uhqVf/XVV0ZjX27evEnbtm1ZvXo1DRo0KLZ/kZGRWFpa4uDgYFQeGxvLnj172LRpk1G5nZ2dSb/nz5/P7t27WbdunSERKIoQwjC04lHfkzt37nD9+nU8PT1N1hV3bO7f/7JlyxgwYECxY2QUCgVeXl4ArFq1Cl9fX4KCggDIysoynMUpVPi6MLF9EsrsE/ZPPjj79u3j0qVLDB48+Ek3s9S0OfmsnX4cAFsnS5y8rLF1tsTOWYOztzUuvrZY2f2zKdwVCgW2lmpsLdX4ORf9/IT7CSHI1Oq4nZ5735LD7YyC/ydnaA3lyRm55OsF97LyuJeVx4VbGWyPumWoy83Wgjq+DgT6OlDX14FaPvbYWj65wWCSJJW9MWPG0L9/f4KDgwkJCWHhwoVcu3aNYcOGAQVnxuPj4w3Psrlw4QJHjx6lQYMG3Lt3j9mzZ3P27FnDGRBLS0tq1qxptI/CBOH+8u3btyOEwN/fn0uXLvH+++/j7+9vSFycnZ1NJiJVq9V4eHjg7+9vVH7p0iX2799f5BiP+8/8ANjY2ABQqVIlQ3L266+/kpiYSEhICBqNhj179vDRRx/x9ttvm/zhvHTpUjw9PWnfvr1RuVKpNOm3m5ubyfGYPn06wcHBVKpUCa1Wy9atW1m+fDkLFiwwxDzoPcnIyGDSpEl0794dT09P4uLi+PDDD3FxcTH6rn3QsSm0e/duYmNji/2+/fzzz2nXrh1KpZL169fz2WefsWbNGkMC06FDB+bMmcPkyZMNl6U+/PBD/Pz8qFu3brH7/afK9M+Hh/3gFFqyZAkNGjQw+WEpS5kpuWhsBdnpCtLv5pB+N8ckxsLaDHsXDfZuVti7arBz0WDvpsHeVYOVnfljv/aoUCiwsTDDxsKMCi4lJ0N6vSAlO4/b6QWXxC7cSudsfCpRN9O4fDuDpPRcdkTfYkf0rb/qhipuNtTxceCl8k60qOaGq+3TOUMmSdLT0atXL+7cucPkyZNJSEigZs2abN261XATR0JCAteuXTPE63Q6vvjiC2JiYlCr1bRo0YKDBw9Svnz5h9pvamoqYWFh3LhxAycnJ7p3787UqVMf6e6apUuX4u3tTZs2bR56WyhImubPn8+YMWPQ6/VUrFiRyZMnM3z4cKM4vV7P999/z6BBg0zOVJRWZmYm77zzDjdu3ECj0VCtWjVWrFhhNIbmQe+JSqXizJkzLF++nJSUFDw9PWnRogWrV6/G1tb4ykBpjs2SJUsIDQ0lICCgyPW//fYbU6dOJTc3lzp16vDLL78YJXcvv/wyP/30EzNnzmTmzJlYWVkREhLCtm3b0Gg0j3ScSkMhhBBPrPZSmD9/PjNnzjS8SXPmzKFp06ZAwVMW4+Li2Lt3ryE+NTUVT09PvvzyS4YMGfLQ+0tLS8Pe3p7U1FTs7OweVzfIy8nhq4GvobbUoLF1Qm1ph0KhQaczR5tjRm62CgUaUFiiUFoW/Kso/FeJucYMRw8rnDytcfSwxtGz4P+2TpYolGV7e3i2VkfUzVROXk8h8noKJ6+lmIz3USgg0NeBVgHutK7uThU3G3lbu/Svl5OTQ2xsrOFpspIklaykz8zDfH+XeXLztD2p5OZ29BmWf1LyiPuiKUChQaG0QqGwBqUVCoWV4bVKbY21ky12zrbYu9nj4GaHg6cdjh622LvYoDIrm9vCb6fncvJ6Ciev3+OPi8mcupFqtL6ckxWtAtxpFeBGcHknOVBZ+leSyY0kPRyZ3DyiJ3bm5moM59u/So7ajGy1GblqFXkqFXkqJXlmKrQqZcFrM6VR+T+nRKFUoVKZoTJTo7F3wM7VCVsnZ6wcHLF1csbR0xtHT2/sXFxRFPPQqX8qMTWHXedvsTP6Fn9evoM2/38DxazNVYRWdqFZVVeaVXXFx1Ejz+pI/woyuZGkh/O4kptnf8j+cyLdLJs1/ZSQrwVdNnoB6BXohYJ8oUAnFJjng1W+Hk0+WOYJbPPAPE+FOl+FOleFmdYMdGbo9CryUKNVmZGrNidPpUKnVKJTgkAPRnmBHqHXk6/PIz8vm9zsNFISrxXZRjMzMxzc3XH0LoeTty92ru7YODph7eCItYMjVvYOKB/xWrGHvSX9GvjRr4Efmbn5HLiYzM5zt9gbc5vkDOPxOoVjgNrWcGdQowrYWMgfQ0mSJOnxkd8qj4neyYeNPgpA/dfy0DUA2r+WAmb5Arss/lr++n8m2GWDbZYCa60DljpnVMIJlA5o1U5kWDmQpwJEJkKfidCnIXT3EPoU8vPzSY6PJzk+HjhUZCts7GywdXHH1s0TW2cXHD298KhUFZdy5VGV8FTN+1lbmNGupgftanqg1wuiE9LYd+E2e2OSOHEthYzcfM7Ep3ImPpUlf8QyuHEF+tQvh7ONHJAsSZIk/XPystRjkqvLZXvcdlQKFSqlCjOFmeH/GjMNGjMNOqEjJz+HXF0u2fnZ5OpyycnPITs/mzRtGmm5aQX/atPIystCq9OSq88t+Ff3v39zdbnk6/NN2mCWL/BOhkpJTvje88M+tzxmigpkW/miVygKEh393YJkR3cHZV4yCl0aQmSTr9QjSrhUZKY2w71CZTyqBuBZ2R/PKlWxdXZ96MtLufk6rt/N4sS1FL7de5kryZkAmJsp6VTbi4GhftT2cXioOiXpWSUvS0nSw5Fjbh7Rk0punjadXodWr0Wr05KmTeNa2jXi0uKITY0lLi2OuNQ4bmXdAiFwSVMTcNMb94wK2Gn9MBcVQOVoVJ8QAjPtbawyLmCRdQ2V9hYqsxSyrJTcNdOgLeIkn7WDIx6V/XErXxFbFxcc3Dzwrlaj1Je28nV6Np9OYOmfsZy+b0ByHV8H/tOsIm2qe6As4zvFJOmfkMmNJD0cmdw8ohcluSmNrLwsEjITuJtzlzvZd7iTc4fk7GSupl0lPjEJxS0NdikeeKWWxznLB+XfL6cJPTYZ8TiknMcq7Txwk0xHFfcsLbmnsERgOjjZ3s2doFe6UKtFG9QP8cv85PUUlh+MY/PpBLS6gsHIAZ52DGtWkXY1PbAoo7vCJOmfkMmNJD0cmdw8on9TclMaebo8krKTuHInlitXbpIUm052PFgm2WOdZzz5pyo/G5c7Z3C5cxa7lBiyzfJJd1ST46Aiz15DUp4NOdqCxMTK3oEGXXtQu1V7zB4wRcb9kjNy+f7POL4/GEdGbsGlN0crNT2DfRncuAJudvILQnp+yORGkh6OTG4ekUxuSic1N5Vd5/YRE3WdzDiwT/TCKs/eKMY2LRb3pEhckyOxzLlDnm8eub6WnNZ6kppX8ENprtFQrmYgFQLrUT4wCDsXt1LtPyVLyw8HrxJ+7BoJqQVPezY3U9Ir2JcRLavIpyFLzwWZ3EjSw5HJzSOSyc2j0eZriTxznqjj17hzMRerNOMxO7ZpcbjdjsT5ThRWFtfIqGjJKTxIzzf+4XT2KUfDbr3wD21aqsHIOr1gz/kkvt13meNX7wEFt5KPaFmZASHlsXyIWY4l6WmTyY0kPZzHldzIx8ZKpWJuZk6DurV5c0hH3p/Znc6TAvB9RY3aR4tQCNLtynO50qscrT+BQ/6fkXSvE8GXs+igP09Dx6t4OQgUCgV3blxjy1efs27KBG5fjX3gflVKBa2qu7N2WAirhjSkjo89Gbn5TNt6npDpu5i29ZzJVBCSJD0+8+fPN3zR1KtXjwMHDpQYP2/ePAICAtBoNPj7+5vMDXi/8PBwFAoFXbt2NSovX748CoXCZPn7fE7nzp2jc+fO2NvbY2trS8OGDY3mumrevLlJHb17937gvsaPH29Y//333xfZFoVCQVJSkiFOCMGsWbOoWrUqFhYW+Pr6Mm3aNMP6hIQE+vbti7+/P0qlklGjRpkcj6ioKLp3725o09y5c0s61EyfPh2FQmFS1/r162nbti0uLi4oFApOnjxpsm1iYiL9+/fHw8MDa2trgoKCWLdunWH93r17i+33sWPHDHHXrl2jU6dOWFtb4+LiwogRI9BqtUb72r59Ow0bNsTW1hZXV1e6d+9ObOyDf///E/I5N9Ij8fXwxLezJ3SGrDQtV07eJvr4dW5dSiPX0olr5Vpzzbclznej8Dq7n/q2J7Bx1XGhfAsiLqdy7ewpln/wf/jVrstLnbvjVyuwxP0pFApCKjmz4Z1GrI+MZ86OC8SnZLNw/xVWHr7KlFdr8mpdn6fTeUn6l1i9ejWjRo1i/vz5NGrUiO+++4727dsTHR1tMqM2wIIFCwgLC2PRokW89NJLHD16lCFDhuDo6EinTp2MYq9evcrYsWNp0qSJST3Hjh1Dp9MZXp89e5bWrVvTo0cPQ9nly5dp3LgxgwcP5pNPPsHe3p5z586Z/LU/ZMgQJk+ebHhd1GSNkydPNpqrsHB2cCiYqLJdu3ZG8YMGDSInJwc3t/9dZh85ciS///47s2bNolatWqSmppKcnGxYn5ubi6urKx999BFz5swxaQNAVlYWFStWpEePHowePbrImELHjh1j4cKF1K5d22RdZmYmjRo1okePHsXOwdi/f39SU1PZtGkTLi4u/PTTT/Tq1Yvjx49Tt25dQkNDSUhIMNrm448/ZufOnQQHBwMFE6V26NABV1dX/vjjD+7cucPAgQMRQvD1118DcOXKFbp06cKYMWNYuXIlqampjB49mm7duhEZGVliH/8R8S+TmpoqAJGamlrWTXkhaXPyxKade8RnH68Q3wzdZVgWDwgX21oNF6dq1BOnQgLE2n5txBe9OopZPTuIWT07iLVTJojEK5dKvZ98nV7siEoU3eb/KfzGbRZ+4zaL0eGRIj0n7wn2TpIeTnZ2toiOjhbZ2dll3ZRHUr9+fTFs2DCjsmrVqonx48cXGR8SEiLGjh1rVDZy5EjRqFEjo7L8/HzRqFEjsXjxYjFw4EDRpUuXEtsxcuRIUalSJaHX6w1lvXr1Eq+//nqJ2zVr1kyMHDmyxBg/Pz8xZ86cEmPul5SUJNRqtVi+fLmhLDo6WpiZmYnz58+Xqo5/2q709HRRpUoVsWPHjhLrio2NFYCIjIw0WWdtbW3UByGEcHJyEosXLy6yLq1WK9zc3MTkyZMNZVu3bhVKpVLEx8cbylatWiUsLCwM37Fr164VZmZmQqfTGWI2bdokFAqF0Gq1Jvsp6TPzMN/f8rKU9FipLczo1LI54yb34+X3/VDVSUGnzCFH48qlSt34M2QKl116USEqm2bRcVTMSEOpgKunI1kRNoqdi+eRnZH+wP0UXq5aMzSEMa2rolTA+sh4Onx1gFPXU558RyXpUQkB2syyWR5iiKVWqyUiIoI2bdoYlbdp04aDBw8WuU1ubq7JmRONRsPRo0fJy8szlE2ePBlXV1cGDx5cqnasWLGCN9980zBOT6/Xs2XLFqpWrUrbtm1xc3OjQYMGbNy40WT7lStX4uLiQo0aNRg7dizp6aa/X2bMmIGzszOBgYFMnTrV5LLK/ZYvX46VlRWvvfaaoezXX3+lYsWKbN68mQoVKlC+fHneeust7t69+8D+PYrhw4fToUMHWrVq9ch1NG7cmNWrV3P37l30ej3h4eHk5ubSvHnzIuM3bdpEcnIygwYNMpQdOnSImjVr4uXlZShr27Ytubm5REREABAcHIxKpWLZsmXodDpSU1P58ccfadOmDWr1ozzNv3TkZSnpiQmoVImA/1QiT6tjx84jnN0Xj3WqMze9mnDTsxHOd6PwTDyC941zXPa046a9Lad2/MaFw3/S9PU3qdH05QdO9KlSKhjRsgqhlZwZGX6Sq3ey6L7gIF/0rEOXQO+n1FNJegh5WTDN68FxT8KHN8HculShycnJ6HQ63N3djcrd3d1JTEwscpu2bduyePFiunbtSlBQEBERESxdupS8vDySk5Px9PTkzz//ZMmSJUWOAynKxo0bSUlJMfpSTUpKIiMjg88++4wpU6YwY8YMtm3bRrdu3dizZw/NmjUDoF+/flSoUAEPDw/Onj1LWFgYp06dYseOHYa6Ro4cSVBQEI6Ojhw9epSwsDBiY2NZvHhxke1ZunQpffv2Nbq8deXKFa5evcratWtZvnw5Op2O0aNH89prr7F79+5S9bO0wsPDOXHihNG4l0exevVqevXqhbOzM2ZmZlhZWbFhwwYqVapUZPySJUto27Ytvr6+hrLExESTnw9HR0fMzc0NPyPly5fn999/p0ePHgwdOhSdTkdISAhbt279R+1/EJncSE+c2lzFK6+E0q6dnp1HDnL0t8s4Jvlyx7kWd5xrodTl4njvApVSIrhlnURGehrbF8zlzO7faT1kOC6+fg/cR3B5J7aOaMK4n0+zLSqR0atPolQo6FSnjL5EJOkF8fe7GoUQxd7p+PHHH5OYmEjDhg0RQuDu7s6gQYOYOXMmKpWK9PR0Xn/9dRYtWoSLi0up9r9kyRLat29vdHZAry94nlaXLl0MY1MCAwM5ePAg3377rSG5uX+8Sc2aNalSpQrBwcGcOHGCoKAgAKOxLbVr18bR0ZHXXnvNcDbnfocOHSI6OtpkkLReryc3N5fly5dTtWpVQ7vr1atHTEwM/v7+perrg1y/ft0wtuef3n03YcIE7t27x86dO3FxcWHjxo306NGDAwcOUKtWLaPYGzdusH37dtasWWNST1E/C/f/jCQmJvLWW28xcOBA+vTpQ3p6Ov/973957bXX2LFjx0NP4VNaMrmRnhqlUkmbkMa0btiIzSd+Z/vvh/FJqIGt1ok7LrXApRaK/Gyc7m0lVX2NmzHRrPxwDC0H/4eazR98+tXeSs38fkGM+/k0ayNuMGr1SQTQWSY40rNEbVVwBqWs9l1KLi4uqFQqk7M0SUlJJn+tF9JoNCxdupTvvvuOW7du4enpycKFC7G1tcXFxYXTp08TFxdnNLi4MFExMzMjJibG6MzB1atX2blzJ+vXrzdpm5mZGdWrVzcqDwgI4I8//ii2T0FBQajVai5evGhIbv6uYcOGAFy6dMkkuVm8eDGBgYHUq1fPqNzT0xMzMzNDYlPYFii4m+hxJTcREREkJSUZ7V+n07F//36++eYbcnNzUZVi+pvLly/zzTffcPbsWWrUqAFAnTp1OHDgAPPmzePbb781il+2bBnOzs507tzZqNzDw4MjR44Yld27d4+8vDzDz8i8efOws7Nj5syZhpgVK1bg6+vLkSNHDMf7cZPJjfTUKRQKOtVrS9OaDfn6xNdsPhOBa3J5/JPq4ZDjRZZrd8z06ajT15OjvcP2BXO5fvYUjXoPwM7FtcS6lUoFM7rXRgDrIm4wMjySbG0+vV4yvbNDksqEQlHqS0NlydzcnHr16rFjxw5effVVQ/mOHTvo0qVLiduq1Wp8fAruXgwPD6djx44olUqqVavGmTNnjGInTJhAeno6X375pdElDyj4UnVzc6NDhw4mbXvppZeIiYkxKr9w4QJ+fsWf6Y2KiiIvLw9PT89iYwrv4Pl7TEZGBmvWrGH69Okm2zRq1Ij8/HwuX75sSM4uXLgAUGJ7HlbLli1Njt8bb7xBtWrVGDduXKkSGyi4KwsK/uC8n0qlMiSbhYQQLFu2jAEDBpiMkQkJCWHq1KkkJCQYjtfvv/+OhYWFIQHLysoyaVfh67/v63GSyY1UZuwt7JkQMoHc+rkcuHGA+Sfnk35NS5voZlgp6iLsBmCWc5T8nINEH9jD+YMHqPVyGxr3HoDlfbdq/l1hgqNWKVl19Brjfj5DRq6OwY0rPMXeSdLzb8yYMfTv35/g4GBCQkJYuHAh165dY9iwYQCEhYURHx9vuExz4cIFjh49SoMGDbh37x6zZ8/m7Nmz/PDDDwBYWlpSs2ZNo304ODgAmJTr9XqWLVvGwIEDMTMz/ap6//336dWrF02bNqVFixZs27aNX3/9lb179wIFZydWrlzJK6+8gouLC9HR0bz33nvUrVuXRo0aAQWXmQ4fPkyLFi2wt7fn2LFjjB49ms6dO5vc6r569Wry8/Pp16+fSVtatWpFUFAQb775JnPnzkWv1zN8+HBat25tdDancJxRRkYGt2/f5uTJk5ibmxvOQGm1WqKjow3/j4+P5+TJk9jY2FC5cmVsbW1NjpO1tTXOzs5G5Xfv3uXatWvcvFlwhrAwCfTw8MDDw4Nq1apRuXJlhg4dyqxZs3B2dmbjxo3s2LGDzZs3G9W/e/duYmNjixz83aZNG6pXr07//v35/PPPuXv3LmPHjmXIkCGGh+x16NCBOXPmMHnyZMNlqQ8//BA/Pz/q1q1rUudj88D7qV4w8lbwZ1eeLk+siF4hmoY3FX0+aS5+fG2ymDdku/hq8I9idt+hhtvGv3tnkLgefeaB9en1ejFlc5ThVvEvd14wupVUkp605/1WcCGEmDdvnvDz8xPm5uYiKChI7Nu3z7Bu4MCBolmzZobX0dHRIjAwUGg0GmFnZye6dOnywNuji7sVfPv27QIQMTExxW67ZMkSUblyZWFpaSnq1KkjNm7caFh37do10bRpU+Hk5CTMzc1FpUqVxIgRI8SdO3cMMREREaJBgwbC3t5eWFpaCn9/fzFx4kSRmZlpsq+QkBDRt2/fYtsSHx8vunXrJmxsbIS7u7sYNGiQ0b6EEAIwWfz8/AzrC2/d/vty/zH+u6JuBV+2bFmR9UycONEQc+HCBdGtWzfh5uYmrKysRO3atU1uDRdCiD59+ojQ0NBi93/16lXRoUMHodFohJOTk3j33XdFTk6OUcyqVatE3bp1hbW1tXB1dRWdO3cW586dK7K+x3UruJx+QXrm5OnzOHTzEIuPz8d/0zUCktuS4NUEXd41dFnb0evTUSiUNOrdn/pdXitxQJoQgq92XWLOzoJTxEObVmR8+2pPbBCbJN1PTr8gSQ9HTr8gvbDUSjVNfZrybYelXO9bm0VN1+IS/xVWOhvUdgNRmldHCD1/rPqB7Qvmkn/f8zP+TqFQMLJVFSZ0KBjc993+K3y169LT6ookSZJUBmRyIz2zrNRWfN3ia+o06MjU7peIdpiOd8IBLDStMdO8DCiI2reLNZ+MJ+VW0c/dKPRWk4pM7lJwV8CcnRdYfexaifGSJEnS80smN9IzTa1SM73xdEbUG8XOunl80e4Xyt2ZhlOuHWqbVwFzEi7GsPyD/yP6wJ4S6xoQUp7hLQruZPhww1l2n7/1FHogSZIkPW0yuZGeeQqFgrdqvcXMpjNJtVYQ1jkZD/t5VL4RhbltPxRm3uTlZPPbN19w8UjRj4UvNLaNP92DfNDpBcNXRnJSTtUgSZL0wpHJjfTcaFehHSODRgLwfqjA9qXzvHRmCXaqFqgs6gCw5evZ3ImPL7YOhULBZ91r0bSqK9l5Ot78/hixyZlPpf2SJEnS0yGTG+m5MrjmYDpX6oxO6HinyjX+7KGi4ZnP8E6zRqHyQpeXw4oPJxF3pvhLTmqVkgX9gqjlbc/dTC1Dlh8nS5v/FHshSZIkPUkyuZGeKwqFgkmhkxhSawgqhYrvXeOYOsSW2kkrqX3jLgosyM9JYOPnX7Fl3ilSb2cVWY+1hRlLBgXjZmvBpaQMJv8a/ZR7IkmSJD0pMrmRnjtqpZoRQSMI7xhOebvynLVO4b133CjvdJbAqwVP5NTlnuJyxEHCpxwj6kA8RT3Oyc3Wkrm9AlEoIPzYdTadKqP5fiRJkqTHSiY30nOrmlM1lrZdSnm78lzVJTO0txu5wUlUun0XgPzM7Wizkti7MoYt80+TmZprUkdoZRfebVEZgA/WnWLb2YSn2gdJkiTp8ZPJjfRcc7VyZUnbJZS3K09S7j0+eMmKJR1TsNJmI8gnL3U5+Tl/Enc6gfDJR7kSedukjpEtq9Cymhs5eXqGrTjBgr2XizzTI0mSJD0fZHIjPffcrNxY02kNHzb4EG8bb+KcVfzQJhmLvGyEQpCffQRd1iqy09P47bszHP31ilHyYqZS8l3/egwMKZi9d8a28/x0VD7kT5IA5s+fb3gUfr169Thw4ECJ8fPmzSMgIACNRoO/v79hUs2ihIeHo1Ao6Nq1q1H5pEmTUCgURouHh4dRjBCCSZMm4eXlhUajoXnz5kRFRRW5HyEE7du3R6FQsHHjRqN15cuXN9nX+PHjjWJ27dpFaGgotra2eHp6Mm7cOPLzjW9CWLNmDYGBgVhZWeHn58fnn3/+0McmKiqK7t27G9o0d+5ckzqmT5/OSy+9hK2tLW5ubnTt2tVkdnSAc+fO0blzZ+zt7bG1taVhw4Zcu/a/32uXL1/m1VdfxdXVFTs7O3r27MmtW6Y3YmzZsoUGDRqg0WhwcXGhW7duRutHjhxJvXr1sLCwIDAw0GT7nJwcBg0aRK1atTAzMzN5r58UmdxILwSNmYY+1fqw+dXNbOi8gUV1+pPQMI4qiYlY5OWTr72D2uw3hMjj2JY4dv9wDl2+3rC9mUrJJ11q8l7rghl8J/8azbmEtLLqjiQ9E1avXs2oUaP46KOPiIyMpEmTJrRv397oS/J+CxYsICwsjEmTJhEVFcUnn3zC8OHD+fXXX01ir169ytixY2nSpEmRddWoUYOEhATDcubMGaP1M2fOZPbs2XzzzTccO3YMDw8PWrduTXp6ukldc+fOLXE+ucmTJxvta8KECYZ1p0+f5pVXXqFdu3ZERkYSHh7Opk2bjBKg3377jX79+jFs2DDOnj3L/PnzDW17mGOTlZVFxYoV+eyzz0ySuUL79u1j+PDhHD58mB07dpCfn0+bNm3IzPzfIy0uX75M48aNqVatGnv37uXUqVN8/PHHhrmaMjMzadOmDQqFgt27d/Pnn3+i1Wrp1KkTev3/fi/+/PPP9O/fnzfeeINTp07x559/0rdvX6P2CCF488036dWrV5Ht1el0aDQaRowYQatWrYp9Dx67B06t+YKRs4L/e2RuHiP+b1JVcTCwlpjbrZ2Y1bOD+HH8R+KbYbvEN0N3iY1zToicrDyjbXQ6vRi09IjwG7dZvDxrj8jIySumdkl6sOd9VvD69euLYcOGGZVVq1ZNjB8/vsj4kJAQMXbsWKOykSNHikaNGhmV5efni0aNGonFixcXOSv4xIkTRZ06dYptl16vFx4eHuKzzz4zlOXk5Ah7e3vx7bffGsWePHlS+Pj4iISEBAGIDRs2GK338/MTc+bMKXZfYWFhIjg42Khsw4YNwtLSUqSlpQkhCmbOfu2114xi5syZI3x8fIRerxdClP7YlLZdhZKSkgRgNFt7r169xOuvv17sNtu3bxdKpdLoe/Du3bsCEDt27BBCCJGXlye8vb3F4sWLH9gGIR78nglR/Azw93tcs4LLMzfSC8uq7TRGe7rwS5N8guMSUegFt66cpEpQPGoLFTfO32P95xGk380xbKNUKviiZyDudhZcvp3JlC3nyrAH0otICEFWXlaZLOIhxpJptVoiIiJo06aNUXmbNm04eLDoJ4Hn5uaazOSs0Wg4evQoefdNcDt58mRcXV0ZPHhwsfu/ePEiXl5eVKhQgd69e3PlyhXDutjYWBITE43aZmFhQbNmzYzalpWVRZ8+ffjmm2+KPRMCMGPGDJydnQkMDGTq1KlotdoH9iknJ4eIiIgSY27cuMHVq1cf6tg8rNTUVACcnJwA0Ov1bNmyhapVq9K2bVvc3Nxo0KCB0eW43NxcFAoFFhYWhjJLS0uUSiV//PEHACdOnCA+Ph6lUkndunXx9PSkffv2xV76e9aYlXUDJOmJMbOgQvfl+OS241QC1Ii/zVlfN6L2rOOVEVM5uCGFuzcz2Tj7BN3er4e1fcEH3cnanDm9Aum76Airjl6jUx1PQiu5lHFnpBdFdn42DX5qUCb7PtL3CFZqq1LFJicno9PpcHd3Nyp3d3cnMbHoiWrbtm3L4sWL6dq1K0FBQURERLB06VLy8vJITk7G09OTP//8kyVLlnDy5Mli992gQQOWL19O1apVuXXrFlOmTCE0NJSoqCicnZ0N+y+qbYXJBMDo0aMJDQ2lS5cuxe5r5MiRBAUF4ejoyNGjRwkLCyM2NpbFixcb+jR37lxWrVpFz549SUxMZMqUKQAkJCQYYkaPHs2gQYNo0aIFly5dMoyXSUhIoHz58qU6Ng9LCMGYMWNo3LgxNWvWBCApKYmMjAw+++wzpkyZwowZM9i2bRvdunVjz549NGvWjIYNG2Jtbc24ceOYNm0aQgjGjRuHXq839KkwmZw0aRKzZ8+mfPnyfPHFFzRr1owLFy4YkqlnlTxzI73YnCvxRpuvWN9KoDVPxz01E71Ox58r5tB1dC3sXCxJS85h05cnycn8319PoZVc6NegHADjfz4jn2As/Wv9fayKEKLY8Ssff/wx7du3p2HDhqjVarp06cKgQYMAUKlUpKen8/rrr7No0SJcXIr/g6F9+/Z0796dWrVq0apVK7Zs2QLADz/8UOq2bdq0id27dxc5KPd+o0ePplmzZtSuXZu33nqLb7/9liVLlnDnzh2g4EzV559/zrBhw7CwsKBq1ap06NDB0CeAIUOG8O6779KxY0fMzc1p2LAhvXv3Nop50LF5FO+++y6nT59m1apVhrLCMTNdunRh9OjRBAYGMn78eDp27Mi3334LgKurK2vXruXXX3/FxsYGe3t7UlNTCQoKMrSlsJ6PPvqI7t27U69ePZYtW4ZCoWDt2rWP1N6nSZ65kV541v7tGXxtB9N6/c5Hq5NIsfLl3u1bHP7hSzqPfJ/1syK4ezOTzd+couvoupiZF3y4x7evxp7zSVy7m8UXv1/g447Vy7gn0otAY6bhSN8jZbbv0nJxcUGlUpmcpUlKSjI5Y2KoX6Nh6dKlfPfdd9y6dQtPT08WLlyIra0tLi4unD59mri4ODp16mTYpvBL1MzMjJiYGCpVqmRSr7W1NbVq1eLixYsAhktMiYmJRmc87m/b7t27uXz5Mg4ODkZ1de/enSZNmrB3794i+9CwYUMALl26hLOzMwBjxoxh9OjRJCQk4OjoSFxcHGFhYVSoUAEoSLJmzJjBtGnTSExMxNXVlV27dgEFd2OV5tg8rP/7v/9j06ZN7N+/Hx8fH0O5i4sLZmZmVK9u/PsqICDAcMkJCpK2y5cvk5ycjJmZGQ4ODnh4eBj6VHhc76/HwsKCihUrFjug/Fkiz9xI/wqvtZyBk709n/ZRUDktEYQg+sRRruz6mc4jArGwMuNWbBq7fjiH0BeMS7C1VDO1Wy0Alv4Zy4lr98qyC9ILQqFQYKW2KpOlpDuG/s7c3Jx69eqxY8cOo/IdO3YQGhpa4rZqtRofHx9UKhXh4eF07NgRpVJJtWrVOHPmDCdPnjQsnTt3pkWLFpw8eRJfX98i68vNzeXcuXOGL9wKFSrg4eFh1DatVsu+ffsMbRs/fjynT5822hfAnDlzWLZsWbFtj4yMBDC5TKRQKAy3na9atQpfX1+CgoKMYlQqFd7e3pibm7Nq1SpCQkJwc3Mr1bEpLSEE7777LuvXr2f37t2GZKSQubk5L730ksnt4RcuXMDPz8+kPhcXFxwcHNi9ezdJSUl07twZwHB79/315OXlERcXV2Q9zxp55kb6V1Ar1YwI/S9j941l2mt5jNuYzHl7V/auX0WPcuVpP7QWm746yaWIJBzcrWjQuSIALfzd6BbkzfoT8Xyw7jRbRjTGwuzRTiFL0vNmzJgx9O/fn+DgYEJCQli4cCHXrl1j2LBhAISFhREfH294XsuFCxc4evQoDRo04N69e8yePZuzZ88aLidZWloaxoYUKjyzcn/52LFj6dSpE+XKlSMpKYkpU6aQlpbGwIEDgYJEY9SoUUybNo0qVapQpUoVpk2bhpWVleFWZQ8PjyIHEZcrV86QEBw6dIjDhw/TokUL7O3tOXbsGKNHj6Zz586UK1fOsM3nn39Ou3btUCqVrF+/ns8++4w1a9YYLuEkJyezbt06mjdvTk5ODsuWLWPt2rXs27fPUMeDjg0UJGjR0dGG/8fHx3Py5ElsbGyoXLngSerDhw/np59+4pdffsHW1tZwZs3e3h6NpuDM3Pvvv0+vXr1o2rQpLVq0YNu2bfz6669GZ6uWLVtGQEAArq6uHDp0iJEjRzJ69Gj8/f0BsLOzY9iwYUycOBFfX1+jZ/f06NHDUM+lS5fIyMggMTGR7OxsQxJZvXp1zM3NAYiOjkar1XL37l3S09MNMUU9F+exeeD9VC8YeSv4v5derxd9NvcRNb+vKWbMqChWdGwqZvXsIL7t/orITU4W0X/Gi2+GFtwmHrE9zrDdvcxcUe/THcJv3Gbx+bbzZdgD6XnzvN8KLoQQ8+bNE35+fsLc3FwEBQUZ3XI8cOBA0axZM8Pr6OhoERgYKDQajbCzsxNdunQR58+X/Jkp6vbgXr16CU9PT6FWq4WXl5fo1q2biIqKMorR6/Vi4sSJwsPDQ1hYWIimTZuKM2fOlLgv/nYreEREhGjQoIGwt7cXlpaWwt/fX0ycOFFkZmYabdeiRQtDTIMGDcTWrVuN1t++fVs0bNhQWFtbCysrK9GyZUtx+PBho5jSHJvY2FgBmCz3H+Oi1gNi2bJlRnUtWbJEVK5cWVhaWoo6deqIjRs3Gq0fN26ccHd3F2q1WlSpUkV88cUXhtvWC2m1WvHee+8JNzc3YWtrK1q1aiXOnj1rFNOsWbMi2xMbG2uI8fPzKzKmKI/rVnDFXwfrXyMtLc0weMrOzq6smyM9ZccTj/PG9jdQoWD+hWQiztdGa2ZGLbU1rb9fydHfrnF8SxwA9dr50aBLRRQKBb+dSeA/K0+gUirY9G4janjZl21HpOdCTk4OsbGxhif8SpJUspI+Mw/z/S3H3Ej/KsEewTT3aY4OwdCqziT7Fcw1FZ2TzpXJk2nQqSIhrxYMaIzYdpX94RcQekH7Wp68UssDnV7wwbrT5On0Je1GkiRJKkMyuZH+daY0nkL3Kt0xU5ixvraWHItsdColh47s597qNQS19aNZX39QwNl98ez8PhqdTs8nnWviYKUm6mYaC/dfefCOJEmSpDIhkxvpX8fewp5JoZPY2m0rVTVubKt/DxDccrAh4qvZZB49Ss2m3rR+szpKpYILR2+x7buzOFqa8d+/bgf/cudFLtwyncNGkiRJKnsyuZH+tTxtPJnbfhk6m3wiK6cAEOXpyMXRo8lLSKDqSx60/08tVGolcaeT2fzNKToEuPNyNTe0Oj0jw0+Sm68r205IkiRJJmRyI/2r+dqVY9pL4zldOY3b9jnkq1SccLDk5ocfIoSgfC0XOv1fHdSWKuJjUtg09ySftKuGk7U55xLS+HxbzIN3IkmSJD1VMrmR/vWa1+hLT7sq7Kt7B51Szz0bDdEXo0kJDwfAu6ojXUfXxdJaTdLVdA58F820dgEALP4jlgMXb5dl8yVJkqS/kcmNJAGj2i/E2jKfowEFTyG+4OHE1dmz0f71mHE3PztefS8Ia3tz7iVkcvuXawwMLHjk+eRfo9Hp/1VPVJAkSXqmyeRGkgAbKxf+W2sYF8plkGyXS75KxTkHKxI+/i+Fj4Jy8rKm2/v1sHPVkJacQ424PBws1VxMyuCXk/Fl3ANJkiSpkExuJOkvTYOH01btzKGadxEIbjrZcv3sKVLXrzfE2Llo6DyiDuYaM27HpTHE3hGAOTsvoM2Xz76RJEl6FsjkRpLuM7LVV6TZ5xJTLgOAU+Xcuf755+Tf/t+4GntXK1oOLBhzQ0w6wSoLrt/NZvXx62XRZEmSJOlvZHIjSffxca9NH5dgjle7R7aVlhxzMyIdNSRMmWoUVzHQlbqtCybWa5GmwkGn4OtdF8nWylvDpRfL/PnzDY/Cr1evHgcOHCgxft68eQQEBKDRaPD39zdMqnm/lJQUhg8fjqenJ5aWlgQEBLB169ZH3u/QoUNRKBTMnTvXZN2hQ4d4+eWXsba2xsHBgebNm5OdnW0Us2XLFho0aIBGo8HFxYVu3boZrVcoFCbLt99+a1gfExNDixYtcHd3x9LSkooVKzJhwgTy8vIe6tgsWrSIJk2a4OjoiKOjI61ateLo0aNGMeXLly+yPcOHDzeKO3fuHJ07d8be3h5bW1saNmzItb/GEJbm2MTFxTF48GAqVKiARqOhUqVKTJw4Ea1Wa1THyJEjDTOIFzURZk5ODoMGDaJWrVqYmZnRtWtXk5gnQc4KLkl/83bLOWxY3Yzfg5Lp+qcnt+2sOX3iMA67dmHbsqUhrkHXiiTGppJwKZXXFJZ8n5bND4fiGNasUhm2XpIen9WrVzNq1Cjmz59Po0aN+O6772jfvj3R0dFGs2YXWrBgAWFhYSxatIiXXnqJo0ePMmTIEBwdHenUqRNQMNt169atcXNzY926dfj4+HD9+nVsbW0fab8bN27kyJEjeHl5mbTn0KFDtGvXjrCwML7++mvMzc05deoUSuX//q7/+eefGTJkCNOmTePll19GCMGZM2dM6lq2bBnt2rUzvLa3/9/8cmq1mgEDBhAUFISDgwOnTp1iyJAh6PV6pk2bVupjs3fvXvr06UNoaCiWlpbMnDmTNm3aEBUVhbe3NwDHjh1Dp/vfH1Fnz56ldevWRjN1X758mcaNGzN48GA++eQT7O3tOXfunNFcTQ86NufPn0ev1/Pdd99RuXJlzp49y5AhQ8jMzGTWrFmGeoQQvPnmmxw5coTTp0+bHDedTodGo2HEiBH8/PPPJuufmAdOrfmCkbOCS6Xx/ZHPRc3va4rXJ9UXs3p2ELNfay9ONm8m8tPSjOIy7uWIJWP3i2+G7hJv/992UXvSdpGarS2jVkvPmud9VvD69euLYcOGGZVVq1ZNjB8/vsj4kJAQMXbsWKOykSNHikaNGhleL1iwQFSsWFFotcV/Tkq73xs3bghvb29x9uxZ4efnJ+bMmWO0vkGDBmLChAnF7icvL094e3uLxYsXFxsjhOls4qUxevRo0bhxY8Pr0hybv8vPzxe2trbihx9+KDZm5MiRolKlSkYzevfq1Uu8/vrrJbbvQcemKDNnzhQVKlQoct3EiRNFnTp1Sty+qBng/+5xzQouL0tJUhH6Bo+ksqULJ/2y0NllolcqOWsOSbNnG8VZO1jQZnANFAqorTXDPVXHYjnvlFQCIQT6rKwyWYQo/SMLtFotERERtGnTxqi8TZs2HDx4sMhtcnNzTWZy1mg0HD161HCJZtOmTYSEhDB8+HDc3d2pWbMm06ZNM5yNKO1+9Xo9/fv35/3336dGjRombUlKSuLIkSO4ubkRGhqKu7s7zZo1448//jDEnDhxgvj4eJRKJXXr1sXT05P27dsTFRVlUt+7776Li4sLL730Et9++y16ffE3EFy6dIlt27bRrFmzhzo2f5eVlUVeXh5OTk5FrtdqtaxYsYI333wThUJhOC5btmyhatWqtG3bFjc3Nxo0aMDGjRsf6tgUJTU1tdi2PGvkZSlJKoJaqea/zb9gwLaBbK6dSpc/rEh0sOHypo3Yd+yIVb16hlifak7UbevHiW1XaZ1tzk/7YhkQWh4XG4sy7IH0rBLZ2cQE1Xtw4BPgfyIChZVVqWKTk5PR6XS4u7sblbu7u5OYmFjkNm3btmXx4sV07dqVoKAgIiIiWLp0KXl5eSQnJ+Pp6cmVK1fYvXs3/fr1Y+vWrVy8eJHhw4eTn5/Pf//731Lvd8aMGZiZmTFixIgi23LlSsEfGZMmTWLWrFkEBgayfPlyWrZsydmzZ6lSpYpRzOzZsylfvjxffPEFzZo148KFC4Yv8k8//ZSWLVui0WjYtWsX7733HsnJyUyYMMFon6GhoZw4cYLc3FzefvttJk+e/FDH5u/Gjx+Pt7c3rVq1KrKPGzduJCUlhUGDBhnKkpKSyMjI4LPPPmPKlCnMmDGDbdu20a1bN/bs2UOzZs1KdWz+7vLly3z99dd88cUXRbblWSPP3EhSMeq6B9G9Yifu2eVxwycVgGhvF+I//i/63Fyj2PodK+DiY4OVUNAsVcWCPZfKosmS9NgVnhEoJIQwKSv08ccf0759exo2bIharaZLly6GL16VSgUUnFlwc3Nj4cKF1KtXj969e/PRRx+xYMGCUu83IiKCL7/8ku+//77YthSeWRk6dChvvPEGdevWZc6cOfj7+7N06VKjmI8++oju3btTr149li1bhkKhYO3atYa6JkyYQEhICIGBgbz33ntMnjyZzz//3GSfq1ev5sSJE/z0009s2bLFaGxKaY7N/WbOnMmqVatYv369yRmfQkuWLKF9+/ZG440K+9SlSxdGjx5NYGAg48ePp2PHjoZB0KU5Nve7efMm7dq1o0ePHrz11ltFtuVZI8/cSFIJRtcfx8FrezhQLZ1eibakayyIvZ6Ew3ff4XrfX4wqMyWt3qzO6qnHqJivYvfeG9xsUhEvB00Ztl56Fik0GvxPRJTZvkvLxcUFlUplcpYmKSnJ5KxKIY1Gw9KlS/nuu++4desWnp6eLFy4EFtbW1xcXADw9PRErVYbfaEHBASQmJiIVqst1X4PHDhAUlKS0eBinU7He++9x9y5c4mLizOcCalevbpRPQEBAYa7hoqKsbCwoGLFiiZ3Ft2vYcOGpKWlcevWLaNj4evra6hPp9Px9ttv895776FSqUp1bArNmjWLadOmsXPnTmrXrl1kG65evcrOnTtZf99zuKDgfTMzMyuy34WXnUpzbArdvHmTFi1aEBISwsKFC4s9Js8aeeZGkkpgb2HPnFbzEGodx6sUnL254OFE4uLF5F68aBTr7GVD6KsFd0o1yTRjwa/nn3p7pWefQqFAaWVVJktxZzmKYm5uTr169dixY4dR+Y4dOwgNDS1xW7VajY+PDyqVivDwcDp27Gi4C6dRo0ZcunTJaMzKhQsX8PT0xNzcvFT77d+/P6dPn+bkyZOGxcvLi/fff5/t27cDBbdMe3l5ERNjPLnthQsX8PPzAzDcwnx/TF5eHnFxcYaYokRGRmJpaYmDg0OxMUII8vLyTMY5lXRsAD7//HM+/fRTtm3bRnBwcLH1L1u2DDc3Nzp06GBUbm5uzksvvVRiv0tzbADi4+Np3rw5QUFBLFu2zKidz7wHDjl+wubNmyfKly8vLCwsRFBQkNi/f3+J8Tk5OeLDDz8U5cqVE+bm5qJixYpiyZIlpd6fvFtKehQbfn9P1F5aU/x3UBsxq2cHsbFpiLj29lCTOL1OL5ZPOyK+GbpLTHhnh7iUmFZEbdK/xfN+t1R4eLhQq9ViyZIlIjo6WowaNUpYW1uLuLg4IYQQ48ePF/379zfEx8TEiB9//FFcuHBBHDlyRPTq1Us4OTmJ2NhYQ8y1a9eEjY2NePfdd0VMTIzYvHmzcHNzE1OmTCn1fotS1N1Sc+bMEXZ2dmLt2rXi4sWLYsKECcLS0lJcunTJEDNy5Ejh7e0ttm/fLs6fPy8GDx4s3NzcxN27d4UQQmzatEksXLhQnDlzRly6dEksWrRI2NnZiREjRhjqWLFihVi9erWIjo4Wly9fFmvWrBHe3t6iX79+D3VsZsyYIczNzcW6detEQkKCYUlPTzfql06nE+XKlRPjxo0r8lisX79eqNVqsXDhQnHx4kXx9ddfC5VKJQ4cOFDqYxMfHy8qV64sXn75ZXHjxg2j9tzv4sWLIjIyUgwdOlRUrVpVREZGisjISJGbm2uIiYqKEpGRkaJTp06iefPmhpiiPK67pco0uSn8AV60aJGIjo4WI0eOFNbW1uLq1avFbtO5c2fRoEEDsWPHDhEbGyuOHDki/vzzz1LvUyY30iPJ14qx39UUHaYX3Bo+57X24kSNGiLj0GGT0PS72WLuO7vEN0N3icnTS/+zKb14nvfkRoiCP0D9/PyEubm5CAoKEvv27TOsGzhwoGjWrJnhdXR0tAgMDBQajUbY2dmJLl26iPPnz5vUefDgQdGgQQNhYWEhKlasKKZOnSry8/NLvd+iFJXcCCHE9OnThY+Pj7CyshIhISFGX/BCCKHVasV7770n3NzchK2trWjVqpU4e/asYf1vv/0mAgMDhY2NjbCyshI1a9YUc+fOFXl5eYaY8PBwERQUJGxsbIS1tbWoXr26mDZtmtH7Xppj4+fnJwCTZeLEiUZx27dvF4CIiYkp9ngsWbJEVK5cWVhaWoo6deqIjRs3PtSxWbZsWZFt+fs5kWbNmhUZc3/SVly/ivK4khuFEA9xb+Bj1qBBA4KCgowGkgUEBNC1a1emT59uEr9t2zZ69+7NlStXHvl2tLS0NOzt7UlNTcXOzu6R2y79+9zcO5VOV1bR6ogHbimW+NxJo76DO+XXrEbxt9O1O7ddIWZjHHoEoW/XoF6QRxm1WipLOTk5xMbGGp60K0lSyUr6zDzM93eZXUB7lGcobNq0ieDgYGbOnIm3tzdVq1Zl7NixJo/Svl9ubi5paWlGiyQ9Cq8G7zAgM5tjASkA3HCyJenSBdK2bDGJbdWuInedzVCiYP+P58nLldMySJIkPS1lltw8yjMUrly5wh9//MHZs2fZsGEDc+fOZd26dSZzatxv+vTp2NvbG5bC0eyS9NA0jgz2a4/OLos4j0xQKDjv6cytKVPJu3XLJLzjGzVIVwjMs/Vs+cH0oWCSJEnSk1HmQ58f5hkKer0ehULBypUrqV+/Pq+88gqzZ8/m+++/L/bsTVhYGKmpqYbl+nU5c7P06GwaDuc/91KJ8E9BrxAk21mRqNNy84NxCJ3x2Zm6lZ1JCrBGIIg/kUz0HzfLqNWSJEn/LmWW3DzKMxQ8PT3x9vY2mrAsICAAIQQ3btwochsLCwvs7OyMFkl6ZG4BdHeph515Duf80gG46OVC5pEj3Cni4VdvvVadA5b5AOxdFcOtOHlZVJIk6Ukrs+TmUZ6h0KhRI27evElGRoah7MKFCyiVSnx8fJ5oeyWpkLrNFIanpHG2Uio6pSBFY849a0uS5y9Al5pqFFvdyw7HIGcumukQOsH2hWfR5uSXUcslSZL+Hcr0stSYMWNYvHgxS5cu5dy5c4wePZpr164xbNgwoOCS0oABAwzxffv2xdnZmTfeeIPo6Gj279/P+++/z5tvvonmIZ68KUn/iGdtXqn+Ot6KHC55FyTaceW9EdnZpKz72SR8ZOuqbLXWkqrUk343h8MbLj/tFkuSJP2rlGly06tXL+bOncvkyZMJDAxk//79bN261fCExISEBKNHQdvY2LBjxw5SUlIIDg6mX79+dOrUia+++qqsuiD9S6le/ogR2QqiKqQhECSaQbqFmnsrVyLyjc/MBHja0aq2B9s1BTP/ntkXz82LKWXQakmSpH+HMn3OTVmQz7mRHhcRuZL+Rz/BMcqb8res8cnIpfblG3h/+SV2bY0fcRCTmE67L/fTOlNNHa0Z9m4aek+oj5m56YR50otDPudGkh7Oc/+cG0l63ilqvcbIbDhbqWCQcLyNBRnmau4uX24S6+9hyyu1PNmrySPPXEFqUjZHf4192k2WJEn6V5DJjSQ9KjMLXqrVn6rmqVx3y0IAF72cyY6IIPvkSZPwUS2rkKeEX9U5AJzceU3ePSVJkvQEyORGkv6J4Df4v5Q0TlRNQSBIsLcmVWNO8rffmYRWcbelU20vLqv13HMxQwjYvfwcunx9ERVL0rNh/vz5hksE9erV48CBAyXGz5s3j4CAADQaDf7+/iwv4kzm3Llz8ff3R6PR4Ovry+jRo8nJyTGs379/P506dcLLywuFQsHGjRtN6hBCMGnSJLy8vNBoNDRv3pyoKOOHZTZv3hyFQmG09O7d27B+7969JusLl2PHjgFw6tQp+vTpg6+vLxqNhoCAAL788sti+3/p0iVsbW1LnDH8zz//xMzMjMDAwGJjwsPDUSgUdO3a1ah8wYIF1K5d2/Bok5CQEH777TejmEmTJlGtWjWsra1xdHSkVatWHDlyxChm4cKFNG/eHDs7OxQKBSkpKcW2JTc3l8DAQBQKBSf/9ofbrl27CA0NxdbWFk9PT8aNG0f+feMOY2JiaNGiBe7u7lhaWlKxYkUmTJhAXl5esft7HGRyI0n/hL0PNSu0xtc8gytemQDEeDqTsXcvOefPm4SPaFkFpQJWatNRW5lx92YmJ7ZffdqtlqRSWb16NaNGjeKjjz4iMjKSJk2a0L59e6MbPe63YMECwsLCmDRpElFRUXzyyScMHz6cX3/91RCzcuVKxo8fz8SJEzl37hxLlixh9erVhIWFGWIyMzOpU6cO33zzTbFtmzlzJrNnz+abb77h2LFjeHh40Lp1a9LT043ihgwZQkJCgmH57rv//eERGhpqtC4hIYG33nqL8uXLExwcDEBERASurq6sWLGCqKgoPvroI8LCwopsW15eHn369KFJkybFtjs1NZUBAwbQsmXLYmOuXr3K2LFji6zHx8eHzz77jOPHj3P8+HFefvllunTpYpTYVa1alW+++YYzZ87wxx9/UL58edq0acPt27cNMVlZWbRr144PP/yw2HYU+uCDD/Dy8jIpP336NK+88grt2rUjMjKS8PBwNm3axPjx4w0xarWaAQMG8PvvvxMTE8PcuXNZtGgREydOfOB+/5EHTq35gpGzgkuP3eU9Yv3nnqLh/EAxs9crYlbPDuLPunXE9VGjigwfueqE8Bu3WYyd9af4ZugusWD4HnHvVuZTbrT0NDzvs4LXr19fDBs2zKisWrVqYvz48UXGh4SEiLFjxxqVjRw5UjRq1Mjwevjw4eLll182ihkzZoxo3LhxkXUCYsOGDUZler1eeHh4iM8++8xQlpOTI+zt7cW3335rKGvWrJkYOXJksf37O61WK9zc3MTkyZNLjHvnnXdEixYtTMo/+OAD8frrr4tly5YJe3v7Irft1auXmDBhgpg4caKoU6eOyfr8/HzRqFEjsXjxYjFw4EDRpUuXB7bb0dFRLF68uNj1hd97O3fuNFm3Z88eAYh79+4Vue3WrVtFtWrVRFRUlABEZGSkYV1YWJgIDg42it+wYYOwtLQUaWlpxbZn9OjRxb7fj2tWcHnmRpL+qQrNaGvlB5ZaLv919ibO1YH0bdvJvXLFJLzw7M3apHs4VLBFl6/nQPgFxL/rxsV/LSEEebm6Mlke5mfsUSY3zs3NNbnDRaPRcPToUcNliMaNGxMREcHRo0eBgjkDt27dSocOHUrdttjYWBITE43aZmFhQbNmzUzatnLlSlxcXKhRowZjx441ObNzv02bNpGcnMygQYNK3H9qaipOTk5GZbt372bt2rXMmzev2O2WLVvG5cuXSzxrMXnyZFxdXRk8eHCJbQDQ6XSEh4eTmZlJSEhIkTFarZaFCxdib29PnTp1Hljn/W7dusWQIUP48ccfsbKyMllf3Pudk5NDREREkXVeunSJbdu20axZs4dqy8Mye6K1S9K/gUKBVej/0eHAh/xewYIq8TYkOtiQffMOt+fMxedr4+cwVXS1oWugN+sj4/nDTk9tMwXXou9y+cRtKtdzK6NOSE9LvlbPwpH7ymTfb3/ZDLVF6R4/8CiTG7dt25bFixfTtWtXgoKCiIiIYOnSpeTl5ZGcnIynpye9e/fm9u3bNG7cGCEE+fn5/Oc//zG6lPEghfsvqm1Xr/7vMm+/fv2oUKECHh4enD17lrCwME6dOmXyZPxCS5YsoW3btiVOsHzo0CHWrFnDli1bDGV37txh0KBBrFixothblC9evMj48eM5cOAAZmZFf/X++eefLFmyxGRcy9+dOXOGkJAQcnJysLGxYcOGDVSvXt0oZvPmzfTu3ZusrCw8PT3ZsWMHLi4uJdZ7PyEEgwYNYtiwYQQHBxMXF2cS07ZtW+bOncuqVavo2bMniYmJTJkyBSh4Tt39QkNDOXHiBLm5ubz99ttMnjy51G15FPLMjSQ9DjVf47V8S+7Z5ZHonIMArrrak75jB1knTpiE/1/LKqiUCn67mox3SMEv6ANrLqDNllMzSM+Wh5nc+OOPP6Z9+/Y0bNgQtVpNly5dDGdBVKqCpGrv3r1MnTqV+fPnc+LECdavX8/mzZv59NNPH3vbhgwZQqtWrahZsya9e/dm3bp17Ny5kxNFfCZv3LjB9u3bSzxjEhUVRZcuXfjvf/9L69atjfbTt29fmjZtWuR2Op2Ovn378sknn1C1atUiY9LT03n99ddZtGjRA5MQf39/Tp48yeHDh/nPf/7DwIEDiY6ONopp0aIFJ0+e5ODBg7Rr146ePXuSlJRUYr33+/rrr0lLSzMaC/V3bdq04fPPP2fYsGFYWFhQtWpVwxm4wve70OrVqzlx4gQ//fQTW7ZsYdasWaVuyyN54IWrF4wccyM9Mfu/EP2/rSzazXhJzOrZQXzZu7M4HRAgrvTsKfR6vUn46PBI4Tdusxi/5pT4ccJB8c3QXWL/6pgyaLj0pBQ1fkCv1wttTn6ZLEX9HBYnNzdXqFQqsX79eqPyESNGiKZNm5a4rVarFdevXxf5+fli/vz5wtbWVuh0OiGEEI0bNzYZl/Pjjz8KjUZjiLkfRYy5uXz5sgDEiRMnjMo7d+4sBgwYUGy79Hq9UKvVIjw83GTd5MmThaurq9BqtUVuGxUVJdzc3MSHH35oss7e3l6oVCrDolQqBSBUKpVYsmSJuHfvnuF14aJQKAxlu3btEpGRkUXGKBQKoVKpxKVLl4rtV8uWLcXbb79d7HohhKhcubKYNm2aSXlxY266dOkilEqlUXsK2/f3Y6zX60V8fLzIysoS0dHRAhBHjx4tti2F73d+fr7Jusc15kZelpKkxyX4DYYe+ZJhbtmkW+VjmwXx7s74nTpN+rZt2LVvbxTeLciH9ZHx/H7+Fm/3qsvWb05zZs8NqjX0xLWcbRl1QnrSFApFqS8NlaX7Jzd+9dVXDeU7duygS5cuJW6rVqsNkxmHh4fTsWNHlMqCCwVZWVmG/xdSqVQIIUo9JqjwUtOOHTuoW7cuUDC2ZN++fcyYMaPY7aKiosjLy8PT09OoXAjBsmXLGDBgAGq1usjtXn75ZQYOHMjUqVNN1h86dAidTmd4/csvvzBjxgwOHjyIt7c3dnZ2nDlzxmib+fPns3v3btatW0eFChVQqVQmMRMmTCA9PZ0vv/yyxEtlQghyc3OLXV/amPt99dVXhktMADdv3qRt27asXr2aBg0aGMUqFArD3VSrVq3C19eXoKCgEtuSl5f3RMcZyuRGkh4XjSOhjcZR+/Rcosqn0TDaiWt+npRLSCZ5/gJs27UzOmXeoKITjlZq7mRqSdBA5WA3Lh1PYu9PMXT/oB5KZdGn/iXpaRkzZgz9+/cnODiYkJAQFi5caDK5cXx8vOFZNhcuXODo0aM0aNCAe/fuMXv2bM6ePcsPP/xgqLNTp07Mnj2bunXr0qBBAy5dusTHH39M586dDZcyMjIyuHTpkmGb2NhYTp48iZOTE+XKlUOhUDBq1CimTZtGlSpVqFKlCtOmTcPKyoq+ffsCcPnyZVauXMkrr7yCi4sL0dHRvPfee9StW5dGjRoZ9XP37t3ExsYWeUkqKiqKFi1a0KZNG8aMGWMY76NSqXB1dQUgICDAaJvjx4+jVCqpWbOmoez+/wO4ublhaWlZYkzhs3LuL//www9p3749vr6+pKenEx4ezt69e9m2bRtQcBv91KlT6dy5M56enty5c4f58+dz48YNevToYagnMTGRxMREw3E+c+YMtra2lCtXznCc72djYwNApUqVDIkrwOeff067du1QKpWsX7+ezz77jDVr1hjey5UrV6JWq6lVqxYWFhZEREQQFhZGr169ih179Fg88NzOC0ZelpKeKJ1O7Pu+lQhcXEtM69tezOrZQexrHCKi/auJ9D/+MAl/f+1J4Tdus5iw4YzISMkRC0fuFd8M3SVO77leBo2XHrfn/VZwIYSYN2+e8PPzE+bm5iIoKEjs27fPsG7gwIGiWbNmhtfR0dEiMDBQaDQaYWdnJ7p06SLOnz9vVF9eXp6YNGmSqFSpkrC0tBS+vr7inXfeMbosUnip5O/LwIEDDTF6vV5MnDhReHh4CAsLC9G0aVNx5swZw/pr166Jpk2bCicnJ2Fubi4qVaokRowYIe7cuWPSxz59+ojQ0NAi+z9x4sQi2+Ln51fsMSvpVvD76y3qVvD7FXUr+Jtvvml4P1xdXUXLli3F77//blifnZ0tXn31VeHl5SXMzc2Fp6en6Ny5s8llouL6tWzZsiLbEhsba3IruBBCtGjRQtjb2wtLS0vRoEEDsXXrVqP14eHhIigoSNjY2Ahra2tRvXp1MW3atGI/E4/rspScOFOSHjORepO+a1phedmZmrH2uNvYU+/PE1g3aUK5RQuNYvfEJPHGsmO42lpwJKwlUfvj2R9+AbWlin6TGmLtYFFGvZAeBzlxpiQ9HDlxpiQ9oxT2Xgyv9CrnyqejVwhuZaSSZmVJ5oED5F68aBTbqJILtpZm3E7PJeLaPWo09ca9gh15OToOrLlQRj2QJEl6vsnkRpKegEaNwqimzOWqRxYAN2pVAzCZMdzcTEnrgIJbwbecTkCpVNC8nz8KpYLLJ24Tdzr56TZckiTpBSCTG0l6AhQWNozybkV0+YJZv69pM8k1U5G68Rfybt0yiu1Yp+DOjZ9P3CAtJw8XH1sCWxbcGbEvPAZtjnz2jSRJ0sOQyY0kPSGBTcKoYZHGbYdc9Ho9N2v5I/LyuLNkiVFc86puVHazIT0nnxWHC56u+lLHCtg6WZJxN5djm2PLovmSJEnPLZncSNKTYufFu87BRP119uayuQKdQkHKmrXkJ//vcpNSqeCd5pUAWHIglmytDrWFiqZ9Cp5kemr3DW5fK34+HEmSJMmYTG4k6Qnyr/8u5ezukGmZjzYnh9u1qyFycrj7/fdGcZ3qeOHjqOFOppY1x68DUL6WC5WC3BB6wa7l59Dl68ugB5IkSc8fmdxI0pNUriFD8lSc8ys483LZ0QYB3P1pFfn37hnC1ColQ5sVnL1ZsPcyaTkFMyg37V0VS2s1d25kcPy3uKfdekmSpOeSTG4k6UlSqgis0hEr12TylXpS790lq4Y/IivL5M6pHvV88HXSkJiWw/ifTyOEwMrO3HB5KuK3q/LylCRJUinI5EaSnrTqXRiclUKcZ8Ft4TcDKgNw78cV6NLSDGGWahVf9wnCTKlg65lEVh65BkCVYHfD5andP55Dr/9XPXdTkiTpocnkRpKetHKhNFTYkOSVCsDluMsoq1RCn5HBvZUrjUIDfR0Y167gmTiTN0dz9U4mAM36VMVcY0by9QxiDic83fZLkiQ9Z2RyI0lPmsoMZUAnapmnkGKjRZ+Xx93mTQC4+/0P6DIyjcIHN65Aw4pOaPP1hB8rGFyssTUn+JXyABz+5Yp89o301MyfP9/wKPx69epx4MCBEuPnzZtHQEAAGo0Gf39/w6SahRYtWkSTJk1wdHTE0dGRVq1acfToUaOY/Px8JkyYQIUKFdBoNFSsWJHJkyej1xsPqj937hydO3fG3t4eW1tbGjZsyLVrBWc84+LiUCgURS5r16411FG+fHmT9ePHjzfaz65duwgNDcXW1hZPT0/GjRtHfv7/PoOTJk0qcj/W1tZG9eTm5vLRRx/h5+eHhYUFlSpVYunSpUYxc+fOxd/fH41Gg6+vL6NHjyYnJ8coJj4+ntdffx1nZ2esrKwIDAwkIiLCsH79+vW0bdsWFxcXFAoFJ0+eLPb9EkLQvn17FAoFGzduNJTv3bu32ON37NgxAE6dOkWfPn3w9fVFo9EQEBDAl19+aVR/TEwMLVq0wN3dHUtLSypWrMiECRPIy8srtk2Pg5wVXJKehupdaB4dzgLfDOqfc+LirRuE+vmRd/Uq935cjst//mMIVSoVDAgpz+Erd1l/4gZj2/ijUiqo3dyHs/tukJacQ+SOazToVLEMOyT9G6xevZpRo0Yxf/58GjVqxHfffUf79u2Jjo42mTUaYMGCBYSFhbFo0SJeeukljh49ypAhQ3B0dKRTp05AwZdmnz59CA0NxdLSkpkzZ9KmTRuioqLw9vYGYMaMGXz77bf88MMP1KhRg+PHj/PGG29gb2/PyJEjgYJZvxs3bszgwYP55JNPsLe359y5c4b5iHx9fUlIMD7LuXDhQmbOnEn79u2NyidPnsyQIUMMrwtnwAY4ffo0r7zyCh999BHLly8nPj6eYcOGodPpmDVrFgBjx441zJReqGXLlrz00ktGZT179uTWrVssWbKEypUrk5SUZJQkrVy5kvHjx7N06VJCQ0O5cOECgwYNAmDOnDkA3Lt3j0aNGtGiRQt+++033NzcuHz5smEGcSiYGbxRo0b06NHDqF9FmTt3LgqFwqQ8NDTU5Ph9/PHH7Ny5k+DgYAAiIiJwdXVlxYoV+Pr6cvDgQd5++21UKhXvvvsuAGq1mgEDBhAUFISDgwOnTp1iyJAh6PV6pk2bVmLb/pEHTq35gpGzgktlIj9PZM2oIEK+rSlm9H5FzOrZQVz6fpmI9q8mzgXWFdrERKPwnLx8UeeT7cJv3GaxNybJUH4p4pb4Zugu8e27e0RGSs7T7oX0kIqa4Viv1wttdnaZLHq9/qHaX79+fTFs2DCjsmrVqonx48cXGR8SEiLGjh1rVDZy5EjRqFGjYveRn58vbG1txQ8//GAo69Chg3jzzTeN4rp16yZef/11w+tevXoZvS6NwMBAk3r9/PzEnDlzit0mLCxMBAcHG5Vt2LBBWFpairS0tCK3OXnypADE/v37DWW//fabsLe3L3JW8kLDhw8XL7/8slHZmDFjROPGjQ2vx40bZ/S6JMXN5H1/O318fERCQoIAxIYNG4qtS6vVCjc3NzF58uQS9/nOO++IFi1alBgzevToYvvwuGYFl2duJOlpUJmhqd6Vutc3cc09iwoJ1sTpc6gUFET2iRMkffEF3jNnGsItzFR0qePFD4eusvb4dZpVdQWgYl1X3CvYcSs2jZM7r9Ooe+Wy6pH0iPJzc/lq4Gtlsu8RP6xDXcrZybVaLRERESaXaNq0acPBgweL3CY3N9dkJmeNRsPRo0fJy8tDrVabbJOVlUVeXh5OTk6GssaNG/Ptt99y4cIFqlatyqlTp/jjjz+YO3cuAHq9ni1btvDBBx/Qtm1bIiMjqVChAmFhYXTt2rXItkVERHDy5EnmzZtnsm7GjBl8+umn+Pr60qNHD95//33Mzc1L7FNOTg4RERE0b97cpL7FixdTtWpVmjRpYijbtGkTwcHBzJw5kx9//BFra2s6d+7Mp59+ikajMfR7xYoVHD16lPr163PlyhW2bt3KwIEDjepp27YtPXr0YN++fXh7e/POO+888AzN32VlZdGnTx+++eYbPDw8Hhi/adMmkpOTDWeSipOammr0Xv7dpUuX2LZtG926dXuo9j4sOeZGkp6WWq/RLCubS94ZAJz/cz8u4z4AhYK0Tb+SFRlpFP5avYL5pX6PvkVqVsH1aYVCwUsdKgBwdt8NstO1T7ED0r9JcnIyOp0Od3d3o3J3d3cSExOL3KZt27YsXryYiIgIhBAcP36cpUuXkpeXR3Jy0ZPAjh8/Hm9vb1q1amUoGzduHH369KFatWqo1Wrq1q3LqFGj6NOnDwBJSUlkZGTw2Wef0a5dO37//XdeffVVunXrxr59+4rcz5IlSwgICCA0NNSofOTIkYSHh7Nnzx7effdd5s6dyzvvvGPUp4MHD7Jq1Sp0Oh3x8fFMmTIFwOSyDRQkQytXrmTw4MFG5VeuXOGPP/7g7NmzbNiwgblz57Ju3TqGDx9uiOnduzeffvopjRs3Rq1WU6lSJVq0aGGUYF65coUFCxZQpUoVtm/fzrBhwxgxYoTJ2KYHGT16NKGhoXTp0qVU8UuWLKFt27b4+voWG3Po0CHWrFnD0KFDTdYVXoasUqUKTZo0YfLkyQ/V3of2wHM7Lxh5WUoqMzqdSJgTIGotrSkm928nZvXsIC4cPSjiP/pIRPtXEzEhoSLj0CFDuF6vF21m7xN+4zaL5YfijMrXTDsqvhm6Sxxcf6kseiKV0vN8WSo+Pl4A4uDBg0blU6ZMEf7+/kVuk5WVJd544w1hZmYmVCqV8PLyEh988IEAxK1bt0ziZ8yYIRwdHcWpU6eMyletWiV8fHzEqlWrxOnTp8Xy5cuFk5OT+P77743a1qdPH6PtOnXqJHr37l1ku+zt7cWsWbMe2O9169YJQCQnJxvKvvjiC2FnZydUKpWwsrIS06dPF4BYvXq1yfY//fSTMDMzEwkJCUblrVu3FpaWliIlJcVQ9vPPPwuFQiGysrKEEELs2bNHuLu7i0WLFonTp0+L9evXC19fX6NLQWq1WoSEhBjV/X//93+iYcOGJm0p7rLUL7/8IipXrizS09MNZZRwWer69etCqVSKdevWFbleCCHOnj0rXF1dxaefflrk+mvXromoqCjx008/CW9vbzFjxowi4x7XZSmZ3EjS07T9I9H32ypi0LjGYlbPDmLDzE9F3t274nLXV0W0fzURHVBd3Fn+oyF80f7Lwm/cZtHhq/1G1Vw5mSS+GbpLfDdir8hO1z7tXkilVNIv6mddbm6uUKlUYv369UblI0aMEE2bNi1xW61WK65fvy7y8/PF/Pnzha2trdDpdEYxn3/+ubC3txfHjh0z2d7Hx0d88803RmWffvqpIanKzc0VZmZmJl+kH3zwgQgNDTWpb/ny5UKtVoukpCSTdX9348YNAYjDhw8blev1ehEfHy+ysrJEdHS0AMTRo0dNtn/55ZdF165dTcoHDBggKlWqZFRWWM+FCxeEEEI0btzYZMzSjz/+KDQajeH4lStXTgwePNgoZv78+cLLy8tkn8UlNyNHjhQKhUKoVCrDAgilUimaNWtmUs/kyZOFq6ur0GqL/l0TFRUl3NzcxIcffljk+r8r7FN+fr7JuseV3MjLUpL0NNXsTr+0dC75FNz+HRt5DK1KSflVP2H/6qug13Nr+nTDxJrdgnwwVyk5G5/G6RsphmrK13bBxdeGvFwdkTuulkVPpBecubk59erVY8eOHUblO3bsMLm083dqtRofHx9UKhXh4eF07NgRpfJ/Xzeff/45n376Kdu2bTPceXO/rKwso3gAlUpluBXc3Nycl156iZiYGKOYCxcu4OfnZ1LfkiVL6Ny5M66uriV3Goj86/Kwp6enUblCocDLywuNRsOqVavw9fUlKCjIKCY2NpY9e/aYXJICaNSoETdv3iQjI8OovUqlEh8fnxL7LQpORBjqKW2/izN+/HhOnz7NyZMnDQsU3JG1bNkyo1ghBMuWLWPAgAFFjpmKioqiRYsWDBw4kKlTp5Zq/0II8vLyDH16IkqVZr1A5JkbqUzp9UL7dbBovbiaGP9WKzGrZwcRsXXTX6v04sqr3US0fzVxb+1awyYjVp0QfuM2i/E/G5+6v3Lqtrxz6hn3PJ+5EUKI8PBwoVarxZIlS0R0dLQYNWqUsLa2FnFxBZdJx48fL/r372+Ij4mJET/++KO4cOGCOHLkiOjVq5dwcnISsbGxhpgZM2YIc3NzsW7dOpGQkGBY7r9EMnDgQOHt7S02b94sYmNjxfr164WLi4v44IMPDDHr168XarVaLFy4UFy8eFF8/fXXQqVSiQMHDhj14eLFi0KhUIjffvvNpH8HDx4Us2fPFpGRkeLKlSti9erVwsvLS3Tu3NkobubMmeL06dPi7NmzYvLkyUKtVhd5CWfChAnCy8uryDMS6enpwsfHR7z22msiKipK7Nu3T1SpUkW89dZbhpiJEycKW1tbsWrVKnHlyhXx+++/i0qVKomePXsaYo4ePSrMzMzE1KlTxcWLF8XKlSuFlZWVWLFihSHmzp07IjIyUmzZskUAIjw8XERGRppcKrsfxVyW2rlzpwBEdHS0ybrCS1H9+vUzei/vP0O2YsUKsXr1ahEdHS0uX74s1qxZI7y9vUW/fv2KbIe8LPWIZHIjlbnoX8X3X3iLnhNDxKyeHcSqif/7hZ00b56I9q8mrg37j6Hs0OVk4Tduswj4+DeRnpNnKNfr9WLdjGPim6G7xN6V559qF6TSed6TGyGEmDdvnvDz8xPm5uYiKChI7Nu3z7Bu4MCBRpcxoqOjRWBgoNBoNMLOzk506dJFnD9v/LPp5+cnAJNl4sSJhpi0tDQxcuRIUa5cOWFpaSkqVqwoPvroI5Gbm2tU15IlS0TlypWFpaWlqFOnjti4caNJ+8PCwoSPj4/JZTEhhIiIiBANGjQQ9vb2wtLSUvj7+4uJEyeKzMxMo7gWLVoYYho0aCC2bt1qUpdOpxM+Pj4lXpo5d+6caNWqldBoNMLHx0eMGTPGMN5GCCHy8vLEpEmTRKVKlYSlpaXw9fUV77zzjrh3755RPb/++quoWbOmsLCwENWqVRMLFy40Wr9s2bIHHuO/Ky656dOnT5GX+oQoSMaK2o+fn58hJjw8XAQFBQkbGxthbW0tqlevLqZNm1bsZ+JxJTeKvzr1r5GWloa9vT2pqanY2dmVdXOkfyMhyFjemS6Z13hlny8oYNi3P2Lt4EhOTAyxXbqisLCg6qGDKK2sEELQ8ot9XEnOZNqrtejb4H8PT4u/cI+NsyNRKhX0/aQB9q5WZdgx6e9ycnKIjY01POFXkqSSlfSZeZjvbznmRpKeNoUCm/af0yE/lWT7XBBw6dghACyqVkXt7Y3IzSXzr2eJKBQK+tQvSGjWHL9uVJV3VUfKVXdCrxcc/TX26fZDkiTpGSWTG0kqC27VGFipK/HuBQOLj+7fAhQkMratWgKQvnOXIbxLoBcAJ6+nkJRmPM9Mgy4F0zBcOHaLO/EZSJIk/dvJ5EaSyohz8wnUsUsBIOViHFnpaQDYvFyQ3GTs3Yv4a94ZNztL6vg6ALDrfJJRPW5+dlQKcgVRMKmmJEnSv51MbiSprFi7MCioFym2uSiFgm2//wCAVb0gVPb26FJSyDr+v5l+Wwe4AbAz+pZJVQ06V0ShgLjTySReSX067ZckSXpGyeRGksqQc+P3cHAquJR0ct9vACjMzLBt0waA1F9+McS2ql7wGPw/LiWTpc03qsfRw5pqIQXP5Ti04fKTfX6E9NDk+yFJpfO4PisyuZGksmRhw6tNOwBge0vBkZj9AAUP9APStm9Hn1kwLsff3RYfRw25+Xr+uGg6T89LHSugUiu5eTGFK5G3n1IHpJIUPvQsKyurjFsiSc8HrbZgvjyVSvWP6pGzgktSGav48jvkbdqFOtWabRu/o8G4pmjqBmLu54f26lXSft+Bw6tdUSgUtK7uzrI/49h57hZtahjP5GvrZEnd1uU4vjWOP3++hF8tZ8zU/+wXhPTPqFQqHBwcSEoqGCdlZWWFQqEo41ZJ0rNJr9dz+/ZtrKysMDP7Z+mJTG4kqaxZORFc3pxTp0ARfYeE9AQ8bT2xf7Urt+d+SeqGDTi82hWA1gEFyc2uc0no9AKV0viLMqitH+cOJpB+J4dTu65Tr135p98fyYiHR0ESWpjgSJJUPKVSSbly5f7xHwEyuZGkZ0DzJh2JiN6CdY4Zq7fNZ1SPT7Hv3JnbX35F1tGjaG/cwNzHh5cqOGFnacadTC0RV+9Rv4KTUT1qCxUhr1Zi57Jojv92lWohnljbW5RRryQouL3f09MTNzc38vLyyro5kvRMMzc3N5lf61HI5EaSngFmNTri4fgDyUke3PjjKNpuWsy9vLAOaUjmwUOkrFuH26hRqFVKWlf34OcTN9h6JsEkuQGo+pI7p/fcICkujeNb42jWx78MeiT9nUql+sfjCCRJKh05oFiSngVOFWhfruAMi3uiGbuiCu6ccujdG4B7P61Cl54OwCu1Ci5zbDubiF5vemeBQqkg9NVKAEQfuElKkhzMKknSv4tMbiTpGeEW1A7sMlGi4NDvPwNg26oV5pUqoU9L495PqwBoXMUFGwszEtNyiLyeUmRd3v6OlKvx17QMm+SD/SRJ+neRyY0kPSuqvUJd+wQAVGeTuJd9D4VSicvQtwG4+/336LOysDBT0eqvB/r9diah2Ooadik4e3PxeBJJV9OecOMlSZKeHTK5kaRnhWddmnhZkm+mwybbjE07lwFg98orqH190d27R8ratQC0q1nwwL7fziYW+9Ar13K2VK1f8OC/vStj0Ov0T6ETkiRJZU8mN5L0rFAqUQf3w8b5LgCX9h8ACp5Y7PzWWwAkL1yELiOD5v6uWJmriE/J5vCVu8VWGdq9MhZWZty+ls6p3TeefB8kSZKeATK5kaRnSWA/WtvFA2B3Xcu5q6cAcHi1K2q/cuju3OHOosVYqlV0ql0wU/iYNSdJzsgtsjprewtCu1UG4OivV0hLzn4KnZAkSSpbMrmRpGeJnRdVA0LIsctGKRT8snURAApzc9zffx8oGHuTl5DAhI4BVHSxJiE1h//7KZL8Yi47BTTyxLuqA/laPQfXX3pqXZEkSSorMrmRpGdN3f4E2BY8zfbe6Qvcy7kHgE3LllgFByNyc0maMwdbSzXf9a+HtbmKQ1fuMH/v5SKrUygUNOlVFYArkbdJvS3P3kiS9GKTyY0kPWuqtqOlU8GTbF3vqlkV8QNQkKS4jRsHQNqmX8k5d44q7rZMfbUWAAv3X+FeprbIKp29bShX3Qkh4NTu60+hE5IkSWVHJjeS9KwxM8e+/mtYWmeiQMHh/ZvJyc8BQFOrJnavtAfg9tffANC5jhfVPe3IyM1n4YHin2kT2KocAOcOJpCTKacBkCTpxSWTG0l6FtUdQLD1LQBcrwt+vfKrYZXLu++CUknG7t1knzmDUqlgdOuCy04/HIwrdnCxT4Ajzt425OfqiDoQ/+T7IEmSVEZkciNJzyLXqvhX9QHA464layJXGp5nY1GxIvadOgFw+6uvAWgV4EZtH3uytDq+21f82JvA1r4AnN5zA12+fO6NJEkvJpncSNIzyqHxAJwtM1AKBbqLtzh+67hhncvwd0ClIvPAAbJOnEChUDCqVRUA1kXcKHLOKYAqwe5Y25uTlarl4rFbT6UfkiRJT5tMbiTpWVW9K9UcCqZN8E3SsOr8KsMq83LlcOjWDYDbX34FQOPKrliqldzLyuPy7Ywiq1SZKanVouCM0Mmd14p9urEkSdLzTCY3kvSssrChQt2XAPBMtmRv7G5uZf7vbIvLf4ahUKvJOnKEzMNHMDdTEujrAMDRuOKfWlyjiTdmFiruxGdy49y9J9oFSZKkslDmyc38+fOpUKEClpaW1KtXjwMHDhQbu3fvXhQKhcly/vz5p9hiSXp63DqNw8pMi1qnxOWumrUX1hrWqb28cOjRA4DbX32FEIL65Z0AOB5XfNJiaa0mILRgbqqTO689wdZLkiSVjTJNblavXs2oUaP46KOPiIyMpEmTJrRv355r10r+hRsTE0NCQoJhqVKlylNqsSQ9XQrnilSo6A2A921LNl/eZHQpyXnoUBQWFmSfOEHmH3/wUoWC5OZobPFnbgDqvOyLQgHXou+SfKPoS1iSJEnPqzJNbmbPns3gwYN56623CAgIYO7cufj6+rJgwYISt3Nzc8PDw8OwqFSqp9RiSXr6KrTpB4DPbQ3xmQnEpsYa1qnd3XDs3RuAuytWEFTOEZVSQXxKNjdTin8Ssb2rhop13QA4sf3qE2y9JEnS01dmyY1WqyUiIoI2bdoYlbdp04aDBw+WuG3dunXx9PSkZcuW7Nmzp8TY3Nxc0tLSjBZJep741W2IQqHAIcMc62wVB67vNVrv0LPg0lTmnwexyEqnhpcdAMdKGHcDUK+dHwCXjt8i9XbWY2+3JElSWSmz5CY5ORmdToe7u7tRubu7O4mJiUVu4+npycKFC/n5559Zv349/v7+tGzZkv379xe7n+nTp2Nvb29YfH19H2s/JOlJs7SxwatqNQB8kjQcuLzZaL1FpUpYVKsG+fmk79hBsF/BpakHJTeu5WwpV6NgSobI3+XYG0mSXhxlPqBYoVAYvRZCmJQV8vf3Z8iQIQQFBRESEsL8+fPp0KEDs2bNKrb+sLAwUlNTDcv163JeHen5U3jXlE+ShojUS2TmZRqtt+vwCgBpW3+jfgVHAI7FPvhOqHrtygNw7lACmSlFP9lYkiTpeVNmyY2LiwsqlcrkLE1SUpLJ2ZySNGzYkIsXLxa73sLCAjs7O6NFkp43lV8KAQrG3VhmqTh8fZ/Rerv2BclN1pEj1LXRARBzK507xUzFUMirigOelezR5wtO7pKJvyRJL4YyS27Mzc2pV68eO3bsMCrfsWMHoaGhpa4nMjIST0/Px908SXqmOPv4Ur5OEAoU1Ii15UD0aqP15j7eaAIDQQjU+/cYxt1sPZPwwLqD/hp7c3Z/vJxQU5KkF0KZXpYaM2YMixcvZunSpZw7d47Ro0dz7do1hg0bBhRcUhowYIAhfu7cuWzcuJGLFy8SFRVFWFgYP//8M++++25ZdUGSnpqXOr8GQJXrNhxOOGvydGG7VwrO3qRu2Uy3oIKnEK878eAJMv1qOuPsUzCh5pm9Nx5zqyVJkp6+Mk1uevXqxdy5c5k8eTKBgYHs37+frVu34udX8JdkQkKC0TNvtFotY8eOpXbt2jRp0oQ//viDLVu20O2vx9BL0ovMt0YtXH29MdMrcbmqIfLaXqP1dq+0B5WKnFOnecUuB5VSwanrKVxKKvk5NgqFgnptCz5zp3ZfR5uT/6S6IEmS9FQoxL9scpm0tDTs7e1JTU2V42+k5875g/vZ8uVMcsx15Lxqy6fd1hitv/6fd8jYswfntwYz3qUZu88nMbxFJd5vW63EevV6wU8TD5N6O5tGr1UmsFW5J9kNSZKkh/Yw399lfreUJEmlV7VBIyys1VhqVUTFXCUrz/j5NPbdXgUg5Zdf6F6nYCzahhPxxc4SXkipVBD019mbE9uvyrM3kiQ912RyI0nPEaVKRWCzVgCUi7dme/Qqo/W2zZqhcnJCdzuZkLsXsLM042ZqDoev3Hlg3f4hHti7ashOz+PkDvncG0mSnl8yuZGk50yNNl0B8L6tYXPkT0brFObm2HfqBEDWxo10rOMFwM+lGFisUilp2LUSAJE7r5OZKp97I0nS80kmN5L0nHH09MbVyxElCjJjc4hNuWK03v6vAfbpe/fSvaIVAL+dTSAz98GXmioFueJW3o78XB3Ht8Y99rZLkiQ9DTK5kaTnUGDbggSm8g1rVhybY7TO0r8qlrVrQ14e5Q7toLyzFVlaHdujCh6Yef1uFncztUXWq1AoCO1WcPYm+sBN0pKLn3xTkiTpWSWTG0l6Dvk3bYNSCY4Z5uyPOkhyhvHD+pz69QUgZfUaw8Di9Sfi2RF9i+az9tLzu0PF1u1d1RHf6k7o9UKevZEk6bkkkxtJeg5ZWFlTKTAIALdkDT/uHG203rZdO1ROTuQnJNAx8xIAf15OZvhPJ9DpBZeSMkhMzSm2/vodKwBw/nCinDFckqTnjkxuJOk5Va5uQwA871iy+t4Z0q4fMaxTWljg0KNHwf9/+Zn6FQpm/9bm6w0xp26kFFu3R0V7ytVwRugFx7fEPZH2S5IkPSkyuZGk55RvjVoAuKdYkCOUrDnwX6P1jr17gVJJ1uHDDPYqSGpCKjrTNbDgDqrTJSQ38L+zNzFHErmXmFlirCRJ0rPkkZKbH374gS1bthhef/DBBzg4OBAaGsrVq1cfW+MkSSqek5cP1g6OKPUKXFMs+DHnBjnZ9wzr1Z6e2LYqeCZO9S0r+H10U34cXJ/g8k4AnL6RWmL97hXsKF/bBSHg0IbLT64jkiRJj9kjJTfTpk1Do9EAcOjQIb755htmzpyJi4sLo0ePfsDWkiQ9DgqFAt8atQGoctuSuyolG4/ONopxHTUK1Goy9u7F6/wJzFRKavvYA3AmPtVk8s2/C3m1EgqlgthTydy8mPIkuiFJkvTYPVJyc/36dSpXrgzAxo0bee2113j77beZPn06Bw4ceKwNlCSpeIWXpgLSCs7GfH/1N/L1/3uejUXFCjgN6A9A4rRp/D97dx0mZfUFcPz7Tu1sdycsuXR3h6SkogioiKI/u7vFRMXCQhEVk5Bu6a6lNtlguzum398fLwyswLLgUnI/z8MDO3PfmAV2ztx77jk2k4lmAW7o1CpKqsykFdWeLOwV6ExUT2UZa/uCROQLtHEQBEG4FlxScOPi4kJhoVLOfe3atQw8OfWt1+uprhZ1MQThSjk1c2MpBl+TjUzZyOrkFTXG+DzwAGpfH8wn0ij+8Ud0GhXNA10BOHSBpSlQcm+0DmryTpRzfH9e/b8IQRCEenZJwc2gQYOYNm0a06ZNIyEhgeHDhwNw7NgxIiIi6vP+BEGohYd/IC7ePtisNm7J1QHw5f5ZVJpPJwCrXVzwe0xZLi6a9yOyyUTrEA8ADqeXXPAaTm462g1WuoTvXZFywSacgiAIV9slBTdffPEF3bp1Iz8/n4ULF+Lt7Q3A/v37uf322+v1BgVBOD9JkgiLUpamGlub42+xkGYo4JXtr9TIp3EfOQKNry+W/HzK1qy1591cKKn4lDb9Q3Fw0lCcU0XSATF7IwjCte2SghsPDw8+//xzlixZwpAhQ+yPv/7667z44ov1dnOCIFxYRNsOAKTlqpiZW4BGlll7Yi2/xJ1uqinpdHjcfhsART//RJtQDwCOZpVircNMjM5RQ5sBoQDsW5kqcm8EQbimXVJws3r1arZt22b/+osvvqBt27ZMnDiR4uLiWo4UBKG+RXbsgkbnQHFhCYHWEJ4qUv4Pztw3k4LqAvs4z1tvRdJqMRw6TFBWEk46NVUmK/E55XW6Tut+Iej0aoqyKkk+lH9ZXosgCEJ9uKTg5umnn6asrAyAI0eO8OSTTzJs2DCSk5N54okn6vUGBUGonU7vSMP2nQCI03ZlYlkFjW1qLDYL+3L32cdpfHxwGzYMgNJffqFrQ2U5eX1sbp2u4+CkpXX/M2ZvLrCNXBAE4Wq5pOAmJSWFqKgoABYuXMiIESN4++23mT17NqtWrarXGxQE4cKade8NQPyJKpC0dCovAeBA7oEa4zwnK9vCy1avZniwBsDeLbwu2gwIReugpiC9gtQjhfVw54IgCPXvkoIbnU5HVZVSH2P9+vUMHjwYAC8vL/uMjiAIV05Euw7oHB0pLyokK2AY7YxG4OzgxrFlCxzbtwezmY6HN6OS4FhWGekXqHdzit5ZS6u+IQDsW5EiZm8EQbgmXVJw07NnT5544gnefPNN9uzZY98KnpCQQEhISL3eoCAIF6bVOdCoo9JIM94YSQeD0vE7oTiBMlPNDxxekycBYFy8kK4hSr2btTF1W5oCaDswFI1ORd6JctKOFdXH7QuCINSrSwpuPv/8czQaDQsWLODLL78kODgYgFWrVtXYPSUIwpXTtIeyNHV03xFUji0INZuRkTmUd6jGONeBA9H4+2MtKOC2qgTg4pamHF11tOyjfIjZK2ZvBEG4Bl1ScBMWFsby5cs5dOgQ99xzj/3xjz/+mE8//bTebk4QhLqLaNOe0BatMRsNLE8JpX2lCYADeTWXpiStFs+JEwFoun0lyDL7UosoqDDW+VrtBoWh1qrITSkjI1bskBQE4dpyScENgNVqZeHChbz11lvMmDGDRYsWYbVa6/PeBEG4CCqVmuGPPI2TuwcFBeWEJfoDcCD97H5vHrfegqTTYYuLZawtA5sMK49k1/laTm46WvZSZmzF7I0gCNeaSwpujh8/TvPmzZkyZQqLFi1iwYIFTJ48mRYtWpCUlFTf9ygIQh05e3gy7OGnQJIoz/fGvVzLkZJEjNaaszIaT088Txb1u2P/YtQ2K7M3JmEw1/0DSrvBYag1KrKTSslMKKnPlyEIgvCvXFJw88gjjxAZGUl6ejoHDhzg4MGDpKWl0aBBAx555JH6vkdBEC5CeKu2NGzXEYBm2XrM2Diae/CscT7/+x9qT0+cstKYmLuPnDIDP+xIrfN1nD0ciOoRCCg7pwRBEK4VlxTcbN68mffffx8vLy/7Y97e3rz77rts3ry53m5OEIRL06hzN+X3XGcAlh/86qwxand3fB99FIBbj67G1VTJ7I3HKa0y1/k67W4KR6WWyEwoITNe5N4IgnBtuKTgxsHBgfLys0u2V1RUoNPp/vVNCYLw70R26IIkqdCWO+BSpWZJ/n7yq85umeBxy3gcmjZFU1nOfVk7KTNYmPL9bmZvOk5qQeU5zlyTq5eeqB5BAOz8K0nk3giCcE24pOBmxIgR3HfffezevRtZlpFlmV27dnH//fdz88031/c9CoJwkZzc3Alp3gKAbhk6zBL8tG/WWeMktRrfRx4GoH/iNlxlE4cySnl/dTxjZm+n2nThHJyOwyPQOKjJTSkj6YDoOSUIwtV3ScHNp59+SmRkJN26dUOv16PX6+nevTuNGjVi1qxZ9XyLgiBcikaduwPQtMgPgD9SV5xV0A/ApW9ftOFhqCorWNywmNdvboGfqwPFVWZW1GEHlbO7A20HKj2ndv2VhNVqq8dXIQiCcPEuKbjx8PBgyZIlJCQksGDBAv78808SEhJYvHgxHh4e9XyLgiBcikadlIrFFcXQvNxKpWzlt6PzzhonqdV43Xmn8sXC35nSJZQ7u0cA8PvetDpdq92gMBxdtZTmVxOzNate7l8QBOFSaeo68ELdvjdt2mT/80cffXTJNyQIQv1w8/ElILIxOUmJ3JzrSaxrGXOPzWV81B146b1qjPUYPZr8Tz7FnJZG+d9/M75LLz5al8De1GKO51XQyM+l1mvp9Bo6DW/Alt8S2LsihaZdA9Dp6/zjRRAEoV7Veebm4MGDdfoVHR19GW9XEISL0bBDZwBcrM1pbjRRYTPz5bI7wWqpMU7l5ITnhAkAFP/yC/5uevo1PbmctS+9TteK6hWEu68j1eVmDq6r24yPIAjC5SDJN9j2hrKyMtzd3SktLcXNze1q344gXFY5SYnMf+FxtHo9ncd4c2/pDtSyzGKH5jS4/c8aY00ZGSQNHAQqFY03b2JTvo1pP+7D21nHzucHoNNc+LPQ8f15rPn2KBoHNZPe6Iqzu8PlemmCINxgLub9+5LbLwiCcO3zbxCJk7sHZoOBoEb/o69HM6ySxMfFB6A0o8ZYXUgI+jatwWajbM1a+jb1xc/VgcJKE5sT6rYLKrK9L34RbliMVvYuF4X9BEG4OkRwIwj/YZJKRUSb9gCkRO/n8b7voZJho7MT8Yd+PGu829ChAJStXIlGrWJYK6UCcV27hkuSRI9xkQDEbMsi78TZu7MEQRAuNxHcCMJ/XIOTrRhSDu6joXtDBrs2BOD7lGVnjXUbOhQkieoDBzBnZ3NTiwAANsTmYqnjFu+gxp407uSPLMOm+fHYbDfUyrcgCNcAEdwIwn9cROv2SJKKwow0yvLzmNruIQBWy+VkFMTWGKv198epQwcAylatplOEJ55OWoqrzOxJLarzNXuMb4TOUUN+WjlHN2fW34sRBEGoAxHcCMJ/nN7FhcAmzQBIid5H84aD6G5RYZMk5u354KzxrsNOLk2tWIFGrWJgc38A1h7LrfM1nd0d6DZamSHavSSJ6grTv30ZgiAIdSaCG0G4AZzqEh63fQsAUwN6ALA4fx9FhpozMm433QQaDYZjxzDExdmXptYey7mo3lFRvYLxCXXBZLByaH3dtpMLgiDUBxHcCMINIKp3f1RqDRmxR8lKiKNzq7uIMhoxIrM4fkGNsRpvb1wHDQSgeP4v9Gzsg5NOTVapgSOZpXW+pkol0Wl4AwAOb8rAUFn3buOCIAj/hghuBOEG4OrtQ/NefQHYu3QBUkhHbjOqAfgz5kestpoNMr3uuAOA0mXL0FZV0LepLwBLoi+utUKDNj54B7tgNlg59LeYvREE4coQwY0g3CA6jRwHwPG9uyjMymRI22m4Wm1kmkrZnr65xljHDh1waNoU2WCgZNFixncIAeCPvemUG+o+AyNJEh2HRQBw+O8MjFVi9kYQhMtPBDeCcIPwDgklsqPSTHPPX3/g2OUBRhuV7d1/7JtVY6wkSXjeMRGA4l9/pU8jHyJ9nSk3Wvh978XNwES288Uz0BlTtYV9q078+xciCIJwASK4EYQbSOdR4wGI2bqR7X8t4pYWSjfwLeXJZBYn1RjrPmIEKldXzGlpVG3ZzLReyu6nudtT61zzBkBSSXQfqxT2O7whncKsivp4KYIgCOclghtBuIEENWlGz9uVgGbXwt9ITfKgq0lGliTmrXu0xliVkxOetynNNAu++prRbYPwdtaRWVLNyqN1q1h8SkQrHxq08cFmk9nya8JF7boSBEG4WCK4EYQbTJfRt9B/6v0AHFi9nNv9bwXgz6pU0jfPqDHW6847kRwcMBw+jG3/XiZ3Cwdg9sbjWC+y8nDPWxuj0arISiwhYU/da+YIgiBcLBHcCMINqN1NI2jQVqlE7KFtR3fHYCySxOyj38HRhfZxGh8fPG65BVBmb+7sFoGrXkNcTjmLDmSc89zn4+btSIeTycW7/krCbLLWfoAgCMIlEsGNINygAhsrVYtzjsfzSP8PAVjh7MTBDS9hM1fbx3nfMxW0Wqp278Yh/hgP9WsEwMy18VSZLBd1zbYDQ3HxcqCi2MihDWJruCAIl4cIbgThBhXYqAkA2cfjaeHTgpvCBiFLElM8NHT9tQcvb38ZWZbRBgbiMXoUADmvvsrk9gGEeDqSW2ZkztaUi7qmRqum6yglufjAmhNUlYm2DIIg1D8R3AjCDSqgUVMASnKyqS4v48lOT9NeH4BGlqmWzfx1/C/iiuIA8H3sMdQ+PhgTEyn7ZBbPDlFmfb7anER6UdVFXbdJJ398w1wxG6zsXX5xwZEgCEJdiOBGEG5QehcXPIOU4nzZx+MJdAlk3uhF7MkpY0ClErAsTVoKKC0Zgt55G4Din3+mb3ECnRt4UWWy8sLiIxe1+0lSSfQYpyxtHduaSd6Jsvp8WYIgCCK4EYQbmX1pKjFBecDBFW2naYwtV2rRrEhegdmqVBV26dULzymTAcid8TbvjIrCQaNia2IBf+7PoMpk4XheBbY67KIKbupJ407+yDJs/DkO60XUzREEQbgQEdwIwg0s8OTSVM7x+NMPdnmA7hYVPhYrxcZitmRusT/l99hjqN3dMaen43t4N48PUoKjFxYdocWraxj40WbeWxNXp2v3vKUxDk4aCtIrRHKxIAj1SgQ3gnADC2x8KrhJQLadnD1x8UXT6wlGVFQCsDRhkX28yskJj4m3A1D0/Vym9WxAm1APLDaZUytT321N4Xhe+QWv7eSmo8d4ZXlq77IUSvOrL3CEIAhC3YjgRhBuYD5hEWi0OgyVFRTnnNHxu9vD3Cy5AbAlcytFhiL7U14TJyJptVRHR2M6dIif7unM7/d1Ze+LAxnY3B+LTeb1ZTF1ysNp1i2Q4KYeWMw2Nv8SJyoXC4JQL0RwIwg3MLVGg19DZfYkKz729BNaPY0Hv0sLoxELMiuP/Wx/SuPri9uomwEomjsXN72WLg298XV14KXhzdGplTycDbF5F7y+JEn0ndgMtUZFemyxqFwsCEK9EMGNINzgwlq2AeDopnU1n2g6jJt1AQAsjf+zxlPed90FQPn69ZjS0uyPR/g4M7VnA0Ap8lcXHv5OdBweAcC2PxOprhC1bwRB+HdEcCMIN7g2A4egUmvIjIsh+8zEYkliWNv70MgyseYS4gti7E85NGqEc5/eIMsU/TCvxvke6BOJVi0Rl1Nep9wbgHaDw/AKcsZQYWbDvFjki+xbJQiCcKarHtzMnj2bBg0aoNfr6dChA1u3bq3Tcdu3b0ej0dC2bdvLe4OC8B/n4uVNsx69Adi//K8az3m0vIV+RqXFwpIDs2s853333QCULF6MpbjY/ri7k5YejXwAWHWkbt3D1WoVA++OQq1RceJIIfvXnLik1yIIggBXObj5/fffeeyxx3jxxRc5ePAgvXr1YujQoaSdMc19LqWlpUyZMoUBAwZcoTsVhP+2DsNHA5Cweztl+WfkymgcuNm/KwArsrdjtpntTzl16YJD8+bI1dWU/P57jfMNaxkIwMqjdQtuAHxDXel9u7K1fM/SZNLjii5whCAIwrld1eDmo48+4p577mHatGk0b96cWbNmERoaypdfflnrcdOnT2fixIl069btCt2pIPy3+UU0JKxVW2SbjQOrltR4rkfnx/CyWinCwraklfbHJUnCe6oye1P083xsptO5MoOi/FGrJGKzyzhRWFnn+4jqEUTz7oHIMmz6OQ6rRRT3EwTh4l214MZkMrF//34GDx5c4/HBgwezY8eO8x43d+5ckpKSePXVVy/3LQrCDaXTiDEAHFq7ivLCAvvj2qC2jJSdAfh+/yc1tmu7DRmCJiAAa0EBZcuW2R/3dNbRraE3AKsuYvYGoOetjXF001FWYODY1qwLHyAIgvAPVy24KSgowGq14u/vX+Nxf39/cnLO/cMwMTGR5557jvnz56PRaOp0HaPRSFlZWY1fgiCcLbxNe4KbtcBiNrFz4a81nruzzXT0NhvRxny2Jvxlf1zSavGarLRkKJw7t0bgM7SVstNq1ZHsi7oPnV5D55O7p/atTMFUbbmEVyMIwo3sqicUS5JU42tZls96DMBqtTJx4kRef/11mjRpUufzv/POO7i7u9t/hYaG/ut7FoT/IkmS6HX7nQAc3biOoqwM+3O+7adyO64AfLb7HWzy6eUij1tvQeXsjOl4EpVnbAgYHBWAJMGhjFIOpp1OOK6L5j2D8PB3orrczMH1tefgCYIg/NNVC258fHxQq9VnzdLk5eWdNZsDUF5ezr59+3jooYfQaDRoNBreeOMNDh06hEaj4e+//z7ndZ5//nlKS0vtv9LTRQ8bQTif4GZRNOzQGdlmY+svP5yeiVGpmDrgY5xtNuLkatbt/th+jNrVFY9bbgGU2ZtTfF0dGNMuGIDnFh7BdBH5M2q1iq6jGgIQvT6dylLjv31pgiBcSdarO+N61YIbnU5Hhw4dWLeuZuGwdevW0b1797PGu7m5ceTIEaKjo+2/7r//fpo2bUp0dDRdunQ553UcHBxwc3Or8UsQhPPredsUJEnF8b272PDdbHvPKY+wbtzpqvSiej9mLgUFpxtkek2ZDGo1VTt3YYg9Xen4peFReDnriM8t5+vNSRd1Hw3b+eLfwA2L0creFan//oUJglD/bDbIi4PkzRD9Kyx/AmZ3h3kjr+ptXdVlqSeeeII5c+bw/fffExsby+OPP05aWhr3338/oMy6TJkyRblRlYqWLVvW+OXn54der6dly5Y4OztfzZciCP8ZvmERDLrvIZAkDq1bxarZH9sDnDtvmk0Dq0SeWuLJ5XdgNlYAoA0Kwm3IEKDm7I2Xs45XR0YB8Nnfx4nLqXvOmyRJdB8bCUDMtixKcqvq5fUJgnCRon+FWa1hw5tgOWMWNT8B5gyA2V3gx5vhr/th33eQdwwy99Uce4Vd1eBmwoQJzJo1izfeeIO2bduyZcsWVq5cSXh4OADZ2dkXrHkjCEL9a9V/MMMffgqVWk3s1o2kHjoAgJNrAJ/0/xQXm8wBycQHC0ZBlVKPxutkUb+ylaswn7HcfHObIPo388NktXHfj/sprqx7e4Wgxp5EtPJGtsns+uviZn4E4UqQZZnvjnzHhrQNF3XcyiPZdHhzHduPF1x4cH0wVcLOL5Qg5aexYLPW7bg93ypBS8kJ2DoTvukLm96DFU/B170g6wBoHMGnKUT0gq4Pwq0/wmNHQeNwWV9SbST5BmvDW1ZWhru7O6WlpWKJShAuYN23n3N4/WraDRlJ/7un2x/ftHsWD8d9B8BfOSVEtrsL+r3IiXumU7VnD173TMX/6aft44sqTYz6YhvpRdV0a+jNj/d0Rquu22erwswKfn9rD7IMox5vR0hTz3p9jYLwbySXJDNqySicNE5sv307GtWFd/JarDb6ztxERnE1d3QJY8aYVpf3JvPiYN4IqMw//diYb6DNBOXPlYVQmg6VBeAdCV4NlA8t2z6CHZ8pY6JGQ+o2qPpHMNawH4z6AtyDL+9r4OLev6/6bilBEK5dEW3aA9hnbk7p2+Ux+nu1BOB3RzXs+BSW/A+vu5TdViW//4G1osI+3stZx5wpnXDWqdmZXEjfDzbx3MLDRKeXXPAevINdiOql/OD8+8dYTAaxNVy4duRUKbOUVZYqkkrqNru45lguGcXVAKQVXYHl1rUvKoGNRzg0P5kLs3EGWEyw/nX4IBK+6QPzx8GnbeGzjjCr1enApufjcMsP8OAe6PEYdLgLejwK476DSYuuSGBzsURwIwjCeYW1bIOkUlGcnUlpXm6N5yZ0eBiApZ4+VKm1cHQhLtat6Bo0wFZRQcmCBTXGNw1w5ZPb2uGkU5NZUs1ve9O59eud7EwqvOB9dB8biau3nvJCAzsWHq+/FygI/1Jh9el/v0cKjlxwvCzLfLs12f71icJ6CG5kGcpzlKTeiryaz6Vuh+PrQaWBKX/BmK/BxV9ZZvphuDI7g6w85tsMJDUUJoKpAgJawW2/wIBXQZLA2RsGvQ4jP4FBb0Cr8aC6NsOIa/OuBEG4Jjg4ORPUpBlw9uxN18CuhLuFU2kzsry7km8jbf8Ir8GtASia9yOyqWZ+zcAof/a9NJC5d3WiV2MfTBYb9/64j6OZpbXeh06vof+U5gAc25pF6uErlKcgCBdQUH363+LRgqMXHL//RDHR6SWoVUo9t8ySaszWOpZJsFlhwT3w/RDIjVEeO/wnfBQFHzZVkno/bQcJa5TnZBk2vK78uf2d4NUQdM7Q++SSccYe5fch78FTCfDgbngmGSb8DFOWwPSt0Gy4EthcZ0RwIwhCrSJan3tpSiWpmNBUWbP/rTIJucdjALjbVqH28cGSnU3p8hVnnc9Jp6FfMz++ndKRLg28qDBauGvuHkqqak80DmnqSet+IQCs/e4Y+enl//alCcK/VlB1Oo/lcMHhC47/fnsKAOPbh+CgUWG1yWSVVNftYnu+gaMLIG0nfNsf5t8Ki6ZBeRZIKnDyUWZcfpkAK5+GpQ9B+m4l4bf36Rw4e6ADMPA16Hr/6eccPZSlq4Z9r8ug5hQR3AiCUKtTeTdpRw9htdTMd7k58mb0aj2JxYlsb9wLHD1RlafiPbQjAIVz5ti3kf+TXqvm2zs70tDXmYIKEwv2Z5xz3Jm6j2tEcFMPzEYrKz4/REWx4V++OkH4dwpSN9n/nFScRJX5PMtMFiPYrOzJW48+8HdGt/cj1MsJOLk0JctK4m/xiXMfX3wCNryh/Nm7MViqIXGNEtT0eQ6ez4QnYqH9FEBWAqGDPyvju9wHboGnz6XRwV0rYOoaJZ/mpPTydEzWuu9mvJaJ4EYQhFr5NYxE7+qGqbqK7OPxNZ5zd3BnfJPxALy2913KOt4FgIfrPlRubpiSkylfv/6853bTa7mnZwMAftmTxoU2b6o1KoZOb4VnoDOVpSZWfXVEdA4Xrp7oXygsSrR/acPGscJjZ48rSYNP2yPP7o7RZTlaj4PkWg4T7uWEIwY8d72nLCfN7gKftIG/HoSyM3qyyTIsfwzMVRDeQ1k+Gvg6hPegYuLvfOPtTbqhUAlaRn4Ko7+CdpOUpN9hM6HvC2ffk1sQhHW1f3ms8BjDFg3jpW0v1eM36OoRwY0gCLVSqdSEt2oLQOKu7Wc9/3C7hwl1DSW3Kpf31OWgcURddBjPYT0BKPz6m1qDllFtg3HSqUnOr2R3StEF78fBScuIh1rj4KQh70Q5e5anXNoLE4R/I3M/LH2EfLUaAE+rUjfmcO7+muOsFlg4DcoysBTEIWlLAMg3pNHapZhFuldplfwtFKeA2gGQIfpn+KwD7Pte2dG0+H5I+lt5fuSnoFJDz8fg7pXMzN/BZwc/46vDXynXkyRoezuM+oK4Vk/xSnY3ikwXfqs/lHcIgPji+AuMvD6I4EYQhAtq0WcAAIc3rKGqtKTGc05aJ2b0nIFKUrH0xFoGhYfSMyyY+T5bkBz1GI4do3zN2vOe28VBw6i2QQD8uqduRTvdvB3pN0lJdD6w5gSZ8RfXmFMQ/hWbFZY9BjYzBTo9AL0tSpBzNGlNzbGb3lHyXhzcyNY5IZ9MY0mJW8T9cffQXJVOqdpT2Vb9TDLcsx5COoO5EpY/DrNawuHflF1MN38KPo3sp04pTWHx8cUA5FTW7NMI8M7KOH7ceYKP1yVc8CWllyt9F4sMF/6AcT0QwY0gCBcU0aY9/g0bYzEZ2bfir7Oeb+fXjnta3gNAjs1AqVrN964G5I5KTkHeBx9gM56/FPvEzkpV8lVHciiqYwXjyPZ+NO8RCDKs/yEGQ6X5Il+VIFyifd9DzmFMenfKUGZs+jVQ2o8cLkmEHZ/Dztnw4yjY+qFyzMhP2NLkTvspkkqO42Ap46CtEf9z/ljZVu3gAqGdqJq8mJR+z4BaBxW5oHOBiX9Am9tq3MbnBz/HJivLsmduSQcwWqzsOTkTuvhgJpXG2utDZZQrOW8lxhLMtuv//5IIbgRBuCBJkug2XvnBGr1mBdXlZ/eIerjdw/w87Gd+GfYLA33aAvBdyxw0blrMmZkUzf3hvOdvFeJOy2A3TFYbszfWvY5Nz1sa4+7nSEWxkU3z4y+YsyMIl8RUpfRRyopWfv39JgCFvR4DQKPS0K3bU6hlmTy1RO6GV2DN85C8CZCVlgQtx7JHH2E/ZYqDI9kDPmWC6WUOljjW+Lf7yq7XuDn1N9bf/D50/R9MXQ2NB9a4pWOFx1h74vSMaIGhZnmEg2klVJuVwKvCaGFJdFatL/HUzA1AiaGkTt+Wa5kIbgRBqJOG7TvjG9EQs6Ga/SuWnPW8JEm08W1DK99WPNTjNSQk1ng4YW2tfKIs+PprzLl5Zx13yuMDmwAwZ1sKWxLyazyXW2Zgd/LZxf50eg2DprZApZJIOpBH3M6zp+YF4ZLIMhz7i6zP2lH+ThB80Ump4vtNHzCUQkArChr3B8DH0Qd03qiNXgBs9u8KjW+CQW/CwwdgyNsAZFVm2k9fjY3qVv0wS1qqTFYKKkwnLyuzI2sHAO8mzKdqwMtKMb1/+PHYjwD0CO4BQKmxtMaMy7bEAtSOKbiE/oxKn8HPu06cN/i3yTYyKk7vVqzL0pTFasNqu3Y/TIjgRhCEOpEkiW5jldmbfcsXUZiZft6xkR6RDIlQpum/bueIo7cJubqa/I8+Ou8xA5r7M7mrsjz1xB+HyC9XlrHSi6oY/ulWJnyzi/0nzs6t8Y9wo/PNyo6rLb8nUJRVeWkvUBAS1ii5NMseg3kjSV98DyNczdwf4IescwHXQHBwB2c/GPkJBQbl36OP3oeE3AoqjWEA/KjvAnf8AT0eUXo1nVRgyK5xuYyKVILcHQFIK6o8+VgG5SalhlNuVS5fH/76nLeaXKpUOb6t6W2oJSXfp6j6dFCy7XgBWq/tSC5HcQr/moSK7Rw8T7uT/Kp8jNbTy8aFhtqrhpdUmRj+6TZ6v7/xgstdV4sIbgRBqLNGnbvRoG0HrGYza776BFstnYWnt5mOhMR6F0dSuis/OEuXLKE6Ovq8x7w4vDlN/V0pqDByy1c7WH00hzvn7rF/ql15JPucx7UbHE5wEw8sRivLPoumvEjUvxEuQkka/DoRfrkV9s9VfqVuZa+TM2ZJ4rDegcPTVsCTcfB8GjydCMEd7EtBPk4+xOeUYTMGAJBWkXTOWY0KizJzqVMpScjHS44T6qUEN6faMMQUKpWHnbXOAPwY8yPrEo/Q492/6fPBRp7+8xB/7ksno1xZZgp2CcZLr8wYnQpKSqvNHM4oQXVyZ5akMuMYMp9pC77h7ZWxJOef7vsGNZekoPaZG4vVxkO/HCQ+t5zMkmo2/2OW9VohghtBEOpMkiQG3vsQOkdHshPiOLBy6XnHRnpEMjlqMgDPtPJFjlQCjtyn70U2nzu5WK9V88Ud7fB3cyC1sIr7f95Pcn4lDhrlR9XamJxzTq2rVBJD7muFZ4ATFcVGln0ajaHi+k+KFC6D8lzYMhPj5vd486/b2PDTYKW2TPwKpf9Sx3uUujADXiGmw+kE3oWJi8461anWCz6OPsTnVGA9GdxY1FnEZJ2dl2aUlPGtvTsBkFSSRLiXEsT8M7gZ1mAYvYJ7YbFZeHL1F2SWVHOisIo/92fw9KK9VJiV88dnaPB29AZOJxXvSi7EJoPWQRnTwqsdANWOW/lmSzJDPtnKH/tOBzT/DG7i8rKIzzl3BfC3V8ax7fjp/J41x67NpWAR3AiCcFHcfHzpM1nZGbX9958pLzp/n6fHOzxOj+AeGGQLL90cABqZ6vQKyl4aDqZzLx818nNl3RN9uLNbOJIEbnoNv0/vhoNGRXpRNfG55/6hq3fRMvKRtrh4OlCcU8XyLw5hNp5/Zkm4AVXkww/D4O83Wbt3Fn+UHuNZSyZpahU06K30UhrxEfR9Fno9SewZOTKrU1dTYao543EqmPBx9CE+twybQakCrHLIZ8vxmrOMpYYKUCv/dvuH9QUguSSZMG9lR+Gp7uCngpso7yg6+So5PWZ1Fm1CPZgzpSMP9I2kZZjy71q2OvLQ/BiKyxyU+zk5c7P9eAFgwaZSrvdgu3sB8HQvp0cjb0wWG88sOMzzi45gsdrOCm6+33mEsbO3U1pV8wPCofQSe/uI+3or7Rv+jsvDdA0W0hTBjSAIF61V/5sIbhaFxWRk54JfzztOo9LwQe8PaOjekER9BYdvUnJj8tadwDZ3NMQsheVPwKpnlWJlJ7nptbw+qiWbn+rH+if60DbUg56NfABYdyz3XJcCwNVLz4iH2+DgpCE3pYw13x7FWtemhMJ/m6EM5o+DwuPgFsLeoBYAGFUq3mh7E/KUpeAfZR9usVlIKFLqw3g4eFBtqWZlysoap7TP3OiVmRvZ4o4GJyTJxsakmpWKD+ekAiBb9fQIUdqTJJUmEeqlLFElF1Qiy7I9uGnu3ZzoJOU5nWMe8+7qxMAof54d0oxnRvgD4KlTfs8oUHJuCqsLsdlkNifkI2nLABmdSkcbvzYAlJuL+GpyS54a3ARJUupKfbs1xR7ceDh4AGBTlVNpsrI+tub/tXUxytfDWgXw7JBm+Lg4UG6wsOsfyf6VpcarvnNRBDeCIFw0SZLoNVHpBH504zqKss7fF8pV58qr3V4FYGZUFpK3O5YqDcUbj8Efk2Hfd7D7K+X3fwjzdsLPTfkBPyhK+UG+Nub8wQ2Ad5ALwx9sg0ar4sTRQrb88t+ouCpcIrMB9v+g7HLKPqQ0l5yyhH16nX3I7sIj/HX8rxqHpZSmYLAacNI42Ws4LUxcWGPMqeDGQXKnoMKIJEk0cFcSiI8VxGO0nJ45jMlXZjw0Nh9C3ULRqDRUW6rx81SaZsZml3GiNIMyUxkalYbGHo2JTdMjyxI2VSUW1ellrqwKJd+mXVADPp/YDtnqAkB6WR5/x+VxorAKFydllsnf2R83nRueDp4AZFZm8FD/xrwzRtmB9fG6BBKLUgGIdFOCO0mtzKquOlpz9mlTgpIz1L+ZP2qVxKAoP6Dm0lR1hYkF7+1j7XfHMBmuXrKxCG4EQbgkwU2bE9mxC7LNxvbffqp1bHv/9vQJ6YNBY2PL8AgACmLdsTqFQ6NByqDN70F1yXnPMaC5P5IERzJLyS6tvYtyYKQ7N93XEkmCmO3ZJO6rPSAS/qMyDyhbuJc9CkXJ4OQNkxaS6+hKenk6KknF9NbTAfhw/4c1dgzFFsUC0MyrGaMajUKj0hBTGENa2ekq2qeCm/JKJSk4zMuJtv5K5WyrJpuDaSX2sUnFynHOal+0Ki0RbhEAmFTZuDtqMVlsbEg+AEBjj8ZUGiAhx4hsVpKFk0uS7efKqjydTDyidRBBLr4AHMhI4+stSQB0b6YBwN9J+VAQ6hYKYL//CZ1C6dfUF5PVRkqp8lhuvpIz5OSoLJFtSSyg3KAsTeWVGziaqQRYfZoo1xvcQhm/LiYXm03GZrWxds4xKoqM5J8oR76KW8VFcCMIwiXrOWEySBIJu7eTc7z2Eu8Pt3sYCYnPA48iNwjBZoRC1RS4/TfwaQrVxbDt/FvFfV0daB+mfPpcWIcO4hGtfOgwNAKAzb/Eix1UN5oDP8H3Q5SdUK5BcNPb8OhhCGrLvtx9gBK43N/mfrz13pQaSzlWcHopKbZQCW6ivKPw1HvSwltZxtqZcYC1x3IwWaz2nJvCMmUWqKm/K008lXpNaocctiae3kmUWaHk73jqlIAg0kOZ4UkqSaJNqAcAuzIP2a95qs+aHiWP53jJ6eKW2RXKjEqgs/LcwKYnz1WUw97UYrRqiahQZTk2wFm5Xpirsk09rVwJZCRJ4p2xrXF1MmGTlGAm4YQSSLm7GGno44zJYuPvOGW2ZkuCEsi1DHbD11XJ8eke6Y2Lg4a8ciMP/nKAuV9FkxFXjMZBzdD7W+HgpL3gX9PlIoIbQRAumU9YBC16K0mPW3/9odZ19qZeTRnWcBiySuKDjso0duEP8zAkJcNgpeIru76CwqTznmNCR+XT56z1iecs6vdPHYdH4BfhhrHKwoYfYrCJ/Jv/PpsN1rwISx8CqxGaDoMHd0G3B5X2BmAPbjr6d0Sj0tDOT9lNdDDvoP00Z+a+ALT2bQ3AzC1rue+n/dz783YMViVgzixQZkmaBrjS2LMxACp9DhvjTgc3edVKQBLgpPRRa+rZFFAqDbcNcQcgsSQOOBXcKP++w12VxN2kktP/L04tSwW5KOfqE6kEN/LJBOIx7YKpsCjBiD24cTsZ3Jwx8xTgrufhwUpAI1tcUVmVXVdl5hKGtFRmfFYfVf6vbopXgpxWDcrtW8UdNGpGtlECrNiDeRiOlADQf3IzvINduJpEcCMIwr/S/ZY7UGs0pB09zIkj0bWOfarjU3QN7Mq+SJm9jSWwWMh87jnkBv2V3SpWo/JpO33vOY+/pWMIo9oGYbHJPPjLgQsuT6nVKgbdHYXGQU1mQgnb/qx7awfhGmeuhgX3wOrnwXpyV4/ZAAunUrr7C97x8mRf13tgwnzQu9c4dF/O6eAGoK1fWwCi86IBpWJvXJESaDT3UoKbKC8lR6USZXloS7Lyu7PWhcRcJRm+aYArjTyUxpYqbQkxubnklCoBUKlZWRoNO7k8dCpYOpR/iLZhHoBMkUXJy4nyjmJXshJAnFrmSio9I7g5uSwV6KIEFr5OSlAiaZQ8m/t6NyS3SrlegNO5Z25OCfNX/g+1D2rEoZfGAmC0GunX3AOAjfF5FFWa2JpYgEqXy4rC57h37b32nlZvjW7FL/d2YayTKwAHHSxoI65uYAMiuBEE4V9y8/WjzeDhAGz95Qdk2/lnR3wcffh28Lf8NuJ3lo8NpkIPpphYCr+fC6O/Av+WUJkHPwyHIwvOOl6SJN4d25rmgW4UVJiYNGc36Se30J6Ph78TA+9U3qCObMrg0N/nr6wsXEd2fQlHF8Cu2fDHFMg6CHMGwrHFfOTlzS/urrxcGYtNqnlYQXUBqWWpSEi0928PYJ+5ic6PJjG3nHvmr6bKUoUaHeGuEZitNlbsVZZYVPpsXhvVCOeTeSmVVY72mjbNAlxxd3C357moHXLZFJ+HLMsYUGZxGnsrQUYrn1aoJBU5lTkEeZtQ6fKQVRXoVDr8HCKIy1HOOaCREgQllSQhyzJGq9Ge6xPkrMzcnKpzo9JU8eiASBr5udq7hP9zWSq9rOa//1M7pUJdQ3HSOuGoUfKH/D3NhHk5YTDb6PHu35RWm3F2zUVGJqE4gV3Zu5TXqJJo7uyIOt+EDOxxsPDV5vPPvl4pIrgRBOFf6zLmVnSOjuSlJBG/a9sFx7fwacE9/Z5i7kDlR1D+559jLDQqDQKbDFVmcBbeAxvfVnr8nMFRp+abyR0IdNeTlF/JmNk7OJJRWuv1Itv70W2MMnW//c9E0o5deElLuIZVFcG2WSe/kCB+JXzTF3KPcNTNl8WuSmG8jIoM+5vwKaeWpJp4NsHdQZnRae7VHAe1AyXGEobMXsDWtGgATFUBPL8ohklzdrMq2oDN7Iok2WjdsJKHB50s2GdywWS1oVOrCPdWrnt6aSqbv+PyKDYWI0vKDE4LX6UcgpPWyZ6fk1YZi4+fMhPUyK0Nh9IrkWWI9HWmfWATJCRKjCUUGgrtQYujxtG+ddvTwROVpAJkJvdUSiacmrnxd1YCrVPLUnnVeVSZT38gOFF2AoAQ1xAAe7XjIkMRH97ahgY+zvYGnA0CTu9++jX2dAmIo1uUfCKvRm6UqWQWHsggq6T2WdXLTQQ3giD8a05u7nQcqUxpb//9J6yWC28BHRw+mJK+rTkQKYHZTM7rbyj9e26bD90fUQZtfk/ZLl5Vsxx8qJcTi/7XnWYBJ1s1fL2D3/em1Zrz025wGM17BCLLsO77GJFgfD3bMhOMpeDfCqb8BVqlEJ6t0WDebtQOGRm9WikhsCCh5gzg5vTNAHQM6Gh/LCm/GtmgLBep9CcIDFbybWyGEBYeyGB3ShEuDlra+Cr1Yg7nH6bElghAp9AIBkX58/RNTdGqlbfUU+PUzolsP17A0njlmlaDPxHep5fITo07lH8IBzflfO5yK3YmKcF3l4beOGoc7YFHUkmSPd8m0DkQSVKmpdQqtT3QKawuxGg12vNiTi1LuTu424O5M4v2pZQpS2EN3ZXcHm/9yWrHhkI6RXix/ok+fDGxPbd1CrUnKQNszthMenk6ZqPV3rC2x9AGdGnghdkq8+3W07u7rgYR3AiCUC86DB+Nk7sHJTnZHPl77QXHS5LE4x2f4LvBKowaqNq9m7Lly0GlVhKMb/4cVFqIXQZfdIG4FTWOD3R35M/7u9GvqS8Gs41nFx7h0d+i7VtXz3W93rc1wTfMFUOlmbVzjokCf9ey/PgaQe3enL18uu9jyqN/hr3fKg8Oeg0a9oXpW0kf9zVvNGzBkeI4nLXOfNL/EwA2pm20L+MUG4pZk7oGgOENhiPLMnO3p3Dz59spL1UCiMhG0ZRwCLWk5vke96JTq4jwdmLx/7ozMFJpm7AhbQN/xP8BwP3tb+PbKR2592TFXoBeIb0A0DonUWkyMnPrEgDcaYWb/vQOolPBzc7snZTJym7D6IQA5u1MBZTdSFBzZ1V25cmdUifzbU45swVDbqUya6NX6+0BDZyxNHUyuJFlmZQSJbhp4K7MKJ05cwPKstPw1oG8O641lTbl+6iW1MjI/B73O4n7cjFVW3Dz0RPW3IuH+is5R7/uSaOo8nRhzitNBDeCINQLnd6RrmMnALBr4a+YDReeGekU0Iku7UeyqIfyo+jEW69hLTtZrKz9ZLhnDfg2U/JwfpsIf7+l7IY5yVWv5bs7O/Hc0GaoVRJLD2Ux8rNtHM089zKVRqvmpntbonPUkJNcyvY/Eq96JVXhHPLiYHY3+Lo3lrIcPj3wKfesmcq3x77nk60vgdUEDfuR5d+cecfmcc/+dxh+4G17kb2H2j5E96DutPFtg0W2sDhxMQCLEhdhtplp4d2CirIg7py7l9eXxWCy2GjprQQamVVKvsjoRqO5s3NH9r44kPVP9KGxvyutfZT8l+j8aEw2Ex38O9AtqNtZt9/cq7kyA6IyonZKQXZUCkm+MmBsjXGngpvE4kRsWLCZvMgrcsVqkxnbLpihLZUAJtL9dHBj3yl1Mt/mlDNnXM7Mtzk1uwNKXg2cTiouNBRSbi5HJakIdwsHwMvxZHBTXXO2FLAHTeMaj1O+n8cXcWybsiTVolcwkkqiZyMf7ugSxleTOuAptoILgvBf0HrgENz9A6gsKWb/yiV1OuatHm+hn3QrmV6gLa1ixbN3nC6mFtwB7tsM3R5Svt7ygZKLYz69nq9SSdzfJ5I/pncj2MOR1MIqxs7ewS+7085xNXD3dWTAqQTjzZlErxcJxldLflU+h/MPk1ySTJnpjEaTRxdyXKPic6mMsYuG8u2RbzkVgi50dSGr2wMcG/Aco5aMZua+mezJ2YOMTI/gHnwx4AsmRU0C4JYmtwDwW9xvpJel82fCnwCkprTh9m93sSUhHweNijdGteD728bbL69Vabm/zf0AuDtp0ZxcboryjkItqe3jHmr7UI3g4RSVpKJHcA8AdD4bUWkqcNQ4MbBB1xrjQl1D7ZWDATTG5ujUat4a3ZIPb22DWqWc+9TMzZGCI2RUKDWeTm0DP6XGzM0/8m1O+ed28FOFAYNdgnFQK7Vr/jlzc6ZTs0bjm4xXZoTKdOSllCNJ0LSrsvwlSRIzxrSib1O/c35vrhQR3AiCUG/UGi09blXeWPYsWVBrU037MSo1z/d8hdKHlVmfyI3Hee7r8cyPnc+8Y/M4VBIPN82AUV8oXZuPLYLvb4LiEzXO0yHckxWP9GRwlD8mq40XFh/h1SVHMZ9j6alhW1+6j1Omz3csPE7iXlHB+Eo6kn+EJzc9yaAFg7hj5R2MWjKKPr/34eeYn5FlmfnHFzE2OICvPd1JkSw422y8n1dAF603FkniQwczT+56DYPVQHOv5jzb6VlWjl3JVwO/ondIb/t1boq4iRCXEPKq8xi3bByZFZnIVkdysprj4qDh9s6hrHikJ1O6ReCh97Bv457QdIJ9l9GZzkwC7h7UvUbezj+dWprSOCsBRLfArmjVNWcyJEmyz94AvDZwHH8/1YdJXcNrBAYd/DugUWmILYplZbLS3+pUAb9Tzjlz41TzNfxzO3hKac18Gzgd3JxqwnlKtaWaEmMJoARWke6RNCpQdpsFN/XE2d3hvN+Lq0EEN4Ig1Ktm3XsT2LgpZkM1m3/6vk7HSJLEuNtfwzigKypg4B/HeW/XO8zcN5Npa6aRX5UP7SbB5MVKCf3sQ0qvoJilNXZTeTjp+HpyB56+SSmQNm/nCZq8tIrmL6/m1q92klF8epdI24GhtO6n5FmsmxtD3M6afXSEy2NH5g4mrZrE2hNrscpW/J38cdW5YrFZeG/ve9y2ZAzvOpiQJYmeLg15O7+ANemZDG08locGfAzA2hNryazIJNglmDk3zWFS1CT7ksuZ9Bo984bOo5lXM6otymyfuaQjPSID2PPiAN4Z25pGfq728U91fIpxjcfZZ23OZVLUJJp6NuXpjk/X+jq7BXY7uYNJcSrY+adTTS11Kh1DGvUkxNPprDFBLkHM7D0TjaRBPjmHdb6Zm4LqAntwc76Zm1M7pJJLlcDrVL4NnA6S/jlzc2pJylHjiJvOjUj3SBoXdACgcaea17kWiOBGEIR6JalUDJj6AJKkIn7HFtKOHq7zsS1f/wBcnGmYA88faUioUzAGq4Efjv2gDGjQW1mmCmqvtGv4YzLMGwnZp68hSRIP9mvEN5M74O6oRZah2mxlT2pRjW3jkiTR45bGNOsWgGyT2TAvlj3LkinKqsR2FXvi/JcZLAbe3PUmNtlG75DeLBi5gPW3rGf7bdt5ptMzqCQVMSeL1T2GF7PH/sXIobNxH/gW3Pwpbf3b0Sv4ZLKuSsuHfT/ETedW6zV9HX0Z7vMmVLbEZnaloW4IX0/uiJNOc9bYHsE9eK37azWScP/p5sibWXDzAhp5Nqr1uu4O7rT1bWv/umdwz3OO6xvSF61Ky5AGQ+w1Zs5lQPgAPujzARpJg0bS2HNkTrHP3FQXklNVs8bNKQ3cG6CSVORV5ZFdkX3umZvz5NyceU5JkoiwNMWrOhCbykpkO9/z3vfVIoIbQRDqnX/DRrQZPBSADd9/icV87h1M/6Tx8SHgqacAaLsykXcXO+FdJvNH/B/2HS94hCr1cHo9CWoHSN0KX/eGJQ9C+enuxINbBLD3xYHseXEAKx/pRbMAV/LLjdz69U4+WZ9ImcGMSiXRf3Jz2g5UPvXvXZHKr2/sZu7T20iPOTvnQPh3vjn8DRkVGfg5+fF+7/dp6qXMsEmSxOSoyXw58Eu62rR8kFfAPVFTlKWZFmOg+0PKLjrg6U5P096vPTN6zrD3ezqf5PwKbv92F68tSaI8bRKNjO/z01034eJwdmBzOZyarWnk0eicy1wAjTwbsWnCJl7r/toFzzcwfCA/D/+ZbwZ/Y18+OuXUzE1CcYK9R9Y/l6XcdG72gGtTxqZzztycL+fmn0tdTqnKslieT/JV7SF1PiK4EQThsuhx62Qc3dwpykxn26/z6nycx4QJ+L/8EpJej/ZALG//qsZirObHYz+eHqRxgAGvwMP7oOU4QIaDP8On7WD5E5CvbKvVaVT4ueqJCnLjz/u70auxD9VmKx+vT6DXextZsD8DSSXRfVwj+t7RlMBIdzQOagyVZlZ/e5TinMp6/q7cuGILY5l7bC4Az3d+Hmet81ljujuH8+2JJIZUVkOz4ec8TwP3BswbOo+hDYae91omi43P/05kyCdb2ZVchF6r4qXhzVn0QA98XK5cbsgtTW5hRMMRPN2p9iUsN50bWlXdAoQW3i3oFNDprMdPVUUuNBTa82Ui3CPOGtc3tC8AK5JX2BOPzxXclBhLMNtOfyg5cweWbJMpj1Vygo567MBkvXpbvs9Hkm+wfZBlZWW4u7tTWlqKm1vt05mCIPw7x/ftZskHSlPMcS+8QUSb9nU+1picwok7p2DNL+CzkSr2tXXmtxG/1ZhCt0vfA2tegIwzelI1GghdHoDI/qBSPsfZbDIrjmQza30CSflK4DKlWzgvj4iyF2Czmm0smXWQ7KRSPPydGP9sh2vyk+m1KLMik7d3v01aWRpVliqCnIMYGTmSSnMlnx/8HJPNRO/g3nweMgLJvzl4RigHJqyBze9DzhGlOnVoF7jnwrWSzuVAWjHPLzxCfK7SRLJXYx/eGt3SXj34v0qWZb47+h2ZFZn4OfrR0qflOfN8UkpTuPmvm+1fe+m92Dxhs/1rm2yj2y/dqLJUsfjmxfblt9d2vMbCxIU80OYBRupvY+msaExqA/M6vMSfY363V2W+nC7m/VvM3AiCcNk06tiFNoOGAbB69sdUldXeJuFMDg0b4HXHHQCMi9ZTba5i8srJ9qaHNYR2hnvWwZ3LoOlwQILj62H+OPh+sL3TuEolMbJNEGsf78NjA5Ufxj/uPMHwT7fy2540qk1W1FoVQ6a3wsXTgZLcKlZ/cxSrRRT7u5ByUzkPrn+QLRlbSC1LJa8qj+j8aN7c9SYf7f8Ik81Ej6AevOndBem32+DbAcrfS8Y++H0SZO5TAhtHT+jx6EVfP6fUwDMLDjHuyx3E55bj5axj1oS2/Di1838+sAFlaW9aq2m82u1VHmj7wHkTmBu4NyDCLcL+9T8/LKgklX25MK44zv74qZybQOdAYrcryfdFISlY1eYaTT2vFSK4EQThsuozeSpewaFUlhSz7ON36tSa4RSPW29F0ukISq9iaFUjykxl3LfuPp7b+hwrkldgsJxRKFCSlITj23+BRw5A1/+BzkWZzfmqF+ybCzalR45aJfHYwCbMmdIRV72GhNwKnlt0hB7v/c03W5KQ9GqGPdAajYOajLhiNsyLRRZJxudlsVl4avNTJJUm4efoxzeDvuHX4b/yVMenaOTRCFedK692e5Uv+32G1zalcjBVBfDzWPh9slKUr8lQePgAPJNy3iWpczFZbHyyPpG+Mzfyx74MZBnGtQ9hwxN9GN0u+KrWWrlW9QnpY//zmUtSp0QZOxFU2oj4onj7Y6d2S3mr/Eg+qDQB1bZQdh+eqpdzLRHBjSAIl5XWQc/NTzyPztGRjJijbJz3bZ2P1Xh54TZceaN76HgDBoUPwmwzsyJ5Bc9tfY571txz7vV+r4Yw5B343y4I7wnmSlj+mJJ4nLjOPmxglD9bn+nHC8OaEeLpSFGlibdXxtH7g43sKCpjyH0tkFQSiXtz+enr6H/5nfhvKjOV8fimx9mRtQNHjSOfDfiMbkHdaOnTkjtb3MniUYvZftt2xjcZj3T0Tyg8Do5eypJUcSqUZ4FPUxj3LXhHKkFqHUWnlzDis618vD4Bg9lGh3BPFj7QnQ9vbYOns+6yvebr3am8Gzh75iY/vRzXNVGMiHmQE8fz7I+fyrmxJjhjtdjwDnYhrKEfoFROvtaI4EYQhMvOOySMYQ8/BZLEobUrOLx+dZ2P9ZykLE1Vrl3PO5GP88OQH5jaciquOlcOFxzmg70fnP9gj1C4cync9Dbo3SH3KMwfD7/dAWXK1LqHk477ekey6am+fDC+NSGejuSXG3n0t2ie2JLAehclqbL8UDFrFiZc+jfhPyiuKI4JyyawKX0TWpWW93q9R5R31FnjJEkCiwk2vas80PMxmLQIXAPByQdu+wUcXM867nyqTVbeWh7D2NnbScitwNtZxye3tWXB/d3oEO554RPc4Nr6tbVvd2/ocTq4kWWZrb8lgCyhQkXg7g6YjBbKTeVUmCsAyNmvzJY27xFI5MljT+26upaI4EYQhCsiskMXek6YDCjbwzNij9bpOMcWLXDq2hUsFrKfepr2Xq15vMPjvNtLeaP8Lf43ViSfbqqZVZHF/Nj5lBhKlAdUauj2IDwSDd0fVqocxy1XmnH+OlHZXZX0Nxq1ils6hvL3k315clATdGoV+08Uc0BlZpuzspR2fF0G8btzEJSGlFNWTSGjIoNgl2B+GvYT/cL6nXtwUQosuhdKToCzH3S6V5mleeQgPBoNPrXXjDnT4YwShnyyhTnbUrDJMKZdMOue6MOotmIJqq40Kg1v93ybe1vdS9fA0y0hEvbkkp1UikanolJbilu1DxsXHrXP2rQp6k1RRhUqjUTTzgH2wCi1LBWLre7LzVeC2C0lCMIVI8syKz55n/idW3F0c2fS2x/j5ut3weNMGZmkjBmDrbwc72n34HeyFs6nBz7l2yPKMle/0H5EuEUwP3Y+JpuJ7kHd+WrgV2e/4eUchaUPQ9aBmo/3ewl6P2VfFknOr2Du9lQ6RnjSOtidt2bsoL1BAxI07uhPh6HheAe5/PtvynVofux83tvzntLPKagH7/V+T5kJsJjg0K/gEabkP1XkKrugDv4Ep978Rn8JbSde0nUX7s/g+cVHMFlsBLrrmTGmJf2bXXvVca9HpmoL81/dRVWZia6jG/Jlxiza7VN2VXm0lFhdtoQuaSORkGg7MJQe4xtjk210/aUr1ZZqlo5ees78nfp0Me/fIrgRBOGKMhsN/PbKs+SlJuEb0ZDbX38frV5/wePK1q4l8xFlF03oN1/j0rs3VpuVV3a8wtKkpec85osBX9ToNWRns0LyRiXnI30vHP5NebzlOBjyHricXXH1rWXHSFuXSRuTUgBOBiIGhzBibJM6ve7/AqvNysx9M/k59mdAaaD4QpcXlBotsgyLp8Ph35XBroFQVaTsgAKIHAD9X4LgupcDOCWjuIqZa+L5K1rpiD2wuR8fTWiLm15s0a8vhzaks+3PRNx9Hbn9lS68tOtFCjaqaJs1oMa4Vn2C6TWhCdLJpp4Tlk8gpjCGj/t+zMDwgZf1HsVWcEEQrllaBz2jnn4RRzd38lOTWf3lLOryGctt8GA8Jyqf+LOefQ5zbh5qlZoZPWewZPQSxjUeR2vf1szqN4u7W9wNwAd7P6hRiMxOpcbasB90mgZjv4YRH4OkhqMLlUKAmz+w5+Sc8tCAxuz2hnkuBhI1ViQgdW06R3bfGD2p0svTeXzT4/bA5vEOj/NK11dOF5/bOEMJbCQ16D2gPFsJbMK6wV0rYfKiiw5sZFnmsw2J9P9wsz2weWRAY76Z3FEENvVIlmVitivf37YDQ1FrVTTzasau8KWkDdpMRVAONmwYWmbS67bTgQ1AU09l23hMYcxVuffzETM3giBcFRmxR/nzzZewWS30mDCZrmMnXPAYm9FI6m23Y4yNxalTJ8J+mIukVp81rsJUwfDFwykyFDElagp3tbgLX6fTszFLji/hvT3vcWeLO5neZrryYNouWPUsZEefPlFwB+hwF7S5HdRa9p8oZltiAUEeenb9kUjDMrCpYOxj7Qhu8t9MZN2Xs4/PDn7GgTxlGU+n0jGj1wyGRAxRZmuyDsDe7yB6vnLAzZ9B6wmQvBkcXJTg5hJzYWatT2DW+kQAujX05oVhzWkVcv6+T8KlyUkpZeF7+9FoVdz1Xg8cnLTszt7NtLXT0Kq0mG1mdLIDP4/8iebezWsc+0f8H7y56026BXbjm8HfXNb7FMtStRDBjSBcOw6vX826bz8HYNTTL9OoY5cLHmNMSSF13HhsVVV4TZ2K39NPnTORdEHCAl7f+br96/Z+7Xm8w+MUGgp5YtMT2GSlMN/XA7+me3B3ZZDNpsze7Pm6ZrVjzwYQ2Q8q8sDZF/q/zKFCFT9+sI+GZrWyRNXVn0G3NvlPVTP+I/4P3tn9DhbZgoREV4+mPBTcn9bOoZCxB+JWQmHi6QP6PAv9XqiXa8/dnsLry5TZgFdGRHF3jwiRMHyZbPwplpjt2TTtGsDAu5TdbqXGUnr+drrZ5//a/o8H2jxw1rExhTFMWD4BV50r22/bfln/jkRwUwsR3AjCtWX9d19yaO0KtHpHJr41E5/Q8AseU7psOVlPK/16PG6/jYCXXjprBscm2/g17ldWJK/gSMER++MaSYNFthDgHEBOZQ7eem8W3rzQ3njQrjxXWWbZ/olScO5MPk1g0iK+2FNNzLJUosxKHo5JDW07q+gRlYOu/XhQ/6NBo80KVjNoL5xjdLXIskx0fjS/x/9u34U21L0ZT6bF41904uwD1A7QYjR0uBvCu/3r6x/PK+edlXFsiFNqrDwxqAmPDLj8pf1vVCaDhbnPbsditDLmyfYENfawPzfkt8GUFeUTFtaUn4b9dM7+V2abma7zu2KymVg+ZjnhbuGUmcou2K39UojgphYiuBGEa4vVYmHhjJdJjzmCh38gE2d8iKPrhf9vFv3yC7lvvgWyjNuwoQTNnImkOncaYW5lLl8e+pJFiYuQkekf2p+3e73NpJWTOF5ynGZezXi43cP0Cu519idPY4Wy5FKRB84+sOMzKMsE1yDkluM5UO7G7gQ9xsxwdFZl95SjqpThLdfgf88HSv0Wm1XZMfT3W8pW9LtXgdcl7CwxG5St7eo6zA7JsvLrPN+TGkNtNuJj/mB13O+sKU8iQ3X6beHh4lLuLSlFAqX4nnekcl6vBtB0qNLDS//vl4qqTVZmrU9gzrYUrDYZjUriwX6NeGxgYzFjcxkd3ZLJ5l/i8fB3YuJrXWp8rz958R4sx3MJ7daZ0dOfQufodM5z3LHyDg7nH+adXu/QN6QvAxcMpK1fW97r9Z69nk59EMFNLURwIwjXnqqyUua/8ARl+bn4hjdg/Etv4eR24R+KZatWkfnMs2A24/f003jfM7XW8fFF8RwtOMqIyBE4qB04XnycSasmUWlWmmg29mzM1JZTGRIxBI1Kc9bxSSVJpGbvY8CGmVBQs6CfTVaRaOjFnooJlFkDcZDKGRg2h7BGPqiyo6HojEJnvs2VxpDZ0bD/B2WXVm0tB2RZGbf2ZXDxg8mLwTNcmV06vh6aDAFnb+XrtS9C2m5lG7bOCQa8quQN/SNAkGWZxJJEVh/5kbXJKzghna5T4mSzMaiyivHlFbQ1msAjHDrfBx3vBl399mmSZZktiQW8uuQoqYVKOf9BUf48P7QZDX1vzK32V4qhwsz813ZSXVZOz1tb0XZgmP25grRU5j39kP1rd/8AxjzzCt4hYWed59097zI/dj53NL+DCLcIZuyeQUP3hvw16q96DUxFcFMLEdwIwrWpMCONP954garSErxDwhj9zCt4+Adc8Lji334n57XXQKMh4tdfcGzV6qKum1uZy08xP/Fnwp9UWZQ31yDnIEZEjmBoxFB7V+SC6gJGLxlNqbGUz3q+S9/CLKXxY0kaWAwgW5G9G/MbN5O6ugw3sxOOqhJGe72MlyaDCsmFL0wjuEuzGn+pBNzDoDTt9I10uBsGv6Uk4Z4p56jS8TzldOdm3EKg+0NKxV9DCehcod0dcPgPqC46+0U2GggN+oDGgSoHV34rj+evzM2kGAvtQxxsMr11PgwOH0jvgK44VRYos0yhXZRKz5fB4YwS3l4Zy65k5Z4D3PS8NbolA6NE7ZorYcMPMRzdtAxL9VZ6T7qHTiPHnH7u+y+JXrOCwEZNqSgporwgH++QMCa9+wkabc2Zw2VJy3hh2wu09m1NtaWaxOJEnuv8HHc0v6Ne71cEN7UQwY0gXLuKsjL5880XqCgqBEkitHlLOo+5lYjW7c57jCzLZD76GOVr16INC6PBooWoXS7+E3+psZTf439nfux8igynA4ShEUN5o8cbPL3laTalbwKU5OR5Q+ed91yFRdX88d5ObKUgS2aSnOPZoAmgQnKmFcf5Q/cmDpIZJBU07AtJG1Eq50jgFqQkMHtFQHWJUk0ZQOOoFBk89FvNJF4HNzCWnf7avxXc9JZyjrjlWNe/zlGNTIFaTYZGwzx3V/I1yqyUzibTs7qamzxb0GfgBzj7Nr3o79ul+mNfOi8sOoLFJqPTqJjcNZxHBzYWW7yvkLSYQpbO2oWxdA5gRqXWMHHGh/g3iMRkqObr+6dgqq5m3Itv4hfRkHlPPUhVaQldxkyg522TqSwpxmaz4urlQ2ppKiP/Ugr8ycjo1Xo23Lqh3vNuRHBTCxHcCMK1rSQ3h3XffEra0cMAqNQa7v7oSzwCAs97jLW0lOQxY7BkZePSpw8hs7845xbxujBYDGxI28Dq1NVsy9iGRbYQ5BxEVmWWfanKYrPw87CfaePb5vznqTCz/MtD5CaVYQNyQnVMu6ctb6yKwZa4ntsc99B01NM0bN0DkjcpVZNL0s59shZjoP/LSr5LRZ7STTs/Xtmd1P0RiF0Ke76BiF7Q5xnQOAAQWxjLG1uf52hpzcaGwVaYbnVkUORIXFqMA/+z+0FdLgazlU83JDJ7k3JPQ1oE8PLIKII9HK/YPdyIygqrUWtUOLrqOLIpg11/JVFdshmr8fSuQK/gUCa9O4vYLRtZ9+3neAQEMvXjr5FUKhJ2b2fZR+8gqVREduhM0v49qNRqhj74BI279qDnbz0pN5UDMKbRGN7o8Ub9vwYR3JyfCG4E4fpQlp/HqtkfkRFzlCZdezLy8edqHV995CgnJk1CNhrxmjoV/2ee/tf3sDdnL49vepxSYykAj7Z/lNTSVJYkLWFg2ECe7vQ0y5OX0z2oOy19WgLK8lVKaQod/Ttis8ps/jWe2O1KoT83Hz1tRzbgf5tiSS+uBmBAMz/GdwihZyNvXG1lSh+m4hTld1MFtLkN/FvUvDGbFcxVZzWbLDeVM+fIHGIKYyg3lRNbFItNtuGkcaKRZyM8HDzoHtSdW5rcgk59ZbtmF1Wa+HVPGnO3p1JQoVQtfrh/Ix4f2ASVSiQMXy4mg4WtvycQt1PpD6XWqrCabci2Kkzl3yHbzNx0/6Ns++1HKkuKcfb0wmI0YqyqpM+kqXQcOdZ+ruWz3iN+59azrtH7jrv5xmEVu3J2AzB/yM9EOoTh7FG/tZ9EcFMLEdwIwvUj/0QKPz77CMgyt785k6AmzWodX7piBVlPKn2nAt9+G4+xY2odXxcnyk7w8vaX8XX05b3e75FamsqYpWOQkFBJKqyyFUeNI3OHzMVF68Kdq+6k0FDI4x0eZ2rLqciyzLZNR0hcXUp1qVIt2aehG3s8ZRYl53HqJ7BGJdHQ1xl/Nz0NfZwZFBVAl4ZeaNUX3u1ktVlZn7ae9/a8R351fo3nboq4iWc7PVujiOGVIMsyaUVVHMooZe2xHNYcy8FsVV5skLueZ4c2Y1Tb4Ct6TzeKnORS0mOLMBmspETnU5pfDRJIKLnpGp0KT9+9pB/dTEBkYybO+IgThw+yZOYMLCYl8NQ5OjLts+9q7FysKitlzZezcHR1p92QERzbvIGDq5cBIDX0YUHoIdqZGtAu2QvPwGDGPf/6uW7vkongphYiuBGE68vqL2dxbNN6gppGcdvr711w90X+p59SMPtLJK2WsHk/4NT+4nsZXciDGx5kS8YWALz13hQaCvHSe6FT6+wdlFWSii8HfsmOzB3Mi5lHgDaIR6TXydxuwGqxIakkmg8LYxtGNsblkVxQedZ1PJy0jGkXzB1dwon0deZE2QmKDEVYbBaqLdUUGgo5XnKcNSlryKtW6sKEu4Vzd4u78XH0IdAlkCael7/3lSzL5JYZOZRRwuGMEg5nlHI4o5TS6pqtL1oFu3N3jwhGtgmqU9AmXByr2cbupckcXJ+mpHCd5OLpwKCpUfhFuFGcW0n0qvkcXq/UMBr3/OtEtO0AKMFLSU421eWleAaG4BVUe/ApyzIHVy9jy8/fY7XU7Aru6ObOXR/OrtOux7oSwU0tRHAjCNeX8sICvn9sOhaTkTaDhtJ/6v2oVOfPp5FtNjIfe5zytWtRe3nR4M8/0AbX7wxBflU+CxMX0iukF+Gu4dy95m7iiuIAaODegCaeTViTusaeYHmKg9qBkb7jsG33wS8vEoDU8AMcb74Di03GQeWKk+RHSaXEiZJsTJSDrAIJHJxysEoV570nV50rE5tN5N7W9+KgdqjX13supVVmftyZyuaEfBLzKs4KZAB0ahXNg9zoEObJ2PbBtAwWrRMuF0OFmaWfRpOfpuS9NGjjg4NTFRUFMYRGBeDm601JdibJB/aSHqMUtew9aSqdzlh2ulSFmems++YzMuNicHL3oNPIsbQZNKxODXEvhghuaiGCG0G4/hz5ey1rv/kMZJnGnbsz7OGn0OjOnzNiq6oiddIkjDGxODRuTPjPP6F2v3xvrPlV+dy//n5UkorP+3+Ou4M7k1dNJq4oDkeNIy93fZk1qWvYnHFyO7cM7bIG0iVtJAB5zmnsiFhMjltyLVcB2aZBhxfezo54OTnjrffGz8mP3iG96Rnc84rk0aQXVfHzrhPM351GhfH0p3WVBE38XWkd4k7rEA/ahHjQNMAVnebGnqGpKC7Cyc0d1SUmuNeFxWxl6SfRZB8vRe+spd/kZmBLZeVnMzFWnT0jqNZoGPLgEzTr3rve7kG22chNScI7JBStw+WpwC2Cm1qI4EYQrk8Ju7ax8rOZWC0WGnXqxsgnnqt1BsecnU3qrROw5Ofj2LEDYXPmoKrnT5Jnssk2JCT7slleVR4LEhYwJGIIDT0aYrVZ+eHYD2RXZtM5oDOtfFqRtD+PIwsLsZmUcziEWTCGFWAMK8DPxwtPvSc22UZ+uYG9iRo2RGswWZVgoam/K90ivWkZ7I4syxjMVtwctfi6OuCgUVNtspJRXMWelCKOZSlbxbUaCU8nHcEejrQKcWdc+xD02vN/D0urzGxKyONgWgmVRgs5ZQa2Hy/AdvJdo1mAK3f3iKBVsAcNfZ1rPdeNwGwwYDYZ7UsxKdH7Wfzu60T17seQ/z1+Wa4p22SWfrKB1EP7kK3peAXqcHB2ICP2GMgyvhENcfH0orK4GDdfPwIiGxPZsUud2pxca0RwUwsR3AjC9Svt6CEWvfMqVouF9kNvpt9d99U63hAfz4lJk7GVl+M6aCDBs2Zd8hbxy6WqzMSeZcnEbMuyJxerNBLtB4fTfkg4Wt3p+80pNfDNlmR+2XMCg9n2r6/t5+rAhE6hlFWbySwxoFaBXqumrNpMdqmBxLwKrLaz3yJ6NvLhzu4RDGjmJ3Y6nWS1WJj/4hMUZ2cy8c2Z+IRF8PNzj5GXmgSSxF0fzsY7OJTUwwfJSYyn06hxqDW11/SxWa3kJCXgG9EQre7spUZDRQUL3v6Q3KS95zga2gwaRr+77r3gda4XIriphQhuBOH6FrdjCys+eR+AvlPupcPwUbWOr9yzh/Rp9yKbTHjfey9+Tz5xJW7zopUVVJN0IJ/j+3PJO6HkTbh66+l1a2MatKm506mo0sTWxHwOppWQkFuOTqNCr1FTWm0mt9yA2WrDSavBy1lHxwhP2oV5oNeoMVptFJQbSS+qYuGBTDJLqi94X039XenV2AcvFx3OOg09G/sQKdoinCV67Uo2fDcbgMDGTek8+laWfPCm/fkWfQfSaeQ4fn7uUSxmE91vuYNu428HoKKoEL2rW43Kv9XlZSyf9S5pRw/j4u1Dj1vuQK3RkLB7BxVFBTi6upGbkkxVaTEg4Rvegpb9euDm44vZUI2rty8hUS2v6PfgchPBTS1EcCMI17+9SxeyZf5ckCRGPv4cTbr0qHX8mV3Egz74APeRI67EbV4SWZZJPpjPtj8TqShWtuWGt/Km47AI/CPc6q1Xj8liY8H+DPamFuHvpifYUymiZzRbcdVrCHB3pKGPM6Fe526WeKMzVFZwZMMawlq1xTMgkO8evY+q0hL78zpHJ0zVVYS3bseJwwdRqdV4BARRlJkOKHkvk9//jBOHo9k47xvcfHxxCxhGSZ4XFmMapor12Cwl5774GSSVJ4HNxnPbKzf/5xuMiuCmFiK4EYTrnyzLbPj+Kw6tXYFGq2P8yzMIbtq81mPyPvyIwm+/RdLpCPv+O5w6drxCd3tpzEYr+1amEr0+DdvJ+jAung406xZIu0Fh6BzPbuwpXD5mgwGLxYyjiytmg4E/Z7xEdkIckkqFf8NG5BxPwDMwiNYDhrD55+8B0OgcmPbZHFZ+9oG94rajqxs+oeGkxxzBxctbaTVSgxYwnzzek1tfeYmM2KMcWLUUrd6Rpl174BcRSdKBdBL25KFxbMrtr/TAM6B+G5pei0RwUwsR3AjCf4PNamXJhzNI3r8HBydnhjz4BI06djnveNlqJeOhh6nYuBHJyYmwr7/CqVOnK3jHl6Y4p5K9K1JJPVyA2WgFQO+ipfOIBrToHSxyXq6AypJifnnpKSqKCmjesx8VxYWcOHwQtVaL1Xx6C/zNT7xAZKcu/PLik+QmH7fnhZ04HM2CGS8BMOrpl/ELb8APT/4Ps9EAQJcxE0g9nE1uklI7Se/shtUWgUrXg36T29Ki1+lSBumxRexcnGTf8t1xWARdbm54pb4VV9V1FdzMnj2bDz74gOzsbFq0aMGsWbPo1avXOcdu27aNZ599lri4OKqqqggPD2f69Ok8/njds9BFcCMI/x1mg4GF77xCZlwMAJ1GjafnbZPPu4vKZjCQ8b8HqdyxA8nRkbA53+LUocOVvOVLZjFZSTlcwJ5lKZTknuxe3tiDAXc1x81b9GX6t2SbDYvZhCzLaB309iUes8nIn6+/QPbx+BrjtQ56xr/0FtXlZWz95Qd8wxsw7OGnkCSJiqJCEnZto9WAm9A66JFlmQMrl6BzdKJV/8EAHNu8gc0/f0+X0bfQbsjN/PDcdipL8ul5axhtBrTjyMYstv2ZiIOThhEPtcHNx5FdfyURuyP75PXVtB0URseh4ahukIKI101w8/vvvzN58mRmz55Njx49+Prrr5kzZw4xMTGEhYWdNf7gwYPExcXRunVrnJ2d2bZtG9OnT+fjjz/mvvtq3zVxighuBOG/xWoxs+XnuRxYtRSAZj36MPTBJ85bV8RmMJDx0MNUbtumFPlbuABt4Pmbcl5rrFYbMVuz2LE4CYvRik6vps3AMFr2DsbJ7cr2i7pemaqr0OodkSSJ7MR41n83m7yU081FJUmFg4sLPiFh2Gw2suJj0Du7MHj6I8Rs/Zvc5CQG3/9Ird3q60KWZSRJIj22iKWfRKN31nLX+z1Qq1XYbDIL3t1nn6E5fXPQqm8InYZH4OhyY/19XzfBTZcuXWjfvj1ffvml/bHmzZszevRo3nnnnTqdY+zYsTg7O/PTTz/VabwIbgThvyl22yZWz/4Ym9VKk649GfbwU6g1585LsRkMpN4+EWNsLPrWrQn/+SdUtRQFvBaV5FWxfm4MuSlKDRu1RkXTLv60GRCGV9B/P//iXOJ3bmXXwt/oP/V+QqNanfV8eVEB67/9guQDe3H29MIvoiGp0QeQ5dq31avUasa98CZhLVtflvv++6dYYrdn06JXEH3vON0/rSS3iq1/JJKbWoqx0oK7nyP9pzQnqJHHZbmPa911EdyYTCacnJz4888/GTPmdHO7Rx99lOjoaDZv3nzBcxw8eJChQ4fy1ltvMW3atHOOMRqNGI1G+9dlZWWEhoaK4EYQ/oOO793Fso/fxWa1ENmxKyMee9a+vfbUp+RTTBkZpIwbj620FOfu3XAfMxaX3r0uayXj+maz2kg6kE/0+jT79nGA8JbedB7ZAL/wG+dnXEVRIXOfeABTdRV6F1cmvfMx7n4B9udjtvzN33O/PmfF3ua9+tHztsnoXVyRkDBWV1FVWkL+iRTy01IJjWpFZIfOl+W+rRYbc5/ZhrHKwujH2xHc9OxO2rIsY6gwo3fWIt3AOVbXRXCTlZVFcHAw27dvp3v37vbH3377bebNm0d8fPx5jw0JCSE/Px+LxcJrr73Gyy+/fN6xr732Gq+/fnZnUhHcCMJ/U8rBfSz5cAZWs5kGbTsQ0aY90WtXUJafh6u3L17BIQy672FcPL2o2LqV9On3g0355K729CTi99/QnWNZ/FomyzI5SaVEb0gnOTrf3jSxYTtfOo9sgHfQf6cuTUVxEbnJxwlp3hIHp9Pb1Jd99A4Ju7fbv/YNi2DC6++h0mjYOPdrjvy9FoCARk0YOO1BDBXlZCfEEdi4GeGt217pl2F3aEM62/5MxNldx5R3eogE8VpcV8HNjh076Natm/3xGTNm8NNPPxEXF3feY1NSUqioqGDXrl0899xzfP7559x+++3nHCtmbgThxnPiSDR/vf8mFpPxnM+3GTSUgdMeBMAQE0PZqlWUrVyFOTMT5x49CJ3z7XVbM6Qkr4q9K1JI2JOrBDkSNO7oT+NO/gQ2dEfvcn1Wq009fJDNP86hIP0EAIFNmjHh1XdRazQk7d/NX++/iaRSMfrpl1nz1Sf2mjMqtRqb1QqSRPfxE+ky5tbL2ucJlB1N+1efoP3gMMJaeJ93XE5yKYtnHsBmk+l9WxNa9Q25rPd1vbsugpv6WJYCeOutt/jpp59qnek5k8i5EYQbQ0bMUZZ8OAMndw/aDx1JeKt2ZMbHsHr2x6g1GqZ99h0uXqffeIwpKaSMGo1sMhH04Uzchw+/inf/7xVlVbJnWTJJB/NrPB7Q0J2onoE06uCP1uHaakVRm5+efdTeykCtVmO1WOg4ciyRHTqz5IO3MFRW0OnmcfS+424y42JY9vE7VJYUA+Do5s7wh5++IjM0NquN+a/uoqzAgCRBj/GNad0/xB4s71meQurhArxDXMiILaKi2EijDn4Mntbiug2or5SLef++alWgdDodHTp0YN26dTWCm3Xr1jFqVO3l1M8ky3KNmRlBEASAkKiW/O/b+Uiq09tkPQICOfL3WjLjjrFv+SL6TrnX/pzVy5PSMSMo2rCBE59+RJ+WLXEMv/6aC57iFeTMkOmtyE8r5+iWTLKPl1CcU0VOcik5yaXsWJREz1sa06Sz/zX/plqSm0NeahKSSsW0z+aQm3ScpR+9zb5lizi4ailWi4XAxk3t7QyCm0Ux/ct5GKurMFRU4OLpVWsX+fqUdDBfCWxUErJNZtufiRRmVdDn9qbE7shm7/IUAPsuKA9/J/pNbnbN/x1cb65qicsnnniCyZMn07FjR7p168Y333xDWloa999/PwDPP/88mZmZ/PjjjwB88cUXhIWF0ayZkk2+bds2Zs6cycMPP3zVXoMgCNeuMwObU7qOuZWF77zKoXWr6Tz6Vpzc3CnOzuTXl5+murwMgn0AMN91Bx269cH3kYfRBgScdZ7rhW+YK/0mKT8zK0uMxO3KJmZbFmUFBtbPjSF+dw49xje6pvJyirIy+fOtFwlu0pzhjz5D4slcmtColrj5+OHm40e7oSM5uGqZvUv8sIefROtwuuu7pFKhd3ZB73zlXpcsyxxcmwZAx6Hh6Bw17Fh4nNjt2RSkV1CYUQFAqz7BODhrKS800HFYBDq9qDZd367qd3TChAkUFhbyxhtvkJ2dTcuWLVm5ciXhJz8tZWdnk5aWZh9vs9l4/vnnSUlJQaPREBkZybvvvsv06dOv1ksQBOE6E96mPf4NG5ObnMiKT96nx4RJrPr8I6rLy3D3D8DZ0Zms1CSy3JyIXLSI6gMHiPjzD9SurhRlZVKcnXnZds5cbs4eDnQYEkHbgWEcXJfGvhWppMcU8fube2jSOQC9q5aqUhPBTTyI6hl0VWYTLCYTy2e9S0VhAfE7t9Ky3yASd+8AoHGXnvZxve+YioSEk4cnnW4ee97CjVdSZnwx+WnlaLQqWvULwdFFh2egM2u/PWqfqWnc0Y9etzURMzWX2VWvUHyliZwbQRDSjh5i4duvYrNa7I+5+foz8a2ZaHQ6vrxvElazmT5F1TinZ+EyYABBn3zM94/dT1l+LhNee5eQ5td/x+WS3Cp2/pVE8j/yckDZaTVgSvMr3sNq/ZwvOLRulf1rz8AgirOzQJK4/6sfcfY4e6v0tUC2ySz55CCZ8SW07BNMn9ub2p8ryq5kww8x6F20DJneCq3u6gdi16OLef++MWo2C4IgnCGsZRvunPk5EW3aA6B3dmHs86/h7OGJg5MzDdsrPafKht+EpNVSsWEDx957h7L8XECpp/Nf4OHvxNDprRj3bAda9A6mzcBQ2g0OQ6WWSD6Yz+8z9pCwJwebTfkMbLXWXuzu3zq+b7cS2EgSwx5+Cp2joxLYAMFNm1+zgQ3AnhUpZMaXoNJItB1Ys5SAV6AztzzfiZEPtxWBzRUiFvoEQbgheQWFMPb518lOjMfFyws3Hz/7c8169CFx9w6OJ8TS9uWXyH3lVY5u3gCeroBSS6fvlHMXDr0eBTRwJ6DB6eKFke38WP3NEcoKDKz7PobtC45jtdowVlpwdNXiHexC407+NO8eWG/LKzablW2/zgOg44gxNO/Zl5KcbHb8OR+Axp171PlcydH5bPsjkRa9g+gwJOKi78VQYSZ+dw4N2vjg5nPuvl02m0xliRGbVSY7qYR9K1IB6HdHM9x9Ra+vq00EN4Ig3LAkSSKoSbOzHm/QriM6RyfKC/OpahWF69S7yd27yf58UVYGJbk5ePhfv4nGtfFv4Mbtr3Th8KYMotenUVVmsj9XXW4mI66YjLhiinOq6D42sl4CnISd2yjMSMPB2ZmuYycA0GHEaA5vWI2hsoImXesW3BzdnMGW3xKQZdi9NIUGbXzxCqx7OwpTtYWln0aTn1bO3hUpDJragvCWp0sGxO/K5uC6NEpyq7Faas5ktekfSrNu10+fsv8yEdwIgiD8g1bnQOPO3Tm2eT07/phPWKs22ParcKk2obNaKXJxJHn/btoPq3vZinOx2awk7d9DWIvWODhdW/2gdI4aOg6NoHW/EPJPlKN30eLoqqO8yEDq4QL2rUwlel0aFcUGGnf0x9FFS9LBfDLiiglo6EbX0ZHonc9dMNBkqEarc7DvZrPZrOxc8CsAHYaPtn8vdHpHJr0zC7PRiKu3zwXv+djWTDb/mgCA3kWLocLMzkXHGf5gmzq9ZqvZxqqvj9iTf41VFpZ/cYg2/UJpOyiM+N3Z7Por2T5epZZQa5TXENnel+7jIut0HeHyEwnFgiAI55CTlMivLz9dI+m4Y+MWVGzeTFyQDwEaPbd9PQ+1y6UHJbsX/8G2336sUTH5ehGzPYtNP8dxvncQR1ctvW9rSqMOfjUez09L5ZcXnyS4WRRjnn0VtUZD7LZNrPxsJnpnF6Z9/n2Ntgp1ZbPa+OmlnVQUG2l/UzjNugXw2xt7sNlkbn6sLaHNvGo93mq1sW7OMZIO5qN1UHPzo22J25XDsS2ZAEgS9tfablAYLfsE4+KlF+0SriCRUCwIgvAvBUQ2ZuKMD/EOUZJDJZWKjk89R8v/KXW18kxVJE26A3N29iWd32a1Er1uJQAnDkfXyz1fSVE9grj50bY06xqAd7ALDs4aGnX0o9+kZngGOFFdbmbNt0fZufg4NquN5Oh8/v4pls0/zcdiMnLi8EE2/vANGbFHWT/nCwA6jBhzSYENQHJ0ARXFRhxdtXQe0QDPAGda9AkGYNsfiZgMlvMee2Zgo9JIDJ3eioCG7vSd2JThD7YmqLGHPbDpMb4R3cc1ws3HUQQ21zAxcyMIglALs8nI4XWrcfP1pXHn7siyzLfTp1BeWkxkbjGNZA2Rn32GY+vW9vFVJcU1OlKfy6l+SKfc//VP1/RuoIthtdjYsyyZA2uUOmUOzhqMlRZs1lJMZd8Dsn0qRKXWYLNaCGnekpYDHyAzvoKonkGE/KM7dkWxkdgdWWQlllCcXUlEG1+6j420F8BbNHM/2cdL6TA0nK6jlOUhQ4WZ+a/twlBhJqyFN8P+1wq1WoWp2oKkllBJEieOFXJ4Y7p9p9PQ6a2IaHX2Elh+ejk2i4x/A/G+cbVcF72lrhYR3AiC8G9t+vFb9q9YAoAky/iVV9OqZ1/kDu3Yueh3qkpLaNi+Ez0mTMYvouE5z7Ho3ddIObjP/vXIJ56nSZe67wi6FhirKjFWVdbYaXam+N05bPwpDqvFhs5Rg1raSnHmTlSacLROYRjLtgLg7t8MWT0UU7UyEyKpJHre0phWfYORJEmZ9fkxFmNVzdkXVy89PW9pjLOnAwve3Yekkpgyozsung72MTnJpSz5+CAWs43ARu5Ul5spya06617VGhVD729VI3lYuLaI4KYWIrgRBOHfMhsMHFq/itgtG8k7kVzr2CbdetH9lol4B4faHyvNy2XOI9NAlglv3Y4Thw/Sftgo+t15by1nurbINhvzX3yCgvQTTH73E/vy3T8VZFSQl1pGSFNH5j45DYvRiE+DOygv9sNq2I0sm9E4dkOSNLh66/H0dyItpggAzwAnNDq1PcHXN8yV5t0DcXLXsX3BccoLDcpFJECGRh38uOnes4srph4pYOWXR5BtZ7/dObvrlG3tPYIualeVcOVdF40zBUEQrldavZ6OI8bQccQY8tNS2fftlyTEHkFtsxFZUEbkkGEk6NXE795Ows6tJO7aTpexE+hx6x0ARK9dYQ9sWvQdyInDB8mKj7no+yjJzSF6zTJ7j6wrKfXQAXKTjwNwaN0q+t997jY4PiEu+IS4sPnn77EYjfiGN+COt27l6JYsdi/VYDHZiGjlTYvewYQ19wIJDq5LY+fiJIpzTs+wtB0URtdRDe27k0Kbe7FvRSrxe3KoKlW2qrfuH3rOe4ho5cPQ6S1Jjy0muIkHwU09UaklLCYbji5aJJE7858jghtBEIR/wTcsgqFvvseArCxy35pBRczfWH+cT1NfX6LunMThohyS9u9h18Jf8fAPwNnTi/3L/wKg7U0j8ItoAEBuShJmgwGtXl/L1Wra8vP3JO7ZgcVkZuC0/12Ol3de+1cusf85Zuvf9LrjLowVFWyc9y3BTZvTbshI+1bvE4ej2bd8MQDdb7kDtUZNm/6htOgVhNUi4/CPFg/tB4cT2c6PsvxqzCYrbj6O+ITUbICp02voPq4RXcdEkpNUCsgERp4/wGvQxpcGbXz/cY5/8x0QrmUiuBEEQagHuqAgQr74nPJ168h7/wPMGRlYZn5M59Gj8Rs/kZ0LfmHdt5+j1TkgyzZa9htEZIfOSJKEq7cv5YX5ZB+PJ6xl3WqyWMxmUg8fBCBxzw76T51+WZpHlhcVUJieRnjrdvZifQVpqZw4fBBJUuHo5kZVaQmJu7ZzbPMG0o4eImHXNlKi99P3znuRJIlVX3wIskzrgUNo1Kmr/dwarRrNuUvh4O7rWKdKvyqVRFBjj/p4qcJ/iNgKLgiCUE8kScJt8GAarlyB39NPg1pN6V9/0Sgrn4YdOmM1mzFUVhAQ2ZgBUx+wBwvBzaIAyLyIpamM2KOYDdUAVJWWkBl7rF5fS2leLmu++oQ5D01j4duvsH/FX/bn9q9cCkCjzl1pe9NwADZ8/xVpRw+h0erQ6BxIPXSAH554gLmP309lSTHeIWH0vY5yioTrmwhuBEEQ6plKp8P7nqkEvv4aAIVffkVXj0ACIpvgGRjMyCdeQKPT2ccHN1WCm4yYI3W+RvKBPTW+jt+57d/f+EkWs5nfX3uOoxvX2YsY7vnrT0yGakrzcojdthGADsNG07LvICRJhalayY/pPXkqd7z9EYGNm6LR6kCScHL3YMSjz6DVOZz3moJQn8SylCAIwmXiMX485qwsCmZ/SclHH9GjY0f8X38NR5+auR+hLZQaOWlHD7Pjz/l0Gz+Rwow0shJi8Q4Ow69hJObqakpys/EOCUPn6ETyfiW4aTNoKIfWrTrn0pSpuoqU6APkpRyn7U0jzmphkHJwH3E7ttDj1km4+Z7ezp24Zwflhfk4e3gy4vHnWPPlLEpysjm4ahkp0fuwms2ERrUiqGlzJEmiQbsOJB/YS3jrdrQdPBxJkpj41of288myXG8NNgWhLkRwIwiCcBn5PPwwand38mZ9QvW+fZwYMxaf/z2A99SpSCdnb7xDQuk18S62/vIDOxf8SuLuHRSknzjn+Rycnel3532U5uWi1mrpefudxO/YSlVpCRkxxwhrqQRKO/6cz54lC7CazQBkJcZx6yvv2IOMsvw8ln38Lmajgaz4WCa89i4uXkqNl+g1KwBoPXAoIc1a0HXsbaye/THbf/8ZWbahc3Tkpgces59rwNQH8G/YmLY3DT9nECMCG+FKE8tSgiAIl5EkSXjdeScNly3DuVcvZJOJ/FmfkDJuPIb4ePu4zqPG03fKNAAK0k8gSSqCmkbh5O5x6kToHJ0wVlayevbHgDLjo3d2oVHnbgAcWLUUi9nMgZVL2LngV6xmMx4BgWi0OjJijpKwazugzKSsn/MFZqNSJ6YkN5s/33qJypJi8lKTyYqPQaVW03rATQA079kXz8AgZFnpgt3vrum4+/nb793N14/ut0y84tvRBeF8xMyNIAjCFaALCSb0m68pW76C3LffxpiYSOott+L/wgt4TLgVSZLoMHw0rt4+FGVmENWnP24+fsiyTGVJMXpnF2xWC4vefY3MOCXxOLJ9ZwBa9B3I0Y3rSNq3ix+ffpDiHKXfVa+Jd9Hp5nHsXPALOxf8yuafv6Nh+44k7NpOSvR+1BoNo55+mbXffEZRZjo/PvMwXkEhADTq1M0+k6NSq+kxYQrLZ71L0269aNFnwFX4DgpC3YkKxYIgCFeYpaiIrOefp3LzFgDchg0l4I03ULu4XOBIMBmqWfbxuxScSGHSu5/Y+1El7d/Nmq8+pbqsFFBycQbc8z8kScJsNDD38QcoL8xH5+iIqVrZZdXj1kl0HXcbRVmZLP1wBoUZafbr3PrqO4RGtapx7dK8XFx9fC7LlnNBuBDRfqEWIrgRBOFaINtsFM39gbyPPwaLBW1YGMHvv4dj27Z1Pv5UkbxTKkuK2frLPLR6Pf3uvBeV+nQQEr9zG8tnvQsoMzGNOndn2ENPoD5ZaMZiNrNzwS/sXbKQwCbNuO3190SujHBNEcFNLURwIwjCtaQ6OprMJ57EnJUFgHOf3vg++KC9y3h9Sjt6CJVKjX+jxufdlm2oqECt1aB1EOV7hWuLCG5qIYIbQRCuNdbSUnLfeZfSpUvBZoOTSci+jz+GykHUhhEEuLj3b7FbShAE4SpTu7sT9O47RK5aidvNI0GWKfrhB1LHj6dy586rfXuCcN0RwY0gCMI1QhceTvD77xPy5WzU3t4YE4+TdvdUTtx5F1UHDl7t2xOE64YIbgRBEK4xrv360XDZUjwnT0bSaqnavZsTEyeSNn16jdo4giCcmwhuBEEQrkEaLy8CXnyByDWr8bhlPKjVVG7eQsq48RTOmYNss13tWxSEa5YIbgRBEK5h2qAgAt98k8iVK3AZOAAsFvJmfkja3VOpPnL0at+eIFyTxG4pQRCE64Qsy5QuXEjOjLeRTxbic+7ZE7fhw3Hu3h2tv98FziAI1y+xFbwWIrgRBOF6Z0pLo2D2l5QuWwZWq/1xh8aNce7RQ8nTOXAA2WQi5NNP0AYGXsW7FYT6IYKbWojgRhCE/wpTWholixZRuX0HhqNH4Rw/zl0GDCD0i8+vwt0JN6qin+fj1KE9+ubN6/W8IriphQhuBEH4L7IUF1O1axeVO3YCMrrISPJmfggWCyGzZ+Pav9/VvkXhBlCyaDHZL7yAysWFhsuXoQ0IqLdzX8z7t+gKLgiC8B+g8fTEbehQ3IYOtT9mLSigcM535L71FtrAACS9Hl14+Fk9qQThUpzKAavYvAXXwYNROerJfvllADxvm1Cvgc3FEjM3giAI/1G2qiqSRozAkpVtf0wfFUXIl7PR+vvXemzpihWY0zPwvncaklp0ARdqMufmkv3iS1Ru23bWc+6jbibwnXfqPYgW7RcEQRAEVE5OBM2YgTY8DLWPD5JOhyEmhtRbJ2CIizvvcaa0NLKeeZb8WbMo+PKrK3jHwrVOlmVKly4leeTNVG7bhqTT4XHLeDQnZ2mce/Ui8K23rvrsoJi5EQRBuEGYMjJIn34/pqQkUKtx6dMH14EDsZWXYauuxn3kSLTBwWQ99zylf/2lHCRJhH77LS49e1zVexeuPJvJhCk5GV1oKJKjI1V791H0ww9UbNwIgL5VK4LefQeHyEhkiwVjYiIOjRsjaS5PxotIKK6FCG4EQbiRWUtLyXr2OSo2bTrrOY2fH4Ez3iJ9+v1gs+HcvRuVO3ai9vSkweJF9hyKstVr0Pj64NShwxW+e+FKMR4/TvoD/8Ocng6ShNrNDWtpqfKkVovvgw/iPe2eyxbInIsIbmohghtBEAQwJiVR8sefVB87isbHF2NcHKbUVPvzLv36ETzrY1Jvvx1jTCyO7doR/uM8ShYuJOe115EcHIhctRJtUNDVexE3ANlmu6JLPNaKSiq3bCb7lVexVVQgabXIZjMAKhcX3IYNw3PSHeibNLli93SKCG5qIYIbQRCEs5nz8kibPAXTiRMANFi0EH1UFKb0dFLGjsNWXo5Lv35UbN0KFgsAbiNGEDzzAwwJCZQtXYr7mDE4REZezZfxn1K2ciWZTz+D31NP4X33XeccU33sGPkffoRT5854TZmMysnpvOezVVVhzs3FVlkFNivW0lKMiccxpaRgzsvFkpWNMSkJTvYtc+zQgZDPPgVZxpyejkOzZqj0+svxUutEBDe1EMGNIAjCuZmzs8l6+hkc27XF78kn7Y+Xb9hAxoMP2b926tyZqr17QZbxe+YZCmbPtn/K93nwQbzvmYqk1dbLPRV89TWmtDQCXn0FlYNDvZzzemCrquL4TTdhzS8AtZrwn37EqX37GmMMCQmkTZ5iXy7S+Priccst6Fu2QO3hgTk9HWNyCoYjRzDExmItLq7TtbXBwbjedBO+jz2KSqer99d2qURwUwsR3AiCIFy83A8+oOi773GIak7E/PnkzJhB6YKF9ufVXl5Yi4oAcOrYkZDPP0Pt4fGvrmlMSSF56DAAPG65hcA33/hX57ue5M+eTcGnn9m/1gQF0nDxYtTu7tiMRqoPHCDzmWew5hfg0KwZtvJyzJmZFzyvyskJlbs7kiShcnZGFxmJQ8MGaAID0fr749C06QXLBFwtIriphQhuBEEQLp4sy1Tv34++RQtUjo5Y8vNJumkItqoqXPr3J/ijDylfs4acN9/CVlGBLjISn/unU3XgAFis+Nw/HW1w8EVdM+fttyn+8Sf714HvvIPHmNH1/MrAEBeHbLXi2KIFoMxgFcz+Ek2AP46tW+PUpcsVncGwFBSQNPgmbFVVBLz5BoXfzsGclqYEJm5uWIuKkE0mAByaNCH8x3lITk6ULV1K1d59GGJjsVVUoA0NRRcejj4qCn2LFugiwlG7ul6x11HfRHBTCxHcCIIg1I/qw4cxxMXhMXasfdeMIT6B9OnTseTk1Bircncn6J23ce3fH9lmo2LjRorm/gBaDcHvv4/G1xdQlmNUTk7YqqpI7NMXW3k5zj17KjVV9HpCPvsUl169Lul+ZasVc3YO1uIi9C1aIKlUGFNSSBk1Gtlmo8HCBTg0aULa1KlU7dxlP86xQwfCf/qxRmKvbDJRtW8fjm3aoHJ2vqT7Oec9yjLZzz1H6ZKl6Fu1IuL33zAciyF92rTTu5UAta8PLt174Pf0U2h8fOrt+tcyEdzUQgQ3giAIl5c5J4fMJ5/CWlqCc9duShB0+DAAkl6PSq/HWlJiH68NDSXgtVcp+f0PyteuxW3YUPQtW5H3/vtow8OIXLGC9Af+R+XWrQB43nEHnrffhsbfH0NsLOXr14PFgteUKegiIpQdP1u3oHJ2xiEykupjxyj57Xcq9+6Fkzt/3EePJvDtGaRPm3ayH5dSt8V76t1kPv4Ekk6H6+DBlP/9N3JVVY1Zo4pt28mdMQNTSgr6qCjCfpyH2sWlXr539uUoSVLybDp2BMBWWYk5Nw9bVRVqF2e04eFIklQv17xeiOCmFiK4EQRBuLJkk4m8Dz+i6OefwWoFlG3FHhNupXztOqWWynn4Pfss3nffhc1gIO+DmRTPn3/+C2m1uPbtS+XOndgqKs45RNJqka1WsNlw6tKFqt27kXQ6JJ2uxtZnn//9D99HHqZwzhzyZn6Ixt+fhsuWkvvee5QuXFTjnE6dOxP67Tf2hGdZlrFVVmItKcFaXKL8XlqKxs8Xp/bta9SGsRkMmLOzseTkULV3HwWzZwPg/8rLeE2cWKfv741CBDe1EMGNIAjC1WGrqsJSWIi1pBRdgwjULi5Y8vNJmz4dY0wsLn364HbzSPI+mIklJwdJr6fx5k2o3d3t56jYuo38Tz/FlJqKrbwclbs7rv37Y8nPr9HnSBsWhspBhzElFbW7Ox7jxuE+ejS68DBKFy8m+6WX7WN9H30EtacXOa+9BijJu5ErVqBydMRmNJI8dBjmrKzTSdMqFV6TJ+HSrx8ZDz6ErbIStacnkqMe2WRWlo9OzhD9k9rTE33rVljzCzDn5NiTsM/kff90/B57rH6+6f8hIriphQhuBEEQri2y2Yw5JwddaCgAluJiCr+dg2ObNrjdNPi8x9kq/9/e3QdFVb59AP8usCyICCEhu4JA5MsktBNo5ku+LThWMwAADxJJREFUpUyMho6NQNmIj2ljqalpappp2oyOpX/4w7c/8G1qhppSx/nhVDAC6RBFgIXIACWBJUiZKIHAyl7PHz6cxyMIq67s7vH7mdmZ9T73Hq7L69xzLs6eZZugMxiUKyH/fvcdmr7Ph8+Y0fAZOxY6N7dbf4DOw6PTWzh/7d6Nv/+TCs/ISEQcOwqdhwdq/mc+mn/4ASGp/4HvlCnK3Gv/zcClVasAAG6+vhi4cyf6Pj8OANCU/wMuvvkm5MaNTvHpDAa4+/vfevTrh9aKCtV9M8q8Pn2gNxqhNxrhM24sAlJSHrm3nGzB5qYbbG6IiEhE0PLLL/B84gnlE0TWlhbcrKuDZ3i4eq7Vikur18Dy558wfrSl0x8qbG9oQNvFPwAIdB4eSkPj5u2t3o/FguaCArTVXITHgCCloXHr14/NjA3Y3HSDzQ0REZHruZfzt2O/k5yIiIjIztjcEBERkaawuSEiIiJNYXNDREREmsLmhoiIiDSFzQ0RERFpCpsbIiIi0hQ2N0RERKQpbG6IiIhIU9jcEBERkaawuSEiIiJNYXNDREREmsLmhoiIiDSFzQ0RERFpioejA+htIgLg1lenExERkWvoOG93nMe788g1N42NjQCA0NBQB0dCRERE96qxsRF+fn7dztGJLS2QhlitVly6dAm+vr7Q6XR23ff169cRGhqKixcvol+/fnbdt7PQeo5azw9gjlqg9fwA7eeo9fwA++coImhsbITJZIKbW/d31TxyV27c3NwQEhLyUH9Gv379NHuwdtB6jlrPD2COWqD1/ADt56j1/AD75tjTFZsOvKGYiIiINIXNDREREWkKmxs7MhgM2LhxIwwGg6NDeWi0nqPW8wOYoxZoPT9A+zlqPT/AsTk+cjcUExERkbbxyg0RERFpCpsbIiIi0hQ2N0RERKQpbG6IiIhIU9jc2MmePXsQEREBLy8vxMbG4vTp044O6b5t3boVI0eOhK+vL4KCgjBz5kyUl5er5sybNw86nU71eO655xwU8b3ZtGlTp9iDg4OV7SKCTZs2wWQywdvbGxMnTkRpaakDI7534eHhnXLU6XRYvHgxANes33fffYeXXnoJJpMJOp0Ox48fV223pW6tra1YunQpAgMD4ePjg4SEBPzxxx+9mMXddZefxWLBmjVrEB0dDR8fH5hMJsydOxeXLl1S7WPixImd6pqcnNzLmdxdTzW05bh05hoCPefY1brU6XT4+OOPlTnOXEdbzg/OsBbZ3NjB559/juXLl2P9+vUoLi7G888/j/j4eNTU1Dg6tPuSm5uLxYsXIz8/H5mZmbh58ybi4uLQ1NSkmvfiiy+itrZWeZw8edJBEd+74cOHq2IvKSlRtm3fvh07d+5EamoqCgoKEBwcjKlTpyrfS+YKCgoKVPllZmYCAGbPnq3McbX6NTU1wWw2IzU1tcvtttRt+fLlOHbsGNLT03HmzBn8+++/mD59Otrb23srjbvqLr/m5mYUFRVhw4YNKCoqwtGjR1FRUYGEhIROcxcuXKiq6/79+3sjfJv0VEOg5+PSmWsI9Jzj7bnV1tbiwIED0Ol0ePnll1XznLWOtpwfnGItCj2wZ599VhYtWqQaGzZsmKxdu9ZBEdlXfX29AJDc3FxlLCUlRWbMmOG4oB7Axo0bxWw2d7nNarVKcHCwbNu2TRlraWkRPz8/2bdvXy9FaH/Lli2TyMhIsVqtIuLa9RMRASDHjh1T/m1L3RoaGkSv10t6eroy588//xQ3Nzf5+uuvey12W9yZX1d+/PFHASDV1dXK2IQJE2TZsmUPNzg76SrHno5LV6qhiG11nDFjhkyePFk15kp1vPP84CxrkVduHlBbWxsKCwsRFxenGo+Li0NeXp6DorKva9euAQACAgJU4zk5OQgKCsKQIUOwcOFC1NfXOyK8+1JZWQmTyYSIiAgkJyfjwoULAICqqirU1dWp6mkwGDBhwgSXrWdbWxs+/fRTzJ8/X/Vlsa5cvzvZUrfCwkJYLBbVHJPJhKioKJes7bVr16DT6eDv768a/+yzzxAYGIjhw4dj1apVLnXFEej+uNRaDS9fvoyMjAy8/vrrnba5Sh3vPD84y1p85L44097+/vtvtLe3Y8CAAarxAQMGoK6uzkFR2Y+I4J133sG4ceMQFRWljMfHx2P27NkICwtDVVUVNmzYgMmTJ6OwsNDp/+LmqFGjcOTIEQwZMgSXL1/GRx99hDFjxqC0tFSpWVf1rK6udkS4D+z48eNoaGjAvHnzlDFXrl9XbKlbXV0dPD098dhjj3Wa42prtaWlBWvXrsWrr76q+kLCOXPmICIiAsHBwTh37hzee+89/Pzzz8rbks6up+NSSzUEgMOHD8PX1xezZs1SjbtKHbs6PzjLWmRzYye3/0YM3Cr6nWOuaMmSJfjll19w5swZ1XhSUpLyPCoqCiNGjEBYWBgyMjI6LVRnEx8frzyPjo7G6NGjERkZicOHDys3L2qpnmlpaYiPj4fJZFLGXLl+3bmfurlabS0WC5KTk2G1WrFnzx7VtoULFyrPo6KiMHjwYIwYMQJFRUWIiYnp7VDv2f0el65Www4HDhzAnDlz4OXlpRp3lTre7fwAOH4t8m2pBxQYGAh3d/dO3WZ9fX2nztXVLF26FCdOnEB2djZCQkK6nWs0GhEWFobKyspeis5+fHx8EB0djcrKSuVTU1qpZ3V1NbKysrBgwYJu57ly/QDYVLfg4GC0tbXh6tWrd53j7CwWCxITE1FVVYXMzEzVVZuuxMTEQK/Xu2xd7zwutVDDDqdPn0Z5eXmPaxNwzjre7fzgLGuRzc0D8vT0RGxsbKfLhZmZmRgzZoyDonowIoIlS5bg6NGjOHXqFCIiInp8zZUrV3Dx4kUYjcZeiNC+WltbUVZWBqPRqFwKvr2ebW1tyM3Ndcl6Hjx4EEFBQZg2bVq381y5fgBsqltsbCz0er1qTm1tLc6dO+cSte1obCorK5GVlYX+/fv3+JrS0lJYLBaXreudx6Wr1/B2aWlpiI2Nhdls7nGuM9Wxp/OD06xFu9yW/IhLT08XvV4vaWlpcv78eVm+fLn4+PjI77//7ujQ7subb74pfn5+kpOTI7W1tcqjublZREQaGxtl5cqVkpeXJ1VVVZKdnS2jR4+WgQMHyvXr1x0cfc9WrlwpOTk5cuHCBcnPz5fp06eLr6+vUq9t27aJn5+fHD16VEpKSuSVV14Ro9HoErndrr29XQYNGiRr1qxRjbtq/RobG6W4uFiKi4sFgOzcuVOKi4uVTwvZUrdFixZJSEiIZGVlSVFRkUyePFnMZrPcvHnTUWkpusvPYrFIQkKChISEyNmzZ1XrsrW1VUREfv31V/nwww+loKBAqqqqJCMjQ4YNGybPPPOMU+Qn0n2Oth6XzlxDkZ6PUxGRa9euSZ8+fWTv3r2dXu/sdezp/CDiHGuRzY2d7N69W8LCwsTT01NiYmJUH5t2NQC6fBw8eFBERJqbmyUuLk4ef/xx0ev1MmjQIElJSZGamhrHBm6jpKQkMRqNotfrxWQyyaxZs6S0tFTZbrVaZePGjRIcHCwGg0HGjx8vJSUlDoz4/nzzzTcCQMrLy1Xjrlq/7OzsLo/LlJQUEbGtbjdu3JAlS5ZIQECAeHt7y/Tp050m7+7yq6qquuu6zM7OFhGRmpoaGT9+vAQEBIinp6dERkbK22+/LVeuXHFsYrfpLkdbj0tnrqFIz8epiMj+/fvF29tbGhoaOr3e2evY0/lBxDnWou7/giUiIiLSBN5zQ0RERJrC5oaIiIg0hc0NERERaQqbGyIiItIUNjdERESkKWxuiIiISFPY3BAREZGmsLkhokdeTk4OdDodGhoaHB0KEdkBmxsiIiLSFDY3REREpClsbojI4UQE27dvxxNPPAFvb2+YzWZ8+eWXAP7/LaOMjAyYzWZ4eXlh1KhRKCkpUe3jq6++wvDhw2EwGBAeHo4dO3aotre2tmL16tUIDQ2FwWDA4MGDkZaWpppTWFiIESNGoE+fPhgzZgzKy8sfbuJE9FCwuSEih3v//fdx8OBB7N27F6WlpVixYgVee+015ObmKnPeffddfPLJJygoKEBQUBASEhJgsVgA3GpKEhMTkZycjJKSEmzatAkbNmzAoUOHlNfPnTsX6enp2LVrF8rKyrBv3z707dtXFcf69euxY8cO/PTTT/Dw8MD8+fN7JX8isi9+cSYROVRTUxMCAwNx6tQpjB49WhlfsGABmpub8cYbb2DSpElIT09HUlISAOCff/5BSEgIDh06hMTERMyZMwd//fUXvv32W+X1q1evRkZGBkpLS1FRUYGhQ4ciMzMTU6ZM6RRDTk4OJk2ahKysLLzwwgsAgJMnT2LatGm4ceMGvLy8HvL/AhHZE6/cEJFDnT9/Hi0tLZg6dSr69u2rPI4cOYLffvtNmXd74xMQEIChQ4eirKwMAFBWVoaxY8eq9jt27FhUVlaivb0dZ8+ehbu7OyZMmNBtLE8//bTy3Gg0AgDq6+sfOEci6l0ejg6AiB5tVqsVAJCRkYGBAweqthkMBlWDcyedTgfg1j07Hc873H5R2tvb26ZY9Hp9p313xEdEroNXbojIoZ566ikYDAbU1NTgySefVD1CQ0OVefn5+crzq1evoqKiAsOGDVP2cebMGdV+8/LyMGTIELi7uyM6OhpWq1V1Dw8RaRev3BCRQ/n6+mLVqlVYsWIFrFYrxo0bh+vXryMvLw99+/ZFWFgYAGDz5s3o378/BgwYgPXr1yMwMBAzZ84EAKxcuRIjR47Eli1bkJSUhO+//x6pqanYs2cPACA8PBwpKSmYP38+du3aBbPZjOrqatTX1yMxMdFRqRPRQ8LmhogcbsuWLQgKCsLWrVtx4cIF+Pv7IyYmBuvWrVPeFtq2bRuWLVuGyspKmM1mnDhxAp6engCAmJgYfPHFF/jggw+wZcsWGI1GbN68GfPmzVN+xt69e7Fu3Tq89dZbuHLlCgYNGoR169Y5Il0iesj4aSkicmodn2S6evUq/P39HR0OEbkA3nNDREREmsLmhoiIiDSFb0sRERGRpvDKDREREWkKmxsiIiLSFDY3REREpClsboiIiEhT2NwQERGRprC5ISIiIk1hc0NERESawuaGiIiINIXNDREREWnK/wLdzw4wUg5vwwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# summarize effect of lr vs loss\n",
        "#f,ax = plt.subplots(1,2,figsize=(16,12))\n",
        "import matplotlib.pyplot as plt\n",
        "for i in betas_data.values():\n",
        "    plt.plot(i['val_loss'])\n",
        "\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(list(betas_data.keys()), loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# betas= list(betas_data.keys())\n",
        "# # betas.remove(str(0.001))\n",
        "# for i in betas:\n",
        "#     plt.plot(betas_data[i]['val_loss'])\n",
        "# plt.title('model Loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(lrs, loc='upper right')\n",
        "# plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "S0NGBxXcVj4Z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('torch-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
